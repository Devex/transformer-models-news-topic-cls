{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bradloff/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bradloff/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Apex Import\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np \n",
    "from math import ceil, floor\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy, pytextrank\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCELoss, Module, Linear, AvgPool1d, MaxPool1d\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time, logging, sys, json\n",
    "\n",
    "%run -i \"../news-topic-cls/core/models/base.py\"\n",
    "%run -i \"../news-topic-cls/core/models/extension.py\"\n",
    "%run -i \"../news-topic-cls/core/data/data.py\"\n",
    "%run -i \"../news-topic-cls/core/utils/optim.py\"\n",
    "%run -i \"../news-topic-cls/core/utils/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>AG</th>\n",
       "      <th>BF</th>\n",
       "      <th>CE</th>\n",
       "      <th>ED</th>\n",
       "      <th>EN</th>\n",
       "      <th>FU</th>\n",
       "      <th>GH</th>\n",
       "      <th>HA</th>\n",
       "      <th>...</th>\n",
       "      <th>IN</th>\n",
       "      <th>MC</th>\n",
       "      <th>NR</th>\n",
       "      <th>PM</th>\n",
       "      <th>PS</th>\n",
       "      <th>RE</th>\n",
       "      <th>SD</th>\n",
       "      <th>TP</th>\n",
       "      <th>UD</th>\n",
       "      <th>WS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29186</td>\n",
       "      <td>Development jobs in Peru: What you need to kno...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29211</td>\n",
       "      <td>A turn to the slums; a call for alms “The weak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29233</td>\n",
       "      <td>Claire Kupper: An aid veteran serving conflict...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29238</td>\n",
       "      <td>From Turkey to Peru Via UNDP: Tracking the Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29241</td>\n",
       "      <td>Tracy Morrison: Village Queen in Cameroon, Dev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  AG  BF  CE  ED  \\\n",
       "0  29186  Development jobs in Peru: What you need to kno...   0   0   1   0   \n",
       "1  29211  A turn to the slums; a call for alms “The weak...   0   0   0   0   \n",
       "2  29233  Claire Kupper: An aid veteran serving conflict...   0   0   1   0   \n",
       "3  29238  From Turkey to Peru Via UNDP: Tracking the Car...   0   0   1   0   \n",
       "4  29241  Tracy Morrison: Village Queen in Cameroon, Dev...   0   0   1   0   \n",
       "\n",
       "   EN  FU  GH  HA  ...  IN  MC  NR  PM  PS  RE  SD  TP  UD  WS  \n",
       "0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "1   0   1   0   0  ...   1   0   0   0   0   0   0   0   1   0  \n",
       "2   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/article_confirmed.csv\"\n",
    "if \"test\" in data_path:\n",
    "    df = pd.read_csv(data_path).drop(labels=\"Unnamed: 0\", axis=1)\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "#df = clean_data(df)\n",
    "df, label_encoder = encode_labels(df)\n",
    "\n",
    "abbrev_mapping = get_abbreveation_mapping(label_encoder)\n",
    "df = binarize_labels(df, abbrev_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dataframe):\n",
    "    id_list = [62094]\n",
    "    cleaned_df = dataframe[~dataframe.id.isin(id_list)]\n",
    "    \"\"\"\n",
    "    event_preview_ids = []\n",
    "    for index, row in tqdm(dataframe.iterrows()):\n",
    "        if row.text[:15] == \"Events preview:\":\n",
    "            event_preview_ids.append(row.id)\n",
    "    long_enumeration_ids = [94003]\n",
    "    cleaned_df = cleaned_df[~cleaned_df.id.isin(long_enumeration_ids + event_preview_ids)]\n",
    "    \"\"\"\n",
    "    cleaned_df.replace(\"<.*>\", \"\", regex=True, inplace=True)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.7/site-packages/pandas/core/frame.py:4172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_path = \"../data/models/language_models/roberta-lm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"roberta-base\"\n",
    "transformer = TransformerOptions(config_name, lm_path=lm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in transformer.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tr = pytextrank.TextRank(logger=None)\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(tokenized_sentences: list, language_model):\n",
    "    language_model.eval()\n",
    "    for step, tokens in enumerate(tokenized_sentences):\n",
    "        outputs = language_model(tokens)\n",
    "        # set_trace()\n",
    "        embedding = outputs[0][:, 0, :]\n",
    "        if step == 0:\n",
    "            embeddings = embedding.numpy()\n",
    "        else:\n",
    "            embeddings = np.concatenate((embeddings, embedding), axis=0)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_sentences_for_summary(summary_df, cluster_center):\n",
    "    summary_sentences = [0]*len(cluster_center)\n",
    "\n",
    "    for index, center in enumerate(cluster_center):\n",
    "        is_cluster = summary_df.cluster == index\n",
    "        cluster_df = summary_df[is_cluster]\n",
    "        min_distance = np.Inf\n",
    "        for _, row in cluster_df.iterrows():\n",
    "            distance = np.linalg.norm(center-row.embedding)\n",
    "\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                summary_sentences[index] = (row.position, row.sentence)\n",
    "    return sorted(summary_sentences, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def get_row_of_point(summary_df, point):\n",
    "    for _, row in summary_df.iterrows():\n",
    "        if np.all(row.embedding == point):\n",
    "            return row\n",
    "\n",
    "\n",
    "def get_sentences_for_summary_by_exemplar(summary_df, exemplars_):\n",
    "    exemplars = exemplars_.copy()\n",
    "    number_clusters = len(exemplars)\n",
    "    summary_sentences = []\n",
    "\n",
    "    while len(summary_sentences) < 10:\n",
    "        summary_sentences_before_loop = summary_sentences.copy()\n",
    "        for index, exemplar in enumerate(exemplars):\n",
    "            if len(exemplar) == 0:\n",
    "                continue\n",
    "            point = exemplar[0]\n",
    "\n",
    "            row = get_row_of_point(summary_df, point)\n",
    "            # set_trace()\n",
    "            summary_sentences.append((row.position, row.sentence))\n",
    "\n",
    "            exemplar = exemplar[1:]\n",
    "            exemplars[index] = exemplar\n",
    "\n",
    "            if len(summary_sentences) == 10:\n",
    "                break\n",
    "        if summary_sentences == summary_sentences_before_loop:\n",
    "            break\n",
    "\n",
    "    index = 0\n",
    "    while len(summary_sentences) < 10:\n",
    "        summary_sentences_before_loop = summary_sentences.copy()\n",
    "        for cluster in range(number_clusters):\n",
    "            cluster_points = summary_df[summary_df.cluster == cluster]\n",
    "            try:\n",
    "                row = cluster_points.iloc[index, :]\n",
    "                summary_sentences.append((row.position, row.sentence))\n",
    "            except IndexError:\n",
    "                continue\n",
    "            if len(summary_sentences) == 10:\n",
    "                break\n",
    "        if summary_sentences == summary_sentences_before_loop:\n",
    "            break\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    # set_trace()\n",
    "    if len(summary_sentences) < 10:\n",
    "        outlier_df = summary_df[summary_df.cluster == -1]\n",
    "        for _, row in outlier_df.iterrows():\n",
    "            summary_sentences.append((row.position, row.sentence))\n",
    "            if len(summary_sentences) == 10:\n",
    "                return sorted(summary_sentences, key=lambda x: x[0])\n",
    "    else:\n",
    "        return sorted(summary_sentences, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def find_quotes(sentence):\n",
    "    condition = re.compile(\".*?”\")\n",
    "    matches = condition.finditer(sentence)\n",
    "    quotes = [match.group().strip() for match in matches]\n",
    "    if quotes == []:\n",
    "        return [sentence]\n",
    "    last_match = quotes[-1]\n",
    "    lower_bound = sum([len(quote) for quote in quotes]) - len(last_match)\n",
    "    last_match_end = sentence.find(last_match, lower_bound) + len(last_match)\n",
    "    after_quotes = sentence[last_match_end:].strip()\n",
    "    if after_quotes != \"\":\n",
    "        quotes.append(after_quotes)\n",
    "    return quotes\n",
    "\n",
    "\n",
    "def sentenize(article):\n",
    "    sentences = nltk.tokenize.sent_tokenize(article)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        quotes = find_quotes(sentence)\n",
    "        sentences[index] = quotes\n",
    "\n",
    "    def flatten(list_): return [item for sublist in list_ for item in sublist]\n",
    "    return flatten(sentences)\n",
    "\n",
    "\n",
    "def sentenize_without_quotes(article):\n",
    "    article_cleaned = article.replace(\"“\", \"\")\n",
    "    article_cleaned = article_cleaned.replace(\"”\", \"\")\n",
    "\n",
    "    return nltk.tokenize.sent_tokenize(article_cleaned)\n",
    "\n",
    "\n",
    "def postprocess_sentences(sentences):\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if sentence.count(\",\") > 15:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            word_counter = len(tokens)\n",
    "            if word_counter >= 200:\n",
    "                sentences[index] = \"List of upcoming events.\"\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def cluster_embeddings_kmeans(sentences, sentence_embeddings, n_cluster):\n",
    "    clusterer = KMeans(n_clusters=n_cluster)\n",
    "    clusterer.fit(sentence_embeddings)\n",
    "\n",
    "    summary_df = pd.DataFrame(data={\n",
    "        \"position\": range(len(sentences)),\n",
    "        \"sentence\": sentences,\n",
    "        \"embedding\": sentence_embeddings.tolist(),\n",
    "        \"cluster\": clusterer.labels_\n",
    "    })\n",
    "    return summary_df, clusterer.cluster_centers_\n",
    "\n",
    "\n",
    "def cluster_embeddings_hdbscan(sentences, sentence_embeddings):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=None)\n",
    "    clusterer.fit(sentence_embeddings)\n",
    "\n",
    "    summary_df = pd.DataFrame(data={\n",
    "        \"position\": range(len(sentences)),\n",
    "        \"sentence\": sentences,\n",
    "        \"embedding\": sentence_embeddings.tolist(),\n",
    "        \"cluster\": clusterer.labels_\n",
    "    })\n",
    "\n",
    "    return summary_df, clusterer.exemplars_\n",
    "\n",
    "\n",
    "def summarize(article: str, transformer, cluster_alg: str = \"hdbscan\", n_cluster=None):\n",
    "    sentences = sentenize_without_quotes(article)\n",
    "    if len(sentences) <= 10:\n",
    "        return article\n",
    "\n",
    "    tokenized_sentences = [transformer.tokenizer(text=sentence,\n",
    "                                                 truncation=True,\n",
    "                                                 return_tensors=\"pt\")[\"input_ids\"]\n",
    "                           for sentence in sentences]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sentence_embeddings = compute_embeddings(\n",
    "            tokenized_sentences, transformer.model)\n",
    "\n",
    "    if cluster_alg == \"kmeans\":\n",
    "        assert n_cluster is not None\n",
    "        summary_df, cluster_centers = cluster_embeddings_kmeans(sentences,\n",
    "                                                                sentence_embeddings,\n",
    "                                                                n_cluster=10)\n",
    "        summary_sentences = get_sentences_for_summary(\n",
    "            summary_df, cluster_centers)\n",
    "\n",
    "    elif cluster_alg == \"hdbscan\":\n",
    "        summary_df, cluster_exemplars = cluster_embeddings_hdbscan(sentences,\n",
    "                                                                   sentence_embeddings)\n",
    "        summary_sentences = get_sentences_for_summary_by_exemplar(\n",
    "            summary_df, cluster_exemplars)\n",
    "\n",
    "    return \" \".join([sentence for _, sentence in summary_sentences])\n",
    "\n",
    "\n",
    "def textrank_summary(article, processor):\n",
    "    summ = processor(article)\n",
    "    spacy_spans = summ._.textrank.summary(\n",
    "        limit_phrases=None, limit_sentences=10)\n",
    "    summary_sentences = [spacy_span.text for spacy_span in spacy_spans]\n",
    "    return \" \".join(summary_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_id = df.id == 96632\n",
    "article = df[is_id].text.values[0]\n",
    "sentences = sentenize_without_quotes(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 24\n",
      "___________________________\n",
      "Sentences:\n",
      "Facebook fights fake coronavirus news, wants to build trust with public sector SAN FRANCISCO — With its power to influence global health information, Facebook is now helping fight fake coronavirus news.\n",
      "\n",
      "\n",
      "Following the emergence of the new coronavirus, COVID-19, and the proliferation of bad information about it, the social media giant has already begun removing bogus information about cures and prevention methods.\n",
      "\n",
      "\n",
      "Facebook is also looking to partnerships with the public sector to replace this misinformation with helpful, credible alternatives, said Kang-Xing Jin, Facebook's head of health, who spoke at Devex’s Prescription for Progress event on technology for health in San Francisco, California, earlier this month.\n",
      "\n",
      "\n",
      "We need to help get people access to credible information from trusted sources in the moments that they’re seeking it out.\n",
      "\n",
      "\n",
      "--— Kang-Xing Jin, head of health, Facebook We’re working really closely with organizations like the [World Health Organization], as well as a variety of local ministries of health, to help them meet people where they are searching, Jin said.\n",
      "\n",
      "\n",
      "When people search for coronavirus or other related terms, Facebook will direct them to credible information at the top of the results with links to the WHO’s website or other relevant sources, Jin said.\n",
      "\n",
      "\n",
      "Misinformation really thrives when there’s a void or an absence of accurate information or trusted information, he said.\n",
      "\n",
      "\n",
      "The efforts to tackle bad information about COVID-19 are building on similar collaborations aimed at pushing back on misinformation about vaccines and stemming the rise of vaccine hesitancy, which WHO declared one of the 10 biggest threats to health last year, Jin said.\n",
      "\n",
      "\n",
      "We have work to do on removing and reducing harmful information, but just as importantly, we need to help get people access to credible information from trusted sources in the moments that they’re seeking it out, he said.\n",
      "\n",
      "\n",
      "That’s something that I think we really need to do in close partnership with the public sector, because again, we’re not the people who have that information.\n",
      "\n",
      "\n",
      "That actually really shouldn’t be us.\n",
      "\n",
      "\n",
      "But we need to work closely to connect people in that way, he added.\n",
      "\n",
      "\n",
      "In addition to improving the public’s response to these emerging health threats, collaboration between Facebook and public organizations might offer a pathway to establishing — or rebuilding — trust between communities that have found themselves at odds over questions related to misinformation, privacy, and responsibility.\n",
      "\n",
      "\n",
      "Trusted relationships between us and the public sector and the people we work with are incredibly important.\n",
      "\n",
      "\n",
      "Even in the last year that’s gotten better.\n",
      "\n",
      "\n",
      "It just takes time.\n",
      "\n",
      "\n",
      "You have to work together.\n",
      "\n",
      "\n",
      "You have to learn each other’s language, and you have to honestly work through some difficult issues together, said Jin, who was one of Facebook’s earliest employees, and who met Mark Zuckerberg on their first day of classes at Harvard.\n",
      "\n",
      "\n",
      "For the global health community, Facebook offers unprecedented reach to individuals with access to the social media platform — for better, or for worse.\n",
      "\n",
      "\n",
      "The positive potential of the company’s global footprint stands out in initiatives like Facebook’s blood donation tool, which matches blood banks in need with people who sign up to be notified as potential donors.\n",
      "\n",
      "\n",
      "So far, 60 million people have registered as donors on Facebook, Jin said, adding that 40 million of those are in India, one of the first countries where it launched the tool.\n",
      "\n",
      "\n",
      "Donating blood is fundamentally an altruistic thing, and it’s pretty amazing that this many people are willing to do it when presented with the right opportunity and the right information, he said.\n",
      "\n",
      "\n",
      "That volume and speed of uptake — the tool has only been available for two years — suggests the global health community has still not fully realized the potential mobile phone platforms offer.\n",
      "\n",
      "\n",
      "That’s kind of like the first thing that I’d love to realize over the next 10 years, to be able to realize this at scale and with the associated causal impact measurements so we can really understand what’s working and what’s not, he said.\n",
      "\n",
      "\n",
      "___________________________\n",
      "Labels:\n",
      "Global Health\n",
      "Media And Communications\n",
      "Private Sector\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences: {}\".format(len(sentences)))\n",
    "print(\"___________________________\")\n",
    "print(\"Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    if sentence==\"\":\n",
    "        print(\"Empty Sentence\")\n",
    "    print(\"\\n\")\n",
    "print(\"___________________________\")\n",
    "print(\"Labels:\")\n",
    "for key, value in df[is_id].iloc[0].iteritems():\n",
    "    if value==1:\n",
    "        print(abbrev_mapping[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing single Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.read_csv(\"../data/article_confirmed_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1.3889808654785156\n",
      "Summary: \n",
      "Researchers were clear on one point though: There has not been enough good research into potential links between malaria prophylactics and a variety of health outcomes experienced by people who have taken them. The evidence base to make a determination is really quite limited, but as far as the question being a reasonable and appropriate one to ask, we think that is the case, Savitz told Devex. In addition to military personnel and others deployed overseas, the researchers looked at available research on malaria drug use and effects among U.S. Peace Corps volunteers, many of whom serve in malaria-endemic countries, and who are required to take malaria prevention medication. Approximately 10% of malaria cases reported annually in U.S. citizens by the Centers for Disease Control and Prevention occur in Peace Corps volunteers, according to the report. Each of these comes with recommendations for who should take them and the concurrent and short-term adverse effects of taking them — those that occur during the drug course — are well documented. The review was only able to find sufficient evidence between one malaria medication and one persistent health outcome, and it should probably not be cause for great concern, Savitz said. The report’s more significant conclusion, Savitz said, relates to the weaknesses of the evidence base for long-term effects of malaria medication. I wish we had clearer answers, but there’s no getting around the observation that the body of research to inform this issue is both limited in volume and in quality, and there’s really, we think, a case to be made for looking further into it, Savitz said. The researchers pointed to some elements of future studies that might make them better suited for drawing clearer conclusions about long-term health outcomes and malaria drugs. These should include better documentation of the antimalarial dosage used and a clear timeline of both antimalarial drug use and symptom or event occurrence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "summary_cluster = summarize(article=article, cluster_alg=\"hdbscan\", transformer=transformer)\n",
    "stop = time.time()\n",
    "print(\"Time {}\".format(stop-start))\n",
    "print(\"Summary: \\n{}\\n\".format(summary_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.19610881805419922\n",
      "Summary: \n",
      "In addition to military personnel and others deployed overseas, the researchers looked at available research on malaria drug use and effects among U.S. Peace Corps volunteers, many of whom serve in malaria-endemic countries, and who are required to take malaria prevention medication. The researchers looked at the available evidence for associations between specific malaria drugs and persistent health outcomes — those that appear during the period of the medication and persist after the course has ended — as well as latent health outcomes — those that appear after use is completed. Approximately 10% of malaria cases reported annually in U.S. citizens by the Centers for Disease Control and Prevention occur in Peace Corps volunteers, according to the report. The review was only able to find sufficient evidence between one malaria medication and one persistent health outcome, and it should probably not be cause for great concern, Savitz said. The study, published Tuesday by the National Academies of Sciences, Engineering, and Medicine, sought to respond to questions raised mainly by advocates for U.S. military personnel deployed in malaria-endemic countries. Researchers were clear on one point though: There has not been enough good research into potential links between malaria prophylactics and a variety of health outcomes experienced by people who have taken them. Peace Corps offers window into long-term impacts of malaria drugs WASHINGTON — Most of the health outcomes the researchers examined — including post-traumatic stress disorder; neurologic and psychiatric events; and cardiovascular events — fell under the category of “inadequate or insufficient evidence of an association.” There are currently six malaria prophylactic drugs approved by the U.S. Food and Drug Administration: chloroquine, primaquine, mefloquine, doxycycline, atovaquone/proguanil, and tafenoquine. The researchers pointed to some elements of future studies that might make them better suited for drawing clearer conclusions about long-term health outcomes and malaria drugs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "summary_textrank = textrank_summary(article, nlp)\n",
    "stop = time.time()\n",
    "print(\"Time {}\".format(stop-start))\n",
    "print(\"Summary: \\n{}\\n\".format(summary_textrank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length tokenized Summary: 462\n"
     ]
    }
   ],
   "source": [
    "tokenized_summary = transformer.tokenizer.encode(summary_textrank)\n",
    "print(\"Length tokenized Summary: {}\".format(len(tokenized_summary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../news-topic-cls/core/utils/utils.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "summary_row.id = 96627\n",
    "summary_row.text = article\n",
    "summary_row.summary_cluster = summary_cluster\n",
    "summary_row.summary_textrank = summary_textrank\n",
    "summary_row.loc[list(abbrev_mapping.keys())] = row.loc[list(abbrev_mapping.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.iloc[15,:] = summary_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(\"../data/article_confirmed_roberta-lm_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = pd.read_csv(\"../data/test_data_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_id = summaries.id == 29211\n",
    "print(summaries[is_id].summary.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_length = []\n",
    "invalid_ids = []\n",
    "long_summary_ids = []\n",
    "for index, row in summaries.iterrows():\n",
    "    \"\"\"\n",
    "    print(row.text)\n",
    "    print(\"\\n\")\n",
    "    print(row.summary)\n",
    "    print(\"\\nLabels:\")\n",
    "    for key, value in row.iteritems():\n",
    "        if value==1:\n",
    "            print(abbrev_mapping[key])\n",
    "    break\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenized_summary = transformer.tokenizer.encode(row.summary)\n",
    "        tokenized_length.append(len(tokenized_summary))\n",
    "        if len(tokenized_summary)>511:\n",
    "            long_summary_ids.append(row.id)\n",
    "    except:\n",
    "        invalid_ids.append(row.id)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df.text.values\n",
    "for index, row in df.iterrows():\n",
    "    article = row.text\n",
    "    sentences = sentenize(article)\n",
    "    for sentence in sentences:\n",
    "        if sentence.count(\",\") > 15:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            word_counter = len(tokens)\n",
    "            if word_counter >= 200:\n",
    "                print(row.id)\n",
    "                print(sentence)\n",
    "                print(\"\\nCommas: {}\".format(sentence.count(\",\")))\n",
    "                print(\"\\nLabels:\")\n",
    "                for key, value in row.iteritems():\n",
    "                    if value==1:\n",
    "                        print(abbrev_mapping[key])\n",
    "                print(\"\\n\")\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                text = nltk.Text(tokens)\n",
    "                tagged = nltk.pos_tag(text)\n",
    "                counts = Counter(tag for word,tag in tagged)\n",
    "                verb_counter = counts[\"VB\"] + counts[\"VBD\"] + counts[\"VBG\"] + counts[\"VBN\"] + counts[\"VBP\"] + counts[\"VBZ\"] \n",
    "                print(\"Verb Counter: {}\".format(verb_counter))\n",
    "                print(\"Word Counter: {}\".format(word_counter))\n",
    "                print(\"_________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"Events preview:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for index, row in df.iterrows():\n",
    "    article = row.text\n",
    "    if \"Events preview:\" in article:\n",
    "        counter +=1\n",
    "        \"\"\"\n",
    "        sentences = sentenize(article)\n",
    "        print(row.id)\n",
    "        print(\"Number of Sentences: {}\".format(len(sentences)))\n",
    "        for sentence in sentences:\n",
    "            print(\"\\n\")\n",
    "            print(sentence)\n",
    "        print(\"\\nLabels:\")\n",
    "        for key, value in row.iteritems():\n",
    "            if value==1:\n",
    "                print(abbrev_mapping[key])\n",
    "                if abbrev_mapping[key]==\"Media And Communications\":\n",
    "        print(\"_______________________________\\n\")\n",
    "        \"\"\"\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
