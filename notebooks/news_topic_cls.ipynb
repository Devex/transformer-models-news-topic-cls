{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "news-topic-cls.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vPaHh_f7hAUE",
        "i3xGsXmuVyuN",
        "LWdlcNutlR19",
        "njORoBtOlVo8",
        "x21TbOh0B0X0",
        "5ovHvUewnKcx",
        "Dok3hO7ByPYU",
        "8Zda-4J3MGRV",
        "GkPqWElZMRIJ",
        "ICbpKAEYMexT",
        "Cl4akQY-tjv6",
        "yk3SyoiaBbKA",
        "GYqxK6ogBUJH",
        "Q9ivTB6oTvPF",
        "VJZsK_QHqPq5"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kWFRzXWnhXY",
        "outputId": "3e28a61c-7ebc-4581-bb42-da432ca75148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPaHh_f7hAUE"
      },
      "source": [
        "\n",
        "# GPU Support"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oUH6O7GOys9",
        "outputId": "9bbf11b2-0753-4e1a-99f3-7836de9ecf52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "!pip install GPUtil\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import torch\n",
        "import GPUtil as GPU\n",
        "print(\"____________________________________________________\")\n",
        "print(\"Device: {}\".format(torch.cuda.get_device_name()))\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=4ae64ccbc4e249db62e402aa28af9a589a97e2807ee41b536a00dd17175aedbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "____________________________________________________\n",
            "Device: Tesla V100-SXM2-16GB\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 247.1 MB\n",
            "GPU RAM Free: 16120MB | Used: 10MB | Util   0% | Total 16130MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xGsXmuVyuN"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWdlcNutlR19"
      },
      "source": [
        "### First Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaOVQVFI-Sfo"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls\"\n",
        "%mkdir cls-data\n",
        "%mkdir lm-data\n",
        "%mkdir classifier-output\n",
        "%mkdir language-models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B94-Ul2g4vhA"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls\"\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls\"\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njORoBtOlVo8"
      },
      "source": [
        "### Not First Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Fyo5POEprk",
        "outputId": "72540d3f-76ab-452c-f100-08d8212a2a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/apex\"\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "!pip install transformers\n",
        "!pip install pytorch-lightning==\"0.9\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/apex\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-7roo1w5z\n",
            "Created temporary directory: /tmp/pip-req-tracker-g73nsuvm\n",
            "Created requirements tracker '/tmp/pip-req-tracker-g73nsuvm'\n",
            "Created temporary directory: /tmp/pip-install-ovyusdaz\n",
            "Processing /content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-jwn97j45\n",
            "  Added file:///content/drive/Shared%20drives/Product%20Development/Google%20Colab/news-topic-cls/apex to build tracker '/tmp/pip-req-tracker-g73nsuvm'\n",
            "    Running setup.py (path:/tmp/pip-req-build-jwn97j45/setup.py) egg_info for package from file:///content/drive/Shared%20drives/Product%20Development/Google%20Colab/news-topic-cls/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-jwn97j45/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-jwn97j45/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-jwn97j45/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-jwn97j45/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-jwn97j45/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-jwn97j45/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-jwn97j45/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-jwn97j45 has version 0.1, which satisfies requirement apex==0.1 from file:///content/drive/Shared%20drives/Product%20Development/Google%20Colab/news-topic-cls/apex\n",
            "  Removed apex==0.1 from file:///content/drive/Shared%20drives/Product%20Development/Google%20Colab/news-topic-cls/apex from build tracker '/tmp/pip-req-tracker-g73nsuvm'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-98qqq0eo\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-jwn97j45/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-jwn97j45/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-98qqq0eo/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-jwn97j45/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:335: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/record_function.h(18): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(126): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/record_function.h(18): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(126): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-98qqq0eo/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-jwn97j45\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-g73nsuvm'\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=cfbe27aec4c068fdc8d6e0bfd0aae6a99eb38a9545864648fc9f4ac8ecbe455e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting pytorch-lightning==0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/af/2f10c8ee22d7a05fe8c9be58ad5c55b71ab4dd895b44f0156bfd5535a708/pytorch_lightning-0.9.0-py3-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9) (1.6.0+cu101)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9) (4.41.1)\n",
            "Collecting tensorboard==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/f5/d75a6f7935e4a4870d85770bc9976b12e7024fbceb83a1a6bc50e6deb7c4/tensorboard-2.2.0-py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 13.5MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9) (20.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (50.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (0.35.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (3.2.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (1.32.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9) (1.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytorch-lightning==0.9) (2.4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning==0.9) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning==0.9) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning==0.9) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning==0.9) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch-lightning==0.9) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning==0.9) (2.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning==0.9) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning==0.9) (3.2.0)\n",
            "Building wheels for collected packages: PyYAML, future\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=b21f4a728995cfd5163b3f57d682401e71123eae468bd08efd396b80df8ca167\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=4464971d68509288ea616090074d270e8ecdaadc7dcdb9aef8dd1744a4e28f67\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built PyYAML future\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: PyYAML, tensorboard, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 future-0.18.2 pytorch-lightning-0.9.0 tensorboard-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x21TbOh0B0X0"
      },
      "source": [
        "# Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ovHvUewnKcx"
      },
      "source": [
        "## RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQgkCu9Tsss8"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNoVkn1NdEFb",
        "outputId": "e2e894e6-23f1-45fb-a4ae-e5b80784f533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models\"\n",
        "%mkdir \"roberta-large-lm\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02_XZ5pfcsZj",
        "outputId": "87e23da6-9ba8-4e3c-af74-c815cb1f7dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-large-lm\" \\\n",
        "    --model_type=roberta \\\n",
        "    --model_name_or_path=\"roberta-large\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=\"/content/drive/My Drive/language_model/data/text_eval.txt\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\n",
            "2020-10-17 07:34:30.239184: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:332: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "10/17/2020 07:34:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/17/2020 07:34:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-large-lm', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct17_07-34-32_5feb47410d42', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False)\n",
            "10/17/2020 07:34:32 - INFO - filelock -   Lock 140193174352056 acquired on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n",
            "Downloading: 100% 482/482 [00:00<00:00, 343kB/s]\n",
            "10/17/2020 07:34:33 - INFO - filelock -   Lock 140193174352056 released on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n",
            "10/17/2020 07:34:33 - INFO - filelock -   Lock 140192780627920 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.09MB/s]\n",
            "10/17/2020 07:34:34 - INFO - filelock -   Lock 140192780627920 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "10/17/2020 07:34:34 - INFO - filelock -   Lock 140192780626632 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.06MB/s]\n",
            "10/17/2020 07:34:35 - INFO - filelock -   Lock 140192780626632 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "10/17/2020 07:34:35 - INFO - filelock -   Lock 140192780627752 acquired on /root/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536.lock\n",
            "Downloading: 100% 1.43G/1.43G [01:23<00:00, 17.1MB/s]\n",
            "10/17/2020 07:35:59 - INFO - filelock -   Lock 140192780627752 released on /root/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536.lock\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:267: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "10/17/2020 07:37:08 - INFO - __main__ -   *** Evaluate ***\n",
            "Evaluation: 100% 10892/10892 [16:09<00:00, 11.24it/s]\n",
            "{'eval_loss': 1.552426039581096, 'step': 0}\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1175: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "10/17/2020 07:53:17 - INFO - __main__ -   ***** Eval results *****\n",
            "10/17/2020 07:53:17 - INFO - __main__ -     perplexity = 4.722914272032829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEJKqSwxS0aJ",
        "outputId": "8d331c21-f951-43fc-994d-d312f1ef6170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-large-lm\" \\\n",
        "    --model_type=roberta \\\n",
        "    --model_name_or_path=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-large-lm\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/lm-data/text_eval.txt\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\n",
            "2020-10-19 08:24:57.965232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:332: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "10/19/2020 08:25:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/19/2020 08:25:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-large-lm', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct19_08-25-00_b1a08a720f64', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:267: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "10/19/2020 08:26:39 - INFO - __main__ -   *** Evaluate ***\n",
            "Evaluation: 100% 10892/10892 [16:17<00:00, 11.14it/s]\n",
            "{'eval_loss': 1.4244555070253166, 'step': 0}\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1175: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "10/19/2020 08:42:58 - INFO - __main__ -   ***** Eval results *****\n",
            "10/19/2020 08:42:58 - INFO - __main__ -     perplexity = 4.155594534604506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88F5elH8CyfH"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-large-lm\" \\\n",
        "    --model_type=roberta \\\n",
        "    --model_name_or_path=roberta-large \\\n",
        "    --do_train \\\n",
        "    --train_data_file=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/lm-data/text_train.txt\" \\\n",
        "    --save_steps=10000 \\\n",
        "    --save_total_limit=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --num_train_epochs=1 \\\n",
        "    --fp16 \\\n",
        "    --fp16_opt_level=\"O1\" \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --logging_steps=100 \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dok3hO7ByPYU"
      },
      "source": [
        "## Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5dvx25z0Ev8"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\" \\\n",
        "    --model_type=longformer \\\n",
        "    --model_name_or_path=\"allenai/longformer-base-4096\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/lm-data/text_eval.txt\" \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvK12PiByiXu"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm/part1\" \\\n",
        "    --model_type=longformer \\\n",
        "    --model_name_or_path=allenai/longformer-base-4096 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/lm-data/text_train1.txt\" \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --num_train_epochs=1 \\\n",
        "    --gradient_accumulation_steps=8 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --logging_steps=100 \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAZbsW6WZDPO"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\" \\\n",
        "    --model_type=longformer \\\n",
        "    --model_name_or_path=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm/part1\" \\\n",
        "    --do_train \\\n",
        "    --train_data_file=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/lm-data/text_train2.txt\" \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --num_train_epochs=1 \\\n",
        "    --gradient_accumulation_steps=8 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --logging_steps=100 \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI2I2_NfGgCA"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/transformers/examples/language-modeling\"\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\" \\\n",
        "    --model_type=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\" \\\n",
        "    --model_name_or_path=\"allenai/longformer-base-4096\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=\"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/lm-data/text_eval.txt\" \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --line_by_line \\\n",
        "    --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zda-4J3MGRV"
      },
      "source": [
        "# Classifier Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odpeDU99pmqa"
      },
      "source": [
        "root_dir = \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\"\n",
        "root_dir = os.path.abspath(root_dir)\n",
        "print(root_dir)\n",
        "for subdir, dirs, files in os.walk(root_dir):\n",
        "    if subdir.split(\"/\")[-1] == \"checkpoints\":\n",
        "        for subdir, dirs, files in os.walk(subdir):\n",
        "            for file in files:\n",
        "                os.remove(os.path.join(subdir, file))\n",
        "                print(\"Removed File: {}\".format(file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkPqWElZMRIJ"
      },
      "source": [
        "## Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y284saK4VNW7"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_longformer.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/test\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--amount_labels 1 \\\n",
        "--num_workers 4 \\\n",
        "--batch_size 1 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--gpus 1 \\\n",
        "--max_epochs 3 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfE-107qMNGM",
        "outputId": "a4bdf577-5711-48ae-dafd-83f2a1d55ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python longformer_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 1 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 6 \\\n",
        "--accumulate_grad_batches 8 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-05 07:48:04.602188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 1.76MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n",
            "Downloading: 100% 694/694 [00:00<00:00, 511kB/s]\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at /content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/4\",\n",
            "    \"amount_labels\": 5,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"loss_weight\": 2.0,\n",
            "    \"batch_size\": 1,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 6,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 8,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 3897/3897 [00:24<00:00, 156.00it/s]\n",
            "100% 885/885 [00:05<00:00, 170.95it/s]\n",
            "100% 77/77 [00:00<00:00, 173.68it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | longformer | LongformerModel     | 148 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 594 K \n",
            "Epoch 5: 100% 3897/3897 [39:40<00:00,  1.64it/s, loss=0.514, v_num=5-4, train_loss=0.523]Saving latest checkpoint..\n",
            "Epoch 5: 100% 3897/3897 [39:41<00:00,  1.64it/s, loss=0.514, v_num=5-4, train_loss=0.523]\n",
            "Computing Input\n",
            "100% 3897/3897 [00:22<00:00, 174.27it/s]\n",
            "100% 885/885 [00:04<00:00, 181.62it/s]\n",
            "100% 77/77 [00:00<00:00, 190.19it/s]\n",
            "Testing: 100% 77/77 [00:12<00:00,  6.14it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 27.0\n",
            "fp: 30.0\n",
            "fn: 16.0\n",
            "tn: 312.0\n",
            "precision: 0.4736842215061188\n",
            "recall: 0.6279069781303406\n",
            "f1: 0.5400000214576721\n",
            "mcc: 0.47909054160118103\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.5584), 'test_loss': tensor(0.3464, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 77/77 [00:13<00:00,  5.88it/s]\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at /content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/4\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/longformer-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"loss_weight\": 4.0,\n",
            "    \"batch_size\": 1,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 6,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 8,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 6138/6138 [00:38<00:00, 159.09it/s]\n",
            "100% 885/885 [00:05<00:00, 170.72it/s]\n",
            "100% 77/77 [00:00<00:00, 170.43it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | longformer | LongformerModel     | 148 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 606 K \n",
            "Epoch 5: 100% 6138/6138 [1:02:36<00:00,  1.63it/s, loss=0.623, v_num=21-4, train_loss=0.616]Saving latest checkpoint..\n",
            "Epoch 5: 100% 6138/6138 [1:02:37<00:00,  1.63it/s, loss=0.623, v_num=21-4, train_loss=0.616]\n",
            "Computing Input\n",
            "100% 6138/6138 [00:33<00:00, 180.55it/s]\n",
            "100% 885/885 [00:04<00:00, 179.65it/s]\n",
            "100% 77/77 [00:00<00:00, 169.83it/s]\n",
            "Testing: 100% 77/77 [00:12<00:00,  6.15it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 113.0\n",
            "fp: 228.0\n",
            "fn: 66.0\n",
            "tn: 1210.0\n",
            "precision: 0.331378310918808\n",
            "recall: 0.6312848925590515\n",
            "f1: 0.4346153736114502\n",
            "mcc: 0.3635949492454529\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0390), 'test_loss': tensor(0.4231, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 77/77 [00:13<00:00,  5.89it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICbpKAEYMexT"
      },
      "source": [
        "## RobSum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHefGoJXJIhE"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"cluster\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 6 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddI04OSvMivQ"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 6 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl4akQY-tjv6"
      },
      "source": [
        "### Classifier Head Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yugAS6F2ANwy",
        "outputId": "9c783f57-24a0-497e-ed65-7d47b184e2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"cluster\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 2 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--cls_hidden_size 107520 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-12 16:20:40.079642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/0\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5814/5814 [00:11<00:00, 527.05it/s]\n",
            "100% 885/885 [00:01<00:00, 578.99it/s]\n",
            "100% 77/77 [00:00<00:00, 635.37it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.61it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 86.0\n",
            "eval_fn: 11.0\n",
            "eval_tn: 61.0\n",
            "eval_precision: 0.1041666641831398\n",
            "eval_recall: 0.4761904776096344\n",
            "eval_f1: 0.17094017565250397\n",
            "eval_mcc: -0.07273929566144943\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 727/838 [03:14<00:29,  3.74it/s, loss=0.425, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 728/838 [03:14<00:29,  3.74it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  87% 730/838 [03:14<00:28,  3.75it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  87% 732/838 [03:14<00:28,  3.76it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  88% 734/838 [03:15<00:27,  3.76it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  88% 736/838 [03:15<00:27,  3.77it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  88% 738/838 [03:15<00:26,  3.78it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  88% 740/838 [03:15<00:25,  3.78it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  89% 742/838 [03:15<00:25,  3.79it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  89% 744/838 [03:15<00:24,  3.80it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  89% 746/838 [03:16<00:24,  3.80it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  89% 748/838 [03:16<00:23,  3.81it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  89% 750/838 [03:16<00:23,  3.82it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  90% 752/838 [03:16<00:22,  3.83it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  90% 754/838 [03:16<00:21,  3.83it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  90% 756/838 [03:16<00:21,  3.84it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  90% 758/838 [03:17<00:20,  3.85it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  91% 760/838 [03:17<00:20,  3.85it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  91% 762/838 [03:17<00:19,  3.86it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  91% 764/838 [03:17<00:19,  3.87it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  91% 766/838 [03:17<00:18,  3.87it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  92% 768/838 [03:17<00:18,  3.88it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  92% 770/838 [03:18<00:17,  3.89it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  92% 772/838 [03:18<00:16,  3.89it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  92% 774/838 [03:18<00:16,  3.90it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  93% 776/838 [03:18<00:15,  3.91it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  93% 778/838 [03:18<00:15,  3.91it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  93% 780/838 [03:18<00:14,  3.92it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  93% 782/838 [03:19<00:14,  3.93it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  94% 784/838 [03:19<00:13,  3.93it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  94% 786/838 [03:19<00:13,  3.94it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  94% 788/838 [03:19<00:12,  3.95it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  94% 790/838 [03:19<00:12,  3.95it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  95% 792/838 [03:20<00:11,  3.96it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  95% 794/838 [03:20<00:11,  3.97it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  95% 796/838 [03:20<00:10,  3.97it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  95% 798/838 [03:20<00:10,  3.98it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  95% 800/838 [03:20<00:09,  3.99it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  96% 802/838 [03:20<00:09,  3.99it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  96% 804/838 [03:21<00:08,  4.00it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  96% 806/838 [03:21<00:07,  4.01it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  96% 808/838 [03:21<00:07,  4.01it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  97% 810/838 [03:21<00:06,  4.02it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  97% 812/838 [03:21<00:06,  4.03it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  97% 814/838 [03:21<00:05,  4.03it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  97% 816/838 [03:22<00:05,  4.04it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  98% 818/838 [03:22<00:04,  4.04it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  98% 820/838 [03:22<00:04,  4.05it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  98% 822/838 [03:22<00:03,  4.06it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  98% 824/838 [03:22<00:03,  4.06it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  99% 826/838 [03:22<00:02,  4.07it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  99% 828/838 [03:23<00:02,  4.08it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  99% 830/838 [03:23<00:01,  4.08it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0:  99% 832/838 [03:23<00:01,  4.09it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0: 100% 834/838 [03:23<00:00,  4.10it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0: 100% 836/838 [03:23<00:00,  4.10it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0: 100% 838/838 [03:23<00:00,  4.11it/s, loss=0.425, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1524.0\n",
            "eval_fp: 1054.0\n",
            "eval_fn: 822.0\n",
            "eval_tn: 15185.0\n",
            "eval_precision: 0.5911559462547302\n",
            "eval_recall: 0.6496163606643677\n",
            "eval_f1: 0.6190089583396912\n",
            "eval_mcc: 0.5618086457252502\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1269497856 bytes == 0x7f40c8dde000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1586872320 bytes == 0x12ac72000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967cd 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1983594496 bytes == 0x7f4052a2a000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967cd 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2479497216 bytes == 0x12ac72000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 3099377664 bytes == 0x7f4052a2a000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 838/838 [03:49<00:00,  3.66it/s, loss=0.425, v_num=7520]\n",
            "Epoch 0: 100% 838/838 [03:49<00:00,  3.64it/s, loss=0.425, v_num=7520]\n",
            "Epoch 1:  87% 727/838 [03:16<00:30,  3.70it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 728/838 [03:17<00:29,  3.69it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  87% 730/838 [03:17<00:29,  3.70it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  87% 732/838 [03:17<00:28,  3.71it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 734/838 [03:17<00:28,  3.71it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 736/838 [03:17<00:27,  3.72it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 738/838 [03:18<00:26,  3.73it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 740/838 [03:18<00:26,  3.73it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 742/838 [03:18<00:25,  3.74it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 744/838 [03:18<00:25,  3.75it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 746/838 [03:18<00:24,  3.75it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 748/838 [03:18<00:23,  3.76it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 750/838 [03:19<00:23,  3.77it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 752/838 [03:19<00:22,  3.77it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 754/838 [03:19<00:22,  3.78it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 756/838 [03:19<00:21,  3.79it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 758/838 [03:19<00:21,  3.80it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 760/838 [03:19<00:20,  3.80it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 762/838 [03:20<00:19,  3.81it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 764/838 [03:20<00:19,  3.82it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 766/838 [03:20<00:18,  3.82it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 768/838 [03:20<00:18,  3.83it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 770/838 [03:20<00:17,  3.84it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 772/838 [03:20<00:17,  3.84it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 774/838 [03:21<00:16,  3.85it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 776/838 [03:21<00:16,  3.86it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 778/838 [03:21<00:15,  3.86it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 780/838 [03:21<00:14,  3.87it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 782/838 [03:21<00:14,  3.88it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 784/838 [03:21<00:13,  3.88it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 786/838 [03:22<00:13,  3.89it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 788/838 [03:22<00:12,  3.90it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 790/838 [03:22<00:12,  3.90it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 792/838 [03:22<00:11,  3.91it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 794/838 [03:22<00:11,  3.92it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 796/838 [03:22<00:10,  3.92it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 798/838 [03:23<00:10,  3.93it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 800/838 [03:23<00:09,  3.93it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 802/838 [03:23<00:09,  3.94it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 804/838 [03:23<00:08,  3.95it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 806/838 [03:23<00:08,  3.95it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 808/838 [03:23<00:07,  3.96it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 810/838 [03:24<00:07,  3.97it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 812/838 [03:24<00:06,  3.97it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 814/838 [03:24<00:06,  3.98it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 816/838 [03:24<00:05,  3.99it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 818/838 [03:24<00:05,  3.99it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 820/838 [03:25<00:04,  4.00it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 822/838 [03:25<00:03,  4.01it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 824/838 [03:25<00:03,  4.01it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 826/838 [03:25<00:02,  4.02it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 828/838 [03:25<00:02,  4.03it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 830/838 [03:25<00:01,  4.03it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 832/838 [03:26<00:01,  4.04it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1: 100% 834/838 [03:26<00:00,  4.04it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1: 100% 836/838 [03:26<00:00,  4.05it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Epoch 1: 100% 838/838 [03:26<00:00,  4.06it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1651.0\n",
            "eval_fp: 1355.0\n",
            "eval_fn: 695.0\n",
            "eval_tn: 14884.0\n",
            "eval_precision: 0.5492348670959473\n",
            "eval_recall: 0.7037510871887207\n",
            "eval_f1: 0.6169655919075012\n",
            "eval_mcc: 0.559484601020813\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 838/838 [03:26<00:00,  4.05it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 838/838 [03:27<00:00,  4.05it/s, loss=0.451, v_num=7520, train_loss=0.612]\n",
            "Computing Input\n",
            "100% 5814/5814 [00:09<00:00, 641.48it/s]\n",
            "100% 885/885 [00:01<00:00, 627.12it/s]\n",
            "100% 77/77 [00:00<00:00, 700.97it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.32it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 116.0\n",
            "fp: 147.0\n",
            "fn: 63.0\n",
            "tn: 1291.0\n",
            "precision: 0.44106462597846985\n",
            "recall: 0.6480447053909302\n",
            "f1: 0.5248869061470032\n",
            "mcc: 0.46405312418937683\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0250), 'test_loss': tensor(0.3147, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.00it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:11<00:00, 509.47it/s]\n",
            "100% 885/885 [00:01<00:00, 560.38it/s]\n",
            "100% 77/77 [00:00<00:00, 610.66it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.17it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 9.0\n",
            "eval_fp: 66.0\n",
            "eval_fn: 13.0\n",
            "eval_tn: 80.0\n",
            "eval_precision: 0.11999999731779099\n",
            "eval_recall: 0.40909090638160706\n",
            "eval_f1: 0.1855670064687729\n",
            "eval_mcc: -0.029155414551496506\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 724/835 [03:14<00:29,  3.73it/s, loss=0.478, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 725/835 [03:14<00:29,  3.73it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  87% 727/835 [03:14<00:28,  3.74it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  87% 729/835 [03:14<00:28,  3.74it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 731/835 [03:14<00:27,  3.75it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 733/835 [03:15<00:27,  3.76it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 735/835 [03:15<00:26,  3.76it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 737/835 [03:15<00:25,  3.77it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 739/835 [03:15<00:25,  3.78it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 741/835 [03:15<00:24,  3.78it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 743/835 [03:15<00:24,  3.79it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 745/835 [03:16<00:23,  3.80it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 747/835 [03:16<00:23,  3.80it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 749/835 [03:16<00:22,  3.81it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 751/835 [03:16<00:21,  3.82it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 753/835 [03:16<00:21,  3.83it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 755/835 [03:17<00:20,  3.83it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 757/835 [03:17<00:20,  3.84it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 759/835 [03:17<00:19,  3.85it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 761/835 [03:17<00:19,  3.85it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 763/835 [03:17<00:18,  3.86it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 765/835 [03:17<00:18,  3.87it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 767/835 [03:18<00:17,  3.87it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 769/835 [03:18<00:17,  3.88it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 771/835 [03:18<00:16,  3.89it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 773/835 [03:18<00:15,  3.89it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 775/835 [03:18<00:15,  3.90it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 777/835 [03:18<00:14,  3.91it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 779/835 [03:19<00:14,  3.91it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 781/835 [03:19<00:13,  3.92it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 783/835 [03:19<00:13,  3.93it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 785/835 [03:19<00:12,  3.93it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 787/835 [03:19<00:12,  3.94it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 789/835 [03:19<00:11,  3.95it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 791/835 [03:20<00:11,  3.95it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 793/835 [03:20<00:10,  3.96it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 795/835 [03:20<00:10,  3.97it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 797/835 [03:20<00:09,  3.97it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 799/835 [03:20<00:09,  3.98it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 801/835 [03:21<00:08,  3.98it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 803/835 [03:21<00:08,  3.99it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 805/835 [03:21<00:07,  4.00it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 807/835 [03:21<00:06,  4.00it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 809/835 [03:21<00:06,  4.01it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 811/835 [03:22<00:05,  4.01it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 813/835 [03:22<00:05,  4.02it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 815/835 [03:22<00:04,  4.03it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 817/835 [03:22<00:04,  4.03it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 819/835 [03:22<00:03,  4.04it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 821/835 [03:22<00:03,  4.05it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 823/835 [03:23<00:02,  4.05it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 825/835 [03:23<00:02,  4.06it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 827/835 [03:23<00:01,  4.07it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 829/835 [03:23<00:01,  4.07it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 831/835 [03:23<00:00,  4.08it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 833/835 [03:23<00:00,  4.09it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 835/835 [03:24<00:00,  4.09it/s, loss=0.478, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1672.0\n",
            "eval_fp: 1601.0\n",
            "eval_fn: 641.0\n",
            "eval_tn: 14671.0\n",
            "eval_precision: 0.5108463168144226\n",
            "eval_recall: 0.7228707075119019\n",
            "eval_f1: 0.5986394286155701\n",
            "eval_mcc: 0.541175901889801\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1999405056 bytes == 0x14dccc000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2499256320 bytes == 0x7f4052a2a000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 3124076544 bytes == 0x7f3f15ca6000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 835/835 [03:50<00:00,  3.62it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 835/835 [03:52<00:00,  3.59it/s, loss=0.478, v_num=7520]\n",
            "Epoch 1:  87% 724/835 [03:16<00:30,  3.68it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 725/835 [03:16<00:29,  3.68it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  87% 727/835 [03:17<00:29,  3.69it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  87% 729/835 [03:17<00:28,  3.70it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  88% 731/835 [03:17<00:28,  3.70it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  88% 733/835 [03:17<00:27,  3.71it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  88% 735/835 [03:17<00:26,  3.72it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  88% 737/835 [03:17<00:26,  3.72it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  89% 739/835 [03:18<00:25,  3.73it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  89% 741/835 [03:18<00:25,  3.74it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  89% 743/835 [03:18<00:24,  3.74it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  89% 745/835 [03:18<00:23,  3.75it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  89% 747/835 [03:18<00:23,  3.76it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  90% 749/835 [03:18<00:22,  3.77it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  90% 751/835 [03:19<00:22,  3.77it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  90% 753/835 [03:19<00:21,  3.78it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  90% 755/835 [03:19<00:21,  3.79it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  91% 757/835 [03:19<00:20,  3.79it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  91% 759/835 [03:19<00:20,  3.80it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  91% 761/835 [03:19<00:19,  3.81it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  91% 763/835 [03:20<00:18,  3.81it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  92% 765/835 [03:20<00:18,  3.82it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  92% 767/835 [03:20<00:17,  3.83it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  92% 769/835 [03:20<00:17,  3.83it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  92% 771/835 [03:20<00:16,  3.84it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  93% 773/835 [03:20<00:16,  3.85it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  93% 775/835 [03:21<00:15,  3.85it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  93% 777/835 [03:21<00:15,  3.86it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  93% 779/835 [03:21<00:14,  3.87it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  94% 781/835 [03:21<00:13,  3.87it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  94% 783/835 [03:21<00:13,  3.88it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  94% 785/835 [03:22<00:12,  3.89it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  94% 787/835 [03:22<00:12,  3.89it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  94% 789/835 [03:22<00:11,  3.90it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  95% 791/835 [03:22<00:11,  3.91it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  95% 793/835 [03:22<00:10,  3.91it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  95% 795/835 [03:22<00:10,  3.92it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  95% 797/835 [03:23<00:09,  3.93it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  96% 799/835 [03:23<00:09,  3.93it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  96% 801/835 [03:23<00:08,  3.94it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  96% 803/835 [03:23<00:08,  3.95it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  96% 805/835 [03:23<00:07,  3.95it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  97% 807/835 [03:23<00:07,  3.96it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  97% 809/835 [03:24<00:06,  3.96it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  97% 811/835 [03:24<00:06,  3.97it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  97% 813/835 [03:24<00:05,  3.98it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  98% 815/835 [03:24<00:05,  3.98it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  98% 817/835 [03:24<00:04,  3.99it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  98% 819/835 [03:24<00:04,  4.00it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  98% 821/835 [03:25<00:03,  4.00it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  99% 823/835 [03:25<00:02,  4.01it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  99% 825/835 [03:25<00:02,  4.02it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  99% 827/835 [03:25<00:01,  4.02it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1:  99% 829/835 [03:25<00:01,  4.03it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1: 100% 831/835 [03:25<00:00,  4.04it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1: 100% 833/835 [03:26<00:00,  4.04it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Epoch 1: 100% 835/835 [03:26<00:00,  4.05it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1725.0\n",
            "eval_fp: 1610.0\n",
            "eval_fn: 588.0\n",
            "eval_tn: 14662.0\n",
            "eval_precision: 0.517241358757019\n",
            "eval_recall: 0.7457846999168396\n",
            "eval_f1: 0.6108356714248657\n",
            "eval_mcc: 0.5564465522766113\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 835/835 [03:26<00:00,  4.04it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 835/835 [03:26<00:00,  4.04it/s, loss=0.482, v_num=7520, train_loss=0.614]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:09<00:00, 629.92it/s]\n",
            "100% 885/885 [00:01<00:00, 630.27it/s]\n",
            "100% 77/77 [00:00<00:00, 668.57it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.74it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 126.0\n",
            "fp: 166.0\n",
            "fn: 53.0\n",
            "tn: 1272.0\n",
            "precision: 0.43150684237480164\n",
            "recall: 0.7039105892181396\n",
            "f1: 0.5350318551063538\n",
            "mcc: 0.4799906313419342\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0125), 'test_loss': tensor(0.3362, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.82it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 6271/6271 [00:12<00:00, 507.82it/s]\n",
            "100% 885/885 [00:01<00:00, 499.50it/s]\n",
            "100% 77/77 [00:00<00:00, 541.55it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.19it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 15.0\n",
            "eval_fp: 73.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 73.0\n",
            "eval_precision: 0.17045454680919647\n",
            "eval_recall: 0.6818181872367859\n",
            "eval_f1: 0.27272728085517883\n",
            "eval_mcc: 0.12281142175197601\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  88% 784/895 [03:30<00:29,  3.73it/s, loss=0.478, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 785/895 [03:30<00:29,  3.72it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 787/895 [03:30<00:28,  3.73it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 789/895 [03:31<00:28,  3.74it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  88% 791/895 [03:31<00:27,  3.74it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 793/895 [03:31<00:27,  3.75it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 795/895 [03:31<00:26,  3.76it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 797/895 [03:31<00:26,  3.76it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 799/895 [03:31<00:25,  3.77it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  89% 801/895 [03:32<00:24,  3.78it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 803/895 [03:32<00:24,  3.78it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 805/895 [03:32<00:23,  3.79it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 807/895 [03:32<00:23,  3.80it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  90% 809/895 [03:32<00:22,  3.80it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 811/895 [03:32<00:22,  3.81it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 813/895 [03:33<00:21,  3.81it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 815/895 [03:33<00:20,  3.82it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  91% 817/895 [03:33<00:20,  3.83it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 819/895 [03:33<00:19,  3.83it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 821/895 [03:33<00:19,  3.84it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 823/895 [03:34<00:18,  3.85it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 825/895 [03:34<00:18,  3.85it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  92% 827/895 [03:34<00:17,  3.86it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 829/895 [03:34<00:17,  3.86it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 831/895 [03:34<00:16,  3.87it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 833/895 [03:34<00:15,  3.88it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  93% 835/895 [03:35<00:15,  3.88it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 837/895 [03:35<00:14,  3.89it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 839/895 [03:35<00:14,  3.90it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 841/895 [03:35<00:13,  3.90it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 843/895 [03:35<00:13,  3.91it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  94% 845/895 [03:35<00:12,  3.91it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 847/895 [03:36<00:12,  3.92it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 849/895 [03:36<00:11,  3.93it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 851/895 [03:36<00:11,  3.93it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  95% 853/895 [03:36<00:10,  3.94it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 855/895 [03:36<00:10,  3.94it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 857/895 [03:36<00:09,  3.95it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 859/895 [03:37<00:09,  3.96it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 861/895 [03:37<00:08,  3.96it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  96% 863/895 [03:37<00:08,  3.97it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 865/895 [03:37<00:07,  3.98it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 867/895 [03:37<00:07,  3.98it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 869/895 [03:37<00:06,  3.99it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  97% 871/895 [03:38<00:06,  3.99it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 873/895 [03:38<00:05,  4.00it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 875/895 [03:38<00:04,  4.01it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 877/895 [03:38<00:04,  4.01it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 879/895 [03:38<00:03,  4.02it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  98% 881/895 [03:38<00:03,  4.02it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 883/895 [03:39<00:02,  4.03it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 885/895 [03:39<00:02,  4.04it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 887/895 [03:39<00:01,  4.04it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0:  99% 889/895 [03:39<00:01,  4.05it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 891/895 [03:39<00:00,  4.05it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 893/895 [03:39<00:00,  4.06it/s, loss=0.478, v_num=7520]\n",
            "Epoch 0: 100% 895/895 [03:40<00:00,  4.07it/s, loss=0.478, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1728.0\n",
            "eval_fp: 1707.0\n",
            "eval_fn: 600.0\n",
            "eval_tn: 14550.0\n",
            "eval_precision: 0.5030567646026611\n",
            "eval_recall: 0.7422680258750916\n",
            "eval_f1: 0.5996876358985901\n",
            "eval_mcc: 0.5434548854827881\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 3083083776 bytes == 0x7f4052a2a000 @  0x7f44ad304615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f44a9761950 0x7f44a9765bf7 0x7f44a9a967e8 0x7f44a9a4c1b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "\n",
            "Epoch 0: 100% 895/895 [04:08<00:00,  3.61it/s, loss=0.478, v_num=7520]\n",
            "Epoch 1:  88% 784/895 [03:33<00:30,  3.68it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 785/895 [03:33<00:29,  3.68it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  88% 787/895 [03:33<00:29,  3.68it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  88% 789/895 [03:33<00:28,  3.69it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  88% 791/895 [03:34<00:28,  3.70it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  89% 793/895 [03:34<00:27,  3.70it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  89% 795/895 [03:34<00:26,  3.71it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  89% 797/895 [03:34<00:26,  3.72it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  89% 799/895 [03:34<00:25,  3.72it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  89% 801/895 [03:34<00:25,  3.73it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  90% 803/895 [03:35<00:24,  3.73it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  90% 805/895 [03:35<00:24,  3.74it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  90% 807/895 [03:35<00:23,  3.75it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  90% 809/895 [03:35<00:22,  3.75it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  91% 811/895 [03:35<00:22,  3.76it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  91% 813/895 [03:35<00:21,  3.77it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  91% 815/895 [03:36<00:21,  3.77it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  91% 817/895 [03:36<00:20,  3.78it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  92% 819/895 [03:36<00:20,  3.78it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  92% 821/895 [03:36<00:19,  3.79it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  92% 823/895 [03:36<00:18,  3.80it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  92% 825/895 [03:36<00:18,  3.80it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  92% 827/895 [03:37<00:17,  3.81it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  93% 829/895 [03:37<00:17,  3.82it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  93% 831/895 [03:37<00:16,  3.82it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  93% 833/895 [03:37<00:16,  3.83it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  93% 835/895 [03:37<00:15,  3.83it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  94% 837/895 [03:37<00:15,  3.84it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  94% 839/895 [03:38<00:14,  3.85it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  94% 841/895 [03:38<00:14,  3.85it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  94% 843/895 [03:38<00:13,  3.86it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  94% 845/895 [03:38<00:12,  3.86it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  95% 847/895 [03:38<00:12,  3.87it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  95% 849/895 [03:38<00:11,  3.88it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  95% 851/895 [03:39<00:11,  3.88it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  95% 853/895 [03:39<00:10,  3.89it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  96% 855/895 [03:39<00:10,  3.90it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  96% 857/895 [03:39<00:09,  3.90it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  96% 859/895 [03:39<00:09,  3.91it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  96% 861/895 [03:40<00:08,  3.91it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  96% 863/895 [03:40<00:08,  3.92it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  97% 865/895 [03:40<00:07,  3.93it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  97% 867/895 [03:40<00:07,  3.93it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  97% 869/895 [03:40<00:06,  3.94it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  97% 871/895 [03:40<00:06,  3.94it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  98% 873/895 [03:41<00:05,  3.95it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  98% 875/895 [03:41<00:05,  3.96it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  98% 877/895 [03:41<00:04,  3.96it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  98% 879/895 [03:41<00:04,  3.97it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  98% 881/895 [03:41<00:03,  3.97it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  99% 883/895 [03:41<00:03,  3.98it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  99% 885/895 [03:42<00:02,  3.99it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  99% 887/895 [03:42<00:02,  3.99it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1:  99% 889/895 [03:42<00:01,  4.00it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1: 100% 891/895 [03:42<00:00,  4.00it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1: 100% 893/895 [03:42<00:00,  4.01it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Epoch 1: 100% 895/895 [03:42<00:00,  4.02it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1690.0\n",
            "eval_fp: 1460.0\n",
            "eval_fn: 638.0\n",
            "eval_tn: 14797.0\n",
            "eval_precision: 0.5365079641342163\n",
            "eval_recall: 0.725944995880127\n",
            "eval_f1: 0.6170135140419006\n",
            "eval_mcc: 0.5612471103668213\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Epoch 1: 100% 895/895 [04:09<00:00,  3.59it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 895/895 [04:09<00:00,  3.59it/s, loss=0.475, v_num=7520, train_loss=0.62]\n",
            "Computing Input\n",
            "100% 6271/6271 [00:11<00:00, 565.24it/s]\n",
            "100% 885/885 [00:01<00:00, 513.46it/s]\n",
            "100% 77/77 [00:00<00:00, 517.81it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.46it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 124.0\n",
            "fp: 156.0\n",
            "fn: 55.0\n",
            "tn: 1282.0\n",
            "precision: 0.44285714626312256\n",
            "recall: 0.6927374005317688\n",
            "f1: 0.5403050184249878\n",
            "mcc: 0.4844651222229004\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0500), 'test_loss': tensor(0.3257, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:02<00:00,  4.79it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/3\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5882/5882 [00:14<00:00, 403.05it/s]\n",
            "100% 885/885 [00:01<00:00, 478.41it/s]\n",
            "100% 77/77 [00:00<00:00, 481.99it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  1.59it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 7.0\n",
            "eval_fp: 80.0\n",
            "eval_fn: 11.0\n",
            "eval_tn: 70.0\n",
            "eval_precision: 0.08045977354049683\n",
            "eval_recall: 0.3888888955116272\n",
            "eval_f1: 0.13333334028720856\n",
            "eval_mcc: -0.08940886706113815\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 736/847 [03:18<00:29,  3.71it/s, loss=0.502, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 737/847 [03:18<00:29,  3.71it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  87% 739/847 [03:19<00:29,  3.71it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  87% 741/847 [03:19<00:28,  3.72it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  88% 743/847 [03:19<00:27,  3.73it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  88% 745/847 [03:19<00:27,  3.73it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  88% 747/847 [03:19<00:26,  3.74it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  88% 749/847 [03:19<00:26,  3.75it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  89% 751/847 [03:20<00:25,  3.75it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  89% 753/847 [03:20<00:24,  3.76it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  89% 755/847 [03:20<00:24,  3.77it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  89% 757/847 [03:20<00:23,  3.77it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  90% 759/847 [03:20<00:23,  3.78it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  90% 761/847 [03:20<00:22,  3.79it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  90% 763/847 [03:21<00:22,  3.79it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  90% 765/847 [03:21<00:21,  3.80it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  91% 767/847 [03:21<00:21,  3.81it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  91% 769/847 [03:21<00:20,  3.81it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  91% 771/847 [03:21<00:19,  3.82it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  91% 773/847 [03:21<00:19,  3.83it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  91% 775/847 [03:22<00:18,  3.83it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  92% 777/847 [03:22<00:18,  3.84it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  92% 779/847 [03:22<00:17,  3.85it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  92% 781/847 [03:22<00:17,  3.85it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  92% 783/847 [03:22<00:16,  3.86it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  93% 785/847 [03:22<00:16,  3.87it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  93% 787/847 [03:23<00:15,  3.87it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  93% 789/847 [03:23<00:14,  3.88it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  93% 791/847 [03:23<00:14,  3.89it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  94% 793/847 [03:23<00:13,  3.89it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  94% 795/847 [03:23<00:13,  3.90it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  94% 797/847 [03:23<00:12,  3.91it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  94% 799/847 [03:24<00:12,  3.91it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  95% 801/847 [03:24<00:11,  3.92it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  95% 803/847 [03:24<00:11,  3.93it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  95% 805/847 [03:24<00:10,  3.93it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  95% 807/847 [03:24<00:10,  3.94it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  96% 809/847 [03:25<00:09,  3.95it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  96% 811/847 [03:25<00:09,  3.95it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  96% 813/847 [03:25<00:08,  3.96it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  96% 815/847 [03:25<00:08,  3.97it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  96% 817/847 [03:25<00:07,  3.97it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  97% 819/847 [03:25<00:07,  3.98it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  97% 821/847 [03:26<00:06,  3.98it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  97% 823/847 [03:26<00:06,  3.99it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  97% 825/847 [03:26<00:05,  4.00it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  98% 827/847 [03:26<00:04,  4.00it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  98% 829/847 [03:26<00:04,  4.01it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  98% 831/847 [03:26<00:03,  4.02it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  98% 833/847 [03:27<00:03,  4.02it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  99% 835/847 [03:27<00:02,  4.03it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  99% 837/847 [03:27<00:02,  4.04it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  99% 839/847 [03:27<00:01,  4.04it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0:  99% 841/847 [03:27<00:01,  4.05it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0: 100% 843/847 [03:27<00:00,  4.05it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0: 100% 845/847 [03:28<00:00,  4.06it/s, loss=0.502, v_num=7520]\n",
            "Epoch 0: 100% 847/847 [03:28<00:00,  4.07it/s, loss=0.502, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1764.0\n",
            "eval_fp: 1789.0\n",
            "eval_fn: 590.0\n",
            "eval_tn: 14442.0\n",
            "eval_precision: 0.49648183584213257\n",
            "eval_recall: 0.7493627667427063\n",
            "eval_f1: 0.5972574949264526\n",
            "eval_mcc: 0.5405887365341187\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Epoch 0: 100% 847/847 [03:55<00:00,  3.59it/s, loss=0.502, v_num=7520]\n",
            "Epoch 1:  87% 736/847 [03:20<00:30,  3.68it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 737/847 [03:20<00:29,  3.67it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  87% 739/847 [03:20<00:29,  3.68it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  87% 741/847 [03:20<00:28,  3.69it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  88% 743/847 [03:21<00:28,  3.69it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  88% 745/847 [03:21<00:27,  3.70it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  88% 747/847 [03:21<00:26,  3.71it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  88% 749/847 [03:21<00:26,  3.71it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  89% 751/847 [03:21<00:25,  3.72it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  89% 753/847 [03:21<00:25,  3.73it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  89% 755/847 [03:22<00:24,  3.73it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  89% 757/847 [03:22<00:24,  3.74it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  90% 759/847 [03:22<00:23,  3.75it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  90% 761/847 [03:22<00:22,  3.75it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  90% 763/847 [03:22<00:22,  3.76it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  90% 765/847 [03:23<00:21,  3.77it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  91% 767/847 [03:23<00:21,  3.77it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  91% 769/847 [03:23<00:20,  3.78it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  91% 771/847 [03:23<00:20,  3.79it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  91% 773/847 [03:23<00:19,  3.79it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  91% 775/847 [03:23<00:18,  3.80it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  92% 777/847 [03:24<00:18,  3.81it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  92% 779/847 [03:24<00:17,  3.81it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  92% 781/847 [03:24<00:17,  3.82it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  92% 783/847 [03:24<00:16,  3.83it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  93% 785/847 [03:24<00:16,  3.83it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  93% 787/847 [03:24<00:15,  3.84it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  93% 789/847 [03:25<00:15,  3.85it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  93% 791/847 [03:25<00:14,  3.85it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  94% 793/847 [03:25<00:13,  3.86it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  94% 795/847 [03:25<00:13,  3.87it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  94% 797/847 [03:25<00:12,  3.87it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  94% 799/847 [03:25<00:12,  3.88it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  95% 801/847 [03:26<00:11,  3.89it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  95% 803/847 [03:26<00:11,  3.89it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  95% 805/847 [03:26<00:10,  3.90it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  95% 807/847 [03:26<00:10,  3.91it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  96% 809/847 [03:26<00:09,  3.91it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  96% 811/847 [03:26<00:09,  3.92it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  96% 813/847 [03:27<00:08,  3.93it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  96% 815/847 [03:27<00:08,  3.93it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  96% 817/847 [03:27<00:07,  3.94it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  97% 819/847 [03:27<00:07,  3.94it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  97% 821/847 [03:27<00:06,  3.95it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  97% 823/847 [03:27<00:06,  3.96it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  97% 825/847 [03:28<00:05,  3.96it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  98% 827/847 [03:28<00:05,  3.97it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  98% 829/847 [03:28<00:04,  3.98it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  98% 831/847 [03:28<00:04,  3.98it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  98% 833/847 [03:28<00:03,  3.99it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  99% 835/847 [03:29<00:03,  3.99it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  99% 837/847 [03:29<00:02,  4.00it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  99% 839/847 [03:29<00:01,  4.01it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1:  99% 841/847 [03:29<00:01,  4.01it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1: 100% 843/847 [03:29<00:00,  4.02it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1: 100% 845/847 [03:29<00:00,  4.03it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1: 100% 847/847 [03:30<00:00,  4.03it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1748.0\n",
            "eval_fp: 1655.0\n",
            "eval_fn: 606.0\n",
            "eval_tn: 14576.0\n",
            "eval_precision: 0.5136644244194031\n",
            "eval_recall: 0.7425658702850342\n",
            "eval_f1: 0.6072607040405273\n",
            "eval_mcc: 0.5508934855461121\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 847/847 [03:55<00:00,  3.59it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Epoch 1: 100% 847/847 [03:56<00:00,  3.58it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 847/847 [03:56<00:00,  3.58it/s, loss=0.459, v_num=7520, train_loss=0.616]\n",
            "Computing Input\n",
            "100% 5882/5882 [00:10<00:00, 553.84it/s]\n",
            "100% 885/885 [00:01<00:00, 531.47it/s]\n",
            "100% 77/77 [00:00<00:00, 566.28it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  4.05it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 126.0\n",
            "fp: 175.0\n",
            "fn: 53.0\n",
            "tn: 1263.0\n",
            "precision: 0.41860464215278625\n",
            "recall: 0.7039105892181396\n",
            "f1: 0.5249999761581421\n",
            "mcc: 0.4693288803100586\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0250), 'test_loss': tensor(0.3352, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:02<00:00,  4.87it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/4\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 6138/6138 [00:14<00:00, 413.52it/s]\n",
            "100% 885/885 [00:01<00:00, 467.65it/s]\n",
            "100% 77/77 [00:00<00:00, 491.46it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  1.74it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 14.0\n",
            "eval_fp: 69.0\n",
            "eval_fn: 13.0\n",
            "eval_tn: 72.0\n",
            "eval_precision: 0.16867469251155853\n",
            "eval_recall: 0.5185185074806213\n",
            "eval_f1: 0.2545454502105713\n",
            "eval_mcc: 0.02141820266842842\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 768/879 [03:26<00:29,  3.71it/s, loss=0.471, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 769/879 [03:27<00:29,  3.71it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  88% 771/879 [03:27<00:29,  3.72it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  88% 773/879 [03:27<00:28,  3.72it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  88% 775/879 [03:27<00:27,  3.73it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  88% 777/879 [03:28<00:27,  3.73it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  89% 779/879 [03:28<00:26,  3.74it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  89% 781/879 [03:28<00:26,  3.75it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  89% 783/879 [03:28<00:25,  3.75it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  89% 785/879 [03:28<00:24,  3.76it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  90% 787/879 [03:28<00:24,  3.77it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  90% 789/879 [03:29<00:23,  3.77it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  90% 791/879 [03:29<00:23,  3.78it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  90% 793/879 [03:29<00:22,  3.79it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  90% 795/879 [03:29<00:22,  3.79it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  91% 797/879 [03:29<00:21,  3.80it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  91% 799/879 [03:29<00:21,  3.81it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  91% 801/879 [03:30<00:20,  3.81it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  91% 803/879 [03:30<00:19,  3.82it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  92% 805/879 [03:30<00:19,  3.83it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  92% 807/879 [03:30<00:18,  3.83it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  92% 809/879 [03:30<00:18,  3.84it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  92% 811/879 [03:30<00:17,  3.84it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  92% 813/879 [03:31<00:17,  3.85it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  93% 815/879 [03:31<00:16,  3.86it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  93% 817/879 [03:31<00:16,  3.86it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  93% 819/879 [03:31<00:15,  3.87it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  93% 821/879 [03:31<00:14,  3.88it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  94% 823/879 [03:31<00:14,  3.88it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  94% 825/879 [03:32<00:13,  3.89it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  94% 827/879 [03:32<00:13,  3.90it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  94% 829/879 [03:32<00:12,  3.90it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  95% 831/879 [03:32<00:12,  3.91it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  95% 833/879 [03:32<00:11,  3.91it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  95% 835/879 [03:32<00:11,  3.92it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  95% 837/879 [03:33<00:10,  3.93it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  95% 839/879 [03:33<00:10,  3.93it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  96% 841/879 [03:33<00:09,  3.94it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  96% 843/879 [03:33<00:09,  3.95it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  96% 845/879 [03:33<00:08,  3.95it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  96% 847/879 [03:34<00:08,  3.96it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  97% 849/879 [03:34<00:07,  3.96it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  97% 851/879 [03:34<00:07,  3.97it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  97% 853/879 [03:34<00:06,  3.98it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  97% 855/879 [03:34<00:06,  3.98it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  97% 857/879 [03:34<00:05,  3.99it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  98% 859/879 [03:35<00:05,  3.99it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  98% 861/879 [03:35<00:04,  4.00it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  98% 863/879 [03:35<00:03,  4.01it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  98% 865/879 [03:35<00:03,  4.01it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  99% 867/879 [03:35<00:02,  4.02it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  99% 869/879 [03:35<00:02,  4.03it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  99% 871/879 [03:36<00:01,  4.03it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0:  99% 873/879 [03:36<00:01,  4.04it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0: 100% 875/879 [03:36<00:00,  4.04it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0: 100% 877/879 [03:36<00:00,  4.05it/s, loss=0.471, v_num=7520]\n",
            "Epoch 0: 100% 879/879 [03:36<00:00,  4.06it/s, loss=0.471, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1684.0\n",
            "eval_fp: 1323.0\n",
            "eval_fn: 669.0\n",
            "eval_tn: 14909.0\n",
            "eval_precision: 0.5600265860557556\n",
            "eval_recall: 0.7156820893287659\n",
            "eval_f1: 0.6283581852912903\n",
            "eval_mcc: 0.5726437568664551\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Epoch 0: 100% 879/879 [04:03<00:00,  3.60it/s, loss=0.471, v_num=7520]\n",
            "Epoch 1:  87% 768/879 [03:28<00:30,  3.68it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 769/879 [03:29<00:29,  3.67it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 771/879 [03:29<00:29,  3.68it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 773/879 [03:29<00:28,  3.68it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 775/879 [03:29<00:28,  3.69it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  88% 777/879 [03:30<00:27,  3.70it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 779/879 [03:30<00:26,  3.70it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 781/879 [03:30<00:26,  3.71it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 783/879 [03:30<00:25,  3.72it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  89% 785/879 [03:30<00:25,  3.72it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 787/879 [03:30<00:24,  3.73it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 789/879 [03:31<00:24,  3.74it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 791/879 [03:31<00:23,  3.74it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 793/879 [03:31<00:22,  3.75it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  90% 795/879 [03:31<00:22,  3.76it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 797/879 [03:31<00:21,  3.76it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 799/879 [03:31<00:21,  3.77it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 801/879 [03:32<00:20,  3.78it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  91% 803/879 [03:32<00:20,  3.78it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 805/879 [03:32<00:19,  3.79it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 807/879 [03:32<00:18,  3.79it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 809/879 [03:32<00:18,  3.80it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 811/879 [03:33<00:17,  3.81it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  92% 813/879 [03:33<00:17,  3.81it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 815/879 [03:33<00:16,  3.82it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 817/879 [03:33<00:16,  3.83it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 819/879 [03:33<00:15,  3.83it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  93% 821/879 [03:33<00:15,  3.84it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 823/879 [03:34<00:14,  3.85it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 825/879 [03:34<00:14,  3.85it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 827/879 [03:34<00:13,  3.86it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  94% 829/879 [03:34<00:12,  3.86it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 831/879 [03:34<00:12,  3.87it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 833/879 [03:34<00:11,  3.88it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 835/879 [03:35<00:11,  3.88it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 837/879 [03:35<00:10,  3.89it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  95% 839/879 [03:35<00:10,  3.89it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 841/879 [03:35<00:09,  3.90it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 843/879 [03:35<00:09,  3.91it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 845/879 [03:35<00:08,  3.91it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  96% 847/879 [03:36<00:08,  3.92it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 849/879 [03:36<00:07,  3.93it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 851/879 [03:36<00:07,  3.93it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 853/879 [03:36<00:06,  3.94it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 855/879 [03:36<00:06,  3.94it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  97% 857/879 [03:36<00:05,  3.95it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 859/879 [03:37<00:05,  3.96it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 861/879 [03:37<00:04,  3.96it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 863/879 [03:37<00:04,  3.97it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  98% 865/879 [03:37<00:03,  3.97it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 867/879 [03:37<00:03,  3.98it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 869/879 [03:37<00:02,  3.99it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 871/879 [03:38<00:02,  3.99it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1:  99% 873/879 [03:38<00:01,  4.00it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1: 100% 875/879 [03:38<00:00,  4.00it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1: 100% 877/879 [03:38<00:00,  4.01it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Epoch 1: 100% 879/879 [03:38<00:00,  4.02it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1743.0\n",
            "eval_fp: 1421.0\n",
            "eval_fn: 610.0\n",
            "eval_tn: 14811.0\n",
            "eval_precision: 0.5508849620819092\n",
            "eval_recall: 0.7407564520835876\n",
            "eval_f1: 0.6318651437759399\n",
            "eval_mcc: 0.5779330730438232\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 879/879 [03:39<00:00,  4.01it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 879/879 [03:39<00:00,  4.01it/s, loss=0.455, v_num=7520, train_loss=0.612]\n",
            "Computing Input\n",
            "100% 6138/6138 [00:09<00:00, 614.07it/s]\n",
            "100% 885/885 [00:01<00:00, 624.56it/s]\n",
            "100% 77/77 [00:00<00:00, 696.18it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.31it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 129.0\n",
            "fp: 152.0\n",
            "fn: 50.0\n",
            "tn: 1286.0\n",
            "precision: 0.4590747356414795\n",
            "recall: 0.7206704020500183\n",
            "f1: 0.560869574546814\n",
            "mcc: 0.5092160701751709\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0500), 'test_loss': tensor(0.3181, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.15it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHhTRGyRmciz",
        "outputId": "3c61fc1d-2c05-48b1-bc7a-44f5410cefe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 2 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--cls_hidden_size 107520 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-12 17:05:48.091771: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/0\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5814/5814 [00:13<00:00, 417.23it/s]\n",
            "100% 885/885 [00:02<00:00, 433.43it/s]\n",
            "100% 77/77 [00:00<00:00, 462.64it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.10it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 86.0\n",
            "eval_fn: 11.0\n",
            "eval_tn: 61.0\n",
            "eval_precision: 0.1041666641831398\n",
            "eval_recall: 0.4761904776096344\n",
            "eval_f1: 0.17094017565250397\n",
            "eval_mcc: -0.07273929566144943\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 727/838 [03:16<00:29,  3.71it/s, loss=0.417, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 728/838 [03:16<00:29,  3.70it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  87% 730/838 [03:16<00:29,  3.71it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  87% 732/838 [03:16<00:28,  3.72it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  88% 734/838 [03:17<00:27,  3.73it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  88% 736/838 [03:17<00:27,  3.73it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  88% 738/838 [03:17<00:26,  3.74it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  88% 740/838 [03:17<00:26,  3.75it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  89% 742/838 [03:17<00:25,  3.75it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  89% 744/838 [03:17<00:25,  3.76it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  89% 746/838 [03:18<00:24,  3.77it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  89% 748/838 [03:18<00:23,  3.77it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  89% 750/838 [03:18<00:23,  3.78it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  90% 752/838 [03:18<00:22,  3.79it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  90% 754/838 [03:18<00:22,  3.79it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  90% 756/838 [03:18<00:21,  3.80it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  90% 758/838 [03:19<00:21,  3.81it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  91% 760/838 [03:19<00:20,  3.81it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  91% 762/838 [03:19<00:19,  3.82it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  91% 764/838 [03:19<00:19,  3.83it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  91% 766/838 [03:19<00:18,  3.83it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  92% 768/838 [03:19<00:18,  3.84it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  92% 770/838 [03:20<00:17,  3.85it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  92% 772/838 [03:20<00:17,  3.85it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  92% 774/838 [03:20<00:16,  3.86it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  93% 776/838 [03:20<00:16,  3.87it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  93% 778/838 [03:20<00:15,  3.87it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  93% 780/838 [03:20<00:14,  3.88it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  93% 782/838 [03:21<00:14,  3.89it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  94% 784/838 [03:21<00:13,  3.89it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  94% 786/838 [03:21<00:13,  3.90it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  94% 788/838 [03:21<00:12,  3.91it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  94% 790/838 [03:21<00:12,  3.91it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  95% 792/838 [03:21<00:11,  3.92it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  95% 794/838 [03:22<00:11,  3.93it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  95% 796/838 [03:22<00:10,  3.93it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  95% 798/838 [03:22<00:10,  3.94it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  95% 800/838 [03:22<00:09,  3.95it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  96% 802/838 [03:22<00:09,  3.95it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  96% 804/838 [03:23<00:08,  3.96it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  96% 806/838 [03:23<00:08,  3.97it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  96% 808/838 [03:23<00:07,  3.97it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  97% 810/838 [03:23<00:07,  3.98it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  97% 812/838 [03:23<00:06,  3.99it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  97% 814/838 [03:23<00:06,  3.99it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  97% 816/838 [03:24<00:05,  4.00it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  98% 818/838 [03:24<00:04,  4.01it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  98% 820/838 [03:24<00:04,  4.01it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  98% 822/838 [03:24<00:03,  4.02it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  98% 824/838 [03:24<00:03,  4.03it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  99% 826/838 [03:24<00:02,  4.03it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  99% 828/838 [03:25<00:02,  4.04it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  99% 830/838 [03:25<00:01,  4.04it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0:  99% 832/838 [03:25<00:01,  4.05it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0: 100% 834/838 [03:25<00:00,  4.06it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0: 100% 836/838 [03:25<00:00,  4.06it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0: 100% 838/838 [03:25<00:00,  4.07it/s, loss=0.417, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1554.0\n",
            "eval_fp: 1063.0\n",
            "eval_fn: 792.0\n",
            "eval_tn: 15176.0\n",
            "eval_precision: 0.5938097238540649\n",
            "eval_recall: 0.6624041199684143\n",
            "eval_f1: 0.6262341141700745\n",
            "eval_mcc: 0.5699679255485535\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1185972224 bytes == 0x162a32000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1482465280 bytes == 0x7f3709a36000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1853087744 bytes == 0x12a160000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2316361728 bytes == 0x7f367f928000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2895454208 bytes == 0x12a160000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07cd 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 838/838 [03:51<00:00,  3.62it/s, loss=0.417, v_num=7520]\n",
            "Epoch 0: 100% 838/838 [03:53<00:00,  3.59it/s, loss=0.417, v_num=7520]\n",
            "Epoch 1:  87% 727/838 [03:18<00:30,  3.65it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 728/838 [03:19<00:30,  3.65it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  87% 730/838 [03:19<00:29,  3.66it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  87% 732/838 [03:19<00:28,  3.66it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 734/838 [03:19<00:28,  3.67it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 736/838 [03:20<00:27,  3.68it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 738/838 [03:20<00:27,  3.69it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 740/838 [03:20<00:26,  3.69it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 742/838 [03:20<00:25,  3.70it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 744/838 [03:20<00:25,  3.71it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 746/838 [03:20<00:24,  3.71it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 748/838 [03:21<00:24,  3.72it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 750/838 [03:21<00:23,  3.73it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 752/838 [03:21<00:23,  3.73it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 754/838 [03:21<00:22,  3.74it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 756/838 [03:21<00:21,  3.75it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 758/838 [03:21<00:21,  3.75it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 760/838 [03:22<00:20,  3.76it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 762/838 [03:22<00:20,  3.77it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 764/838 [03:22<00:19,  3.77it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 766/838 [03:22<00:19,  3.78it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 768/838 [03:22<00:18,  3.79it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 770/838 [03:23<00:17,  3.79it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 772/838 [03:23<00:17,  3.80it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 774/838 [03:23<00:16,  3.81it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 776/838 [03:23<00:16,  3.81it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 778/838 [03:23<00:15,  3.82it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 780/838 [03:23<00:15,  3.83it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 782/838 [03:24<00:14,  3.83it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 784/838 [03:24<00:14,  3.84it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 786/838 [03:24<00:13,  3.85it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 788/838 [03:24<00:12,  3.85it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 790/838 [03:24<00:12,  3.86it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 792/838 [03:24<00:11,  3.87it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 794/838 [03:25<00:11,  3.87it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 796/838 [03:25<00:10,  3.88it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 798/838 [03:25<00:10,  3.88it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 800/838 [03:25<00:09,  3.89it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 802/838 [03:25<00:09,  3.90it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 804/838 [03:25<00:08,  3.90it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 806/838 [03:26<00:08,  3.91it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 808/838 [03:26<00:07,  3.92it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 810/838 [03:26<00:07,  3.92it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 812/838 [03:26<00:06,  3.93it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 814/838 [03:26<00:06,  3.94it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 816/838 [03:26<00:05,  3.94it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 818/838 [03:27<00:05,  3.95it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 820/838 [03:27<00:04,  3.96it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 822/838 [03:27<00:04,  3.96it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 824/838 [03:27<00:03,  3.97it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 826/838 [03:27<00:03,  3.97it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 828/838 [03:27<00:02,  3.98it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 830/838 [03:28<00:02,  3.99it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 832/838 [03:28<00:01,  3.99it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 834/838 [03:28<00:00,  4.00it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 836/838 [03:28<00:00,  4.01it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 838/838 [03:28<00:00,  4.01it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1664.0\n",
            "eval_fp: 1305.0\n",
            "eval_fn: 682.0\n",
            "eval_tn: 14934.0\n",
            "eval_precision: 0.5604580640792847\n",
            "eval_recall: 0.7092924118041992\n",
            "eval_f1: 0.6261523962020874\n",
            "eval_mcc: 0.5701067447662354\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 838/838 [03:29<00:00,  4.01it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 838/838 [03:29<00:00,  4.00it/s, loss=0.446, v_num=7520, train_loss=0.598]\n",
            "Computing Input\n",
            "100% 5814/5814 [00:11<00:00, 501.40it/s]\n",
            "100% 885/885 [00:01<00:00, 505.04it/s]\n",
            "100% 77/77 [00:00<00:00, 478.99it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.31it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 111.0\n",
            "fp: 153.0\n",
            "fn: 68.0\n",
            "tn: 1285.0\n",
            "precision: 0.4204545319080353\n",
            "recall: 0.6201117038726807\n",
            "f1: 0.5011286735534668\n",
            "mcc: 0.436090350151062\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0375), 'test_loss': tensor(0.3178, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.93it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:13<00:00, 415.33it/s]\n",
            "100% 885/885 [00:02<00:00, 441.40it/s]\n",
            "100% 77/77 [00:00<00:00, 418.97it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  1.91it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 70.0\n",
            "eval_fn: 12.0\n",
            "eval_tn: 76.0\n",
            "eval_precision: 0.125\n",
            "eval_recall: 0.4545454680919647\n",
            "eval_f1: 0.19607843458652496\n",
            "eval_mcc: -0.016823481768369675\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 724/835 [03:15<00:30,  3.70it/s, loss=0.462, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 725/835 [03:16<00:29,  3.69it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  87% 727/835 [03:16<00:29,  3.70it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  87% 729/835 [03:16<00:28,  3.71it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  88% 731/835 [03:16<00:27,  3.71it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  88% 733/835 [03:16<00:27,  3.72it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  88% 735/835 [03:17<00:26,  3.73it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  88% 737/835 [03:17<00:26,  3.74it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  89% 739/835 [03:17<00:25,  3.74it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  89% 741/835 [03:17<00:25,  3.75it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  89% 743/835 [03:17<00:24,  3.76it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  89% 745/835 [03:17<00:23,  3.76it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  89% 747/835 [03:18<00:23,  3.77it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  90% 749/835 [03:18<00:22,  3.78it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  90% 751/835 [03:18<00:22,  3.78it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  90% 753/835 [03:18<00:21,  3.79it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  90% 755/835 [03:18<00:21,  3.80it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  91% 757/835 [03:18<00:20,  3.80it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  91% 759/835 [03:19<00:19,  3.81it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  91% 761/835 [03:19<00:19,  3.82it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  91% 763/835 [03:19<00:18,  3.82it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  92% 765/835 [03:19<00:18,  3.83it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  92% 767/835 [03:19<00:17,  3.84it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  92% 769/835 [03:20<00:17,  3.84it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  92% 771/835 [03:20<00:16,  3.85it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  93% 773/835 [03:20<00:16,  3.86it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  93% 775/835 [03:20<00:15,  3.86it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  93% 777/835 [03:20<00:14,  3.87it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  93% 779/835 [03:20<00:14,  3.88it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  94% 781/835 [03:21<00:13,  3.88it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  94% 783/835 [03:21<00:13,  3.89it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  94% 785/835 [03:21<00:12,  3.90it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  94% 787/835 [03:21<00:12,  3.90it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  94% 789/835 [03:21<00:11,  3.91it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  95% 791/835 [03:21<00:11,  3.92it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  95% 793/835 [03:22<00:10,  3.92it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  95% 795/835 [03:22<00:10,  3.93it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  95% 797/835 [03:22<00:09,  3.94it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  96% 799/835 [03:22<00:09,  3.94it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  96% 801/835 [03:22<00:08,  3.95it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  96% 803/835 [03:23<00:08,  3.95it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  96% 805/835 [03:23<00:07,  3.96it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  97% 807/835 [03:23<00:07,  3.97it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  97% 809/835 [03:23<00:06,  3.97it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  97% 811/835 [03:23<00:06,  3.98it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  97% 813/835 [03:24<00:05,  3.98it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  98% 815/835 [03:24<00:05,  3.99it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  98% 817/835 [03:24<00:04,  4.00it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  98% 819/835 [03:24<00:03,  4.00it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  98% 821/835 [03:24<00:03,  4.01it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  99% 823/835 [03:24<00:02,  4.02it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  99% 825/835 [03:25<00:02,  4.02it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  99% 827/835 [03:25<00:01,  4.03it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0:  99% 829/835 [03:25<00:01,  4.04it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0: 100% 831/835 [03:25<00:00,  4.04it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0: 100% 833/835 [03:25<00:00,  4.05it/s, loss=0.462, v_num=7520]\n",
            "Epoch 0: 100% 835/835 [03:25<00:00,  4.06it/s, loss=0.462, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1672.0\n",
            "eval_fp: 1482.0\n",
            "eval_fn: 641.0\n",
            "eval_tn: 14790.0\n",
            "eval_precision: 0.5301204919815063\n",
            "eval_recall: 0.7228707075119019\n",
            "eval_f1: 0.6116700172424316\n",
            "eval_mcc: 0.5555919408798218\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 2049458176 bytes == 0x7f367f928000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2561826816 bytes == 0x7f35634da000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 3202285568 bytes == 0x7f367f928000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "\n",
            "Epoch 0: 100% 835/835 [03:53<00:00,  3.57it/s, loss=0.462, v_num=7520]\n",
            "Epoch 1:  87% 724/835 [03:18<00:30,  3.65it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 725/835 [03:18<00:30,  3.65it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  87% 727/835 [03:19<00:29,  3.65it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  87% 729/835 [03:19<00:28,  3.66it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  88% 731/835 [03:19<00:28,  3.67it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  88% 733/835 [03:19<00:27,  3.67it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  88% 735/835 [03:19<00:27,  3.68it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  88% 737/835 [03:19<00:26,  3.69it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  89% 739/835 [03:20<00:25,  3.69it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  89% 741/835 [03:20<00:25,  3.70it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  89% 743/835 [03:20<00:24,  3.71it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  89% 745/835 [03:20<00:24,  3.72it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  89% 747/835 [03:20<00:23,  3.72it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  90% 749/835 [03:20<00:23,  3.73it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  90% 751/835 [03:21<00:22,  3.74it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  90% 753/835 [03:21<00:21,  3.74it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  90% 755/835 [03:21<00:21,  3.75it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  91% 757/835 [03:21<00:20,  3.76it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  91% 759/835 [03:21<00:20,  3.76it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  91% 761/835 [03:21<00:19,  3.77it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  91% 763/835 [03:22<00:19,  3.78it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  92% 765/835 [03:22<00:18,  3.78it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  92% 767/835 [03:22<00:17,  3.79it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  92% 769/835 [03:22<00:17,  3.80it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  92% 771/835 [03:22<00:16,  3.80it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  93% 773/835 [03:22<00:16,  3.81it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  93% 775/835 [03:23<00:15,  3.82it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  93% 777/835 [03:23<00:15,  3.82it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  93% 779/835 [03:23<00:14,  3.83it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  94% 781/835 [03:23<00:14,  3.84it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  94% 783/835 [03:23<00:13,  3.84it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  94% 785/835 [03:23<00:12,  3.85it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  94% 787/835 [03:24<00:12,  3.86it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  94% 789/835 [03:24<00:11,  3.86it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  95% 791/835 [03:24<00:11,  3.87it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  95% 793/835 [03:24<00:10,  3.88it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  95% 795/835 [03:24<00:10,  3.88it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  95% 797/835 [03:24<00:09,  3.89it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  96% 799/835 [03:25<00:09,  3.89it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  96% 801/835 [03:25<00:08,  3.90it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  96% 803/835 [03:25<00:08,  3.91it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  96% 805/835 [03:25<00:07,  3.91it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  97% 807/835 [03:25<00:07,  3.92it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  97% 809/835 [03:25<00:06,  3.93it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  97% 811/835 [03:26<00:06,  3.93it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  97% 813/835 [03:26<00:05,  3.94it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  98% 815/835 [03:26<00:05,  3.95it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  98% 817/835 [03:26<00:04,  3.95it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  98% 819/835 [03:26<00:04,  3.96it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  98% 821/835 [03:27<00:03,  3.97it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  99% 823/835 [03:27<00:03,  3.97it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  99% 825/835 [03:27<00:02,  3.98it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  99% 827/835 [03:27<00:02,  3.98it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1:  99% 829/835 [03:27<00:01,  3.99it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1: 100% 831/835 [03:27<00:01,  4.00it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1: 100% 833/835 [03:28<00:00,  4.00it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Epoch 1: 100% 835/835 [03:28<00:00,  4.01it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1735.0\n",
            "eval_fp: 1471.0\n",
            "eval_fn: 578.0\n",
            "eval_tn: 14801.0\n",
            "eval_precision: 0.5411728024482727\n",
            "eval_recall: 0.7501080632209778\n",
            "eval_f1: 0.6287370920181274\n",
            "eval_mcc: 0.5763865113258362\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 835/835 [03:28<00:00,  4.00it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 835/835 [03:28<00:00,  4.00it/s, loss=0.476, v_num=7520, train_loss=0.605]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:11<00:00, 502.82it/s]\n",
            "100% 885/885 [00:01<00:00, 496.94it/s]\n",
            "100% 77/77 [00:00<00:00, 515.17it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.30it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 125.0\n",
            "fp: 166.0\n",
            "fn: 54.0\n",
            "tn: 1272.0\n",
            "precision: 0.4295532703399658\n",
            "recall: 0.6983240246772766\n",
            "f1: 0.5319148898124695\n",
            "mcc: 0.47607049345970154\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0375), 'test_loss': tensor(0.3353, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.35it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 6271/6271 [00:14<00:00, 420.34it/s]\n",
            "100% 885/885 [00:02<00:00, 400.85it/s]\n",
            "100% 77/77 [00:00<00:00, 465.31it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.01it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 15.0\n",
            "eval_fp: 73.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 73.0\n",
            "eval_precision: 0.17045454680919647\n",
            "eval_recall: 0.6818181872367859\n",
            "eval_f1: 0.27272728085517883\n",
            "eval_mcc: 0.12281142175197601\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  88% 784/895 [03:32<00:30,  3.69it/s, loss=0.459, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 785/895 [03:32<00:29,  3.69it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  88% 787/895 [03:32<00:29,  3.70it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  88% 789/895 [03:33<00:28,  3.70it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  88% 791/895 [03:33<00:28,  3.71it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  89% 793/895 [03:33<00:27,  3.72it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  89% 795/895 [03:33<00:26,  3.72it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  89% 797/895 [03:33<00:26,  3.73it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  89% 799/895 [03:33<00:25,  3.74it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  89% 801/895 [03:34<00:25,  3.74it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  90% 803/895 [03:34<00:24,  3.75it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  90% 805/895 [03:34<00:23,  3.75it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  90% 807/895 [03:34<00:23,  3.76it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  90% 809/895 [03:34<00:22,  3.77it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  91% 811/895 [03:34<00:22,  3.77it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  91% 813/895 [03:35<00:21,  3.78it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  91% 815/895 [03:35<00:21,  3.79it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  91% 817/895 [03:35<00:20,  3.79it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  92% 819/895 [03:35<00:20,  3.80it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  92% 821/895 [03:35<00:19,  3.81it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  92% 823/895 [03:35<00:18,  3.81it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  92% 825/895 [03:36<00:18,  3.82it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  92% 827/895 [03:36<00:17,  3.82it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  93% 829/895 [03:36<00:17,  3.83it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  93% 831/895 [03:36<00:16,  3.84it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  93% 833/895 [03:36<00:16,  3.84it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  93% 835/895 [03:36<00:15,  3.85it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  94% 837/895 [03:37<00:15,  3.85it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  94% 839/895 [03:37<00:14,  3.86it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  94% 841/895 [03:37<00:13,  3.87it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  94% 843/895 [03:37<00:13,  3.87it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  94% 845/895 [03:37<00:12,  3.88it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  95% 847/895 [03:37<00:12,  3.89it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  95% 849/895 [03:38<00:11,  3.89it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  95% 851/895 [03:38<00:11,  3.90it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  95% 853/895 [03:38<00:10,  3.90it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  96% 855/895 [03:38<00:10,  3.91it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  96% 857/895 [03:38<00:09,  3.92it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  96% 859/895 [03:39<00:09,  3.92it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  96% 861/895 [03:39<00:08,  3.93it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  96% 863/895 [03:39<00:08,  3.93it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  97% 865/895 [03:39<00:07,  3.94it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  97% 867/895 [03:39<00:07,  3.95it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  97% 869/895 [03:39<00:06,  3.95it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  97% 871/895 [03:40<00:06,  3.96it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  98% 873/895 [03:40<00:05,  3.96it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  98% 875/895 [03:40<00:05,  3.97it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  98% 877/895 [03:40<00:04,  3.98it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  98% 879/895 [03:40<00:04,  3.98it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  98% 881/895 [03:40<00:03,  3.99it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  99% 883/895 [03:41<00:03,  3.99it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  99% 885/895 [03:41<00:02,  4.00it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  99% 887/895 [03:41<00:01,  4.01it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0:  99% 889/895 [03:41<00:01,  4.01it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0: 100% 891/895 [03:41<00:00,  4.02it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0: 100% 893/895 [03:41<00:00,  4.02it/s, loss=0.459, v_num=7520]\n",
            "Epoch 0: 100% 895/895 [03:42<00:00,  4.03it/s, loss=0.459, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1740.0\n",
            "eval_fp: 1622.0\n",
            "eval_fn: 588.0\n",
            "eval_tn: 14635.0\n",
            "eval_precision: 0.5175490975379944\n",
            "eval_recall: 0.7474226951599121\n",
            "eval_f1: 0.611599326133728\n",
            "eval_mcc: 0.5569334030151367\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 2849079296 bytes == 0x7f367f928000 @  0x7f3af885e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f3af4cbb950 0x7f3af4cbfbf7 0x7f3af4ff07e8 0x7f3af4fa61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "\n",
            "Epoch 0: 100% 895/895 [04:08<00:00,  3.61it/s, loss=0.459, v_num=7520]\n",
            "Epoch 1:  88% 784/895 [03:35<00:30,  3.65it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 785/895 [03:35<00:30,  3.64it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 787/895 [03:35<00:29,  3.65it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 789/895 [03:35<00:29,  3.66it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 791/895 [03:36<00:28,  3.66it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 793/895 [03:36<00:27,  3.67it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 795/895 [03:36<00:27,  3.67it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 797/895 [03:36<00:26,  3.68it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 799/895 [03:36<00:26,  3.69it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 801/895 [03:36<00:25,  3.69it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 803/895 [03:37<00:24,  3.70it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 805/895 [03:37<00:24,  3.71it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 807/895 [03:37<00:23,  3.71it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 809/895 [03:37<00:23,  3.72it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 811/895 [03:37<00:22,  3.72it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 813/895 [03:37<00:21,  3.73it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 815/895 [03:38<00:21,  3.74it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 817/895 [03:38<00:20,  3.74it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 819/895 [03:38<00:20,  3.75it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 821/895 [03:38<00:19,  3.76it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 823/895 [03:38<00:19,  3.76it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 825/895 [03:38<00:18,  3.77it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 827/895 [03:39<00:18,  3.77it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 829/895 [03:39<00:17,  3.78it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 831/895 [03:39<00:16,  3.79it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 833/895 [03:39<00:16,  3.79it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 835/895 [03:39<00:15,  3.80it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 837/895 [03:39<00:15,  3.80it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 839/895 [03:40<00:14,  3.81it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 841/895 [03:40<00:14,  3.82it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 843/895 [03:40<00:13,  3.82it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 845/895 [03:40<00:13,  3.83it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 847/895 [03:40<00:12,  3.84it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 849/895 [03:41<00:11,  3.84it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 851/895 [03:41<00:11,  3.85it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 853/895 [03:41<00:10,  3.85it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 855/895 [03:41<00:10,  3.86it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 857/895 [03:41<00:09,  3.87it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 859/895 [03:41<00:09,  3.87it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 861/895 [03:42<00:08,  3.88it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 863/895 [03:42<00:08,  3.88it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 865/895 [03:42<00:07,  3.89it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 867/895 [03:42<00:07,  3.90it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 869/895 [03:42<00:06,  3.90it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 871/895 [03:42<00:06,  3.91it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 873/895 [03:43<00:05,  3.91it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 875/895 [03:43<00:05,  3.92it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 877/895 [03:43<00:04,  3.93it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 879/895 [03:43<00:04,  3.93it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 881/895 [03:43<00:03,  3.94it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 883/895 [03:43<00:03,  3.94it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 885/895 [03:44<00:02,  3.95it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 887/895 [03:44<00:02,  3.96it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 889/895 [03:44<00:01,  3.96it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 891/895 [03:44<00:01,  3.97it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 893/895 [03:44<00:00,  3.97it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 895/895 [03:44<00:00,  3.98it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1705.0\n",
            "eval_fp: 1454.0\n",
            "eval_fn: 623.0\n",
            "eval_tn: 14803.0\n",
            "eval_precision: 0.5397277474403381\n",
            "eval_recall: 0.7323883175849915\n",
            "eval_f1: 0.6214689016342163\n",
            "eval_mcc: 0.5666140913963318\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 895/895 [04:10<00:00,  3.57it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 895/895 [04:12<00:00,  3.55it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 895/895 [04:12<00:00,  3.55it/s, loss=0.477, v_num=7520, train_loss=0.598]\n",
            "Computing Input\n",
            "100% 6271/6271 [00:14<00:00, 436.69it/s]\n",
            "100% 885/885 [00:02<00:00, 424.45it/s]\n",
            "100% 77/77 [00:00<00:00, 404.20it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  4.00it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 120.0\n",
            "fp: 156.0\n",
            "fn: 59.0\n",
            "tn: 1282.0\n",
            "precision: 0.43478259444236755\n",
            "recall: 0.6703910827636719\n",
            "f1: 0.5274725556373596\n",
            "mcc: 0.4685991704463959\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0250), 'test_loss': tensor(0.3299, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:02<00:00,  4.40it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/3\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5882/5882 [00:17<00:00, 343.89it/s]\n",
            "100% 885/885 [00:02<00:00, 335.21it/s]\n",
            "100% 77/77 [00:00<00:00, 383.58it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  1.63it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 8.0\n",
            "eval_fp: 77.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 73.0\n",
            "eval_precision: 0.0941176488995552\n",
            "eval_recall: 0.4444444477558136\n",
            "eval_f1: 0.1553398072719574\n",
            "eval_mcc: -0.04261696711182594\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 736/847 [03:19<00:30,  3.69it/s, loss=0.472, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 737/847 [03:19<00:29,  3.69it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  87% 739/847 [03:20<00:29,  3.69it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  87% 741/847 [03:20<00:28,  3.70it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  88% 743/847 [03:20<00:28,  3.71it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  88% 745/847 [03:20<00:27,  3.71it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  88% 747/847 [03:20<00:26,  3.72it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  88% 749/847 [03:21<00:26,  3.73it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  89% 751/847 [03:21<00:25,  3.73it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  89% 753/847 [03:21<00:25,  3.74it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  89% 755/847 [03:21<00:24,  3.75it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  89% 757/847 [03:21<00:23,  3.75it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  90% 759/847 [03:21<00:23,  3.76it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  90% 761/847 [03:22<00:22,  3.77it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  90% 763/847 [03:22<00:22,  3.77it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  90% 765/847 [03:22<00:21,  3.78it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  91% 767/847 [03:22<00:21,  3.79it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  91% 769/847 [03:22<00:20,  3.79it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  91% 771/847 [03:22<00:19,  3.80it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  91% 773/847 [03:23<00:19,  3.81it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  91% 775/847 [03:23<00:18,  3.81it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  92% 777/847 [03:23<00:18,  3.82it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  92% 779/847 [03:23<00:17,  3.83it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  92% 781/847 [03:23<00:17,  3.83it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  92% 783/847 [03:23<00:16,  3.84it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  93% 785/847 [03:24<00:16,  3.85it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  93% 787/847 [03:24<00:15,  3.85it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  93% 789/847 [03:24<00:15,  3.86it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  93% 791/847 [03:24<00:14,  3.87it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  94% 793/847 [03:24<00:13,  3.87it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  94% 795/847 [03:24<00:13,  3.88it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  94% 797/847 [03:25<00:12,  3.89it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  94% 799/847 [03:25<00:12,  3.89it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  95% 801/847 [03:25<00:11,  3.90it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  95% 803/847 [03:25<00:11,  3.91it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  95% 805/847 [03:25<00:10,  3.91it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  95% 807/847 [03:25<00:10,  3.92it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  96% 809/847 [03:26<00:09,  3.92it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  96% 811/847 [03:26<00:09,  3.93it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  96% 813/847 [03:26<00:08,  3.94it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  96% 815/847 [03:26<00:08,  3.94it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  96% 817/847 [03:26<00:07,  3.95it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  97% 819/847 [03:26<00:07,  3.96it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  97% 821/847 [03:27<00:06,  3.96it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  97% 823/847 [03:27<00:06,  3.97it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  97% 825/847 [03:27<00:05,  3.98it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  98% 827/847 [03:27<00:05,  3.98it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  98% 829/847 [03:27<00:04,  3.99it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  98% 831/847 [03:28<00:04,  4.00it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  98% 833/847 [03:28<00:03,  4.00it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  99% 835/847 [03:28<00:02,  4.01it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  99% 837/847 [03:28<00:02,  4.01it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  99% 839/847 [03:28<00:01,  4.02it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0:  99% 841/847 [03:28<00:01,  4.03it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0: 100% 843/847 [03:29<00:00,  4.03it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0: 100% 845/847 [03:29<00:00,  4.04it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0: 100% 847/847 [03:29<00:00,  4.05it/s, loss=0.472, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1776.0\n",
            "eval_fp: 1732.0\n",
            "eval_fn: 578.0\n",
            "eval_tn: 14499.0\n",
            "eval_precision: 0.5062713623046875\n",
            "eval_recall: 0.7544605135917664\n",
            "eval_f1: 0.6059365272521973\n",
            "eval_mcc: 0.5505499839782715\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0: 100% 847/847 [03:54<00:00,  3.61it/s, loss=0.472, v_num=7520]\n",
            "Epoch 0: 100% 847/847 [03:56<00:00,  3.58it/s, loss=0.472, v_num=7520]\n",
            "Epoch 1:  87% 736/847 [03:21<00:30,  3.65it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 737/847 [03:21<00:30,  3.65it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  87% 739/847 [03:22<00:29,  3.66it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  87% 741/847 [03:22<00:28,  3.66it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 743/847 [03:22<00:28,  3.67it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 745/847 [03:22<00:27,  3.68it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 747/847 [03:22<00:27,  3.68it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  88% 749/847 [03:23<00:26,  3.69it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 751/847 [03:23<00:25,  3.70it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 753/847 [03:23<00:25,  3.70it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 755/847 [03:23<00:24,  3.71it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  89% 757/847 [03:23<00:24,  3.72it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 759/847 [03:23<00:23,  3.72it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 761/847 [03:24<00:23,  3.73it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 763/847 [03:24<00:22,  3.74it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  90% 765/847 [03:24<00:21,  3.74it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 767/847 [03:24<00:21,  3.75it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 769/847 [03:24<00:20,  3.76it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 771/847 [03:24<00:20,  3.76it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 773/847 [03:25<00:19,  3.77it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  91% 775/847 [03:25<00:19,  3.78it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 777/847 [03:25<00:18,  3.78it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 779/847 [03:25<00:17,  3.79it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 781/847 [03:25<00:17,  3.80it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  92% 783/847 [03:25<00:16,  3.80it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 785/847 [03:26<00:16,  3.81it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 787/847 [03:26<00:15,  3.82it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 789/847 [03:26<00:15,  3.82it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  93% 791/847 [03:26<00:14,  3.83it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 793/847 [03:26<00:14,  3.83it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 795/847 [03:26<00:13,  3.84it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 797/847 [03:27<00:12,  3.85it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  94% 799/847 [03:27<00:12,  3.85it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 801/847 [03:27<00:11,  3.86it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 803/847 [03:27<00:11,  3.87it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 805/847 [03:27<00:10,  3.87it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  95% 807/847 [03:27<00:10,  3.88it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 809/847 [03:28<00:09,  3.89it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 811/847 [03:28<00:09,  3.89it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 813/847 [03:28<00:08,  3.90it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 815/847 [03:28<00:08,  3.91it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  96% 817/847 [03:28<00:07,  3.91it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 819/847 [03:29<00:07,  3.92it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 821/847 [03:29<00:06,  3.92it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 823/847 [03:29<00:06,  3.93it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  97% 825/847 [03:29<00:05,  3.94it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 827/847 [03:29<00:05,  3.94it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 829/847 [03:29<00:04,  3.95it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 831/847 [03:30<00:04,  3.96it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  98% 833/847 [03:30<00:03,  3.96it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 835/847 [03:30<00:03,  3.97it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 837/847 [03:30<00:02,  3.98it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 839/847 [03:30<00:02,  3.98it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1:  99% 841/847 [03:30<00:01,  3.99it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 843/847 [03:31<00:01,  3.99it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 845/847 [03:31<00:00,  4.00it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 847/847 [03:31<00:00,  4.01it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1740.0\n",
            "eval_fp: 1510.0\n",
            "eval_fn: 614.0\n",
            "eval_tn: 14721.0\n",
            "eval_precision: 0.5353845953941345\n",
            "eval_recall: 0.7391673922538757\n",
            "eval_f1: 0.6209850311279297\n",
            "eval_mcc: 0.5657387375831604\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 847/847 [03:56<00:00,  3.57it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Epoch 1: 100% 847/847 [03:58<00:00,  3.55it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 847/847 [03:58<00:00,  3.55it/s, loss=0.449, v_num=7520, train_loss=0.598]\n",
            "Computing Input\n",
            "100% 5882/5882 [00:13<00:00, 436.96it/s]\n",
            "100% 885/885 [00:02<00:00, 419.66it/s]\n",
            "100% 77/77 [00:00<00:00, 401.35it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.07it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 124.0\n",
            "fp: 160.0\n",
            "fn: 55.0\n",
            "tn: 1278.0\n",
            "precision: 0.43661972880363464\n",
            "recall: 0.6927374005317688\n",
            "f1: 0.5356371402740479\n",
            "mcc: 0.47946879267692566\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0125), 'test_loss': tensor(0.3314, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:02<00:00,  4.61it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/4\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 6138/6138 [00:17<00:00, 341.08it/s]\n",
            "100% 885/885 [00:02<00:00, 379.74it/s]\n",
            "100% 77/77 [00:00<00:00, 330.57it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  1.75it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 15.0\n",
            "eval_fp: 72.0\n",
            "eval_fn: 12.0\n",
            "eval_tn: 69.0\n",
            "eval_precision: 0.17241379618644714\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.2631579041481018\n",
            "eval_mcc: 0.03301433473825455\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 768/879 [03:28<00:30,  3.68it/s, loss=0.460, v_num=7520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 769/879 [03:29<00:29,  3.68it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  88% 771/879 [03:29<00:29,  3.68it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  88% 773/879 [03:29<00:28,  3.69it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  88% 775/879 [03:29<00:28,  3.70it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  88% 777/879 [03:29<00:27,  3.70it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  89% 779/879 [03:30<00:26,  3.71it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  89% 781/879 [03:30<00:26,  3.72it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  89% 783/879 [03:30<00:25,  3.72it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  89% 785/879 [03:30<00:25,  3.73it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  90% 787/879 [03:30<00:24,  3.74it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  90% 789/879 [03:30<00:24,  3.74it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  90% 791/879 [03:31<00:23,  3.75it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  90% 793/879 [03:31<00:22,  3.75it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  90% 795/879 [03:31<00:22,  3.76it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  91% 797/879 [03:31<00:21,  3.77it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  91% 799/879 [03:31<00:21,  3.77it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  91% 801/879 [03:31<00:20,  3.78it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  91% 803/879 [03:32<00:20,  3.79it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  92% 805/879 [03:32<00:19,  3.79it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  92% 807/879 [03:32<00:18,  3.80it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  92% 809/879 [03:32<00:18,  3.81it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  92% 811/879 [03:32<00:17,  3.81it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  92% 813/879 [03:32<00:17,  3.82it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  93% 815/879 [03:33<00:16,  3.82it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  93% 817/879 [03:33<00:16,  3.83it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  93% 819/879 [03:33<00:15,  3.84it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  93% 821/879 [03:33<00:15,  3.84it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  94% 823/879 [03:33<00:14,  3.85it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  94% 825/879 [03:33<00:14,  3.86it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  94% 827/879 [03:34<00:13,  3.86it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  94% 829/879 [03:34<00:12,  3.87it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  95% 831/879 [03:34<00:12,  3.87it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  95% 833/879 [03:34<00:11,  3.88it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  95% 835/879 [03:34<00:11,  3.89it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  95% 837/879 [03:34<00:10,  3.89it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  95% 839/879 [03:35<00:10,  3.90it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  96% 841/879 [03:35<00:09,  3.91it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  96% 843/879 [03:35<00:09,  3.91it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  96% 845/879 [03:35<00:08,  3.92it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  96% 847/879 [03:35<00:08,  3.92it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  97% 849/879 [03:35<00:07,  3.93it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  97% 851/879 [03:36<00:07,  3.94it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  97% 853/879 [03:36<00:06,  3.94it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  97% 855/879 [03:36<00:06,  3.95it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  97% 857/879 [03:36<00:05,  3.96it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  98% 859/879 [03:36<00:05,  3.96it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  98% 861/879 [03:37<00:04,  3.97it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  98% 863/879 [03:37<00:04,  3.97it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  98% 865/879 [03:37<00:03,  3.98it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  99% 867/879 [03:37<00:03,  3.99it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  99% 869/879 [03:37<00:02,  3.99it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  99% 871/879 [03:37<00:02,  4.00it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0:  99% 873/879 [03:38<00:01,  4.00it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0: 100% 875/879 [03:38<00:00,  4.01it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0: 100% 877/879 [03:38<00:00,  4.02it/s, loss=0.460, v_num=7520]\n",
            "Epoch 0: 100% 879/879 [03:38<00:00,  4.02it/s, loss=0.460, v_num=7520]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1701.0\n",
            "eval_fp: 1252.0\n",
            "eval_fn: 652.0\n",
            "eval_tn: 14980.0\n",
            "eval_precision: 0.5760243535041809\n",
            "eval_recall: 0.7229069471359253\n",
            "eval_f1: 0.6411609649658203\n",
            "eval_mcc: 0.5874074101448059\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Epoch 0: 100% 879/879 [04:03<00:00,  3.61it/s, loss=0.460, v_num=7520]\n",
            "Epoch 1:  87% 768/879 [03:30<00:30,  3.64it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 769/879 [03:31<00:30,  3.64it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  88% 771/879 [03:31<00:29,  3.65it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  88% 773/879 [03:31<00:29,  3.65it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  88% 775/879 [03:31<00:28,  3.66it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  88% 777/879 [03:31<00:27,  3.67it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  89% 779/879 [03:32<00:27,  3.67it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  89% 781/879 [03:32<00:26,  3.68it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  89% 783/879 [03:32<00:26,  3.69it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  89% 785/879 [03:32<00:25,  3.69it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  90% 787/879 [03:32<00:24,  3.70it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  90% 789/879 [03:32<00:24,  3.70it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  90% 791/879 [03:33<00:23,  3.71it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  90% 793/879 [03:33<00:23,  3.72it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  90% 795/879 [03:33<00:22,  3.72it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  91% 797/879 [03:33<00:21,  3.73it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  91% 799/879 [03:33<00:21,  3.74it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  91% 801/879 [03:33<00:20,  3.74it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  91% 803/879 [03:34<00:20,  3.75it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  92% 805/879 [03:34<00:19,  3.76it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  92% 807/879 [03:34<00:19,  3.76it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  92% 809/879 [03:34<00:18,  3.77it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  92% 811/879 [03:34<00:18,  3.77it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  92% 813/879 [03:35<00:17,  3.78it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  93% 815/879 [03:35<00:16,  3.79it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  93% 817/879 [03:35<00:16,  3.79it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  93% 819/879 [03:35<00:15,  3.80it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  93% 821/879 [03:35<00:15,  3.81it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  94% 823/879 [03:35<00:14,  3.81it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  94% 825/879 [03:36<00:14,  3.82it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  94% 827/879 [03:36<00:13,  3.82it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  94% 829/879 [03:36<00:13,  3.83it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  95% 831/879 [03:36<00:12,  3.84it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  95% 833/879 [03:36<00:11,  3.84it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  95% 835/879 [03:36<00:11,  3.85it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  95% 837/879 [03:37<00:10,  3.86it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  95% 839/879 [03:37<00:10,  3.86it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  96% 841/879 [03:37<00:09,  3.87it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  96% 843/879 [03:37<00:09,  3.87it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  96% 845/879 [03:37<00:08,  3.88it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  96% 847/879 [03:37<00:08,  3.89it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  97% 849/879 [03:38<00:07,  3.89it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  97% 851/879 [03:38<00:07,  3.90it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  97% 853/879 [03:38<00:06,  3.90it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  97% 855/879 [03:38<00:06,  3.91it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  97% 857/879 [03:38<00:05,  3.92it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  98% 859/879 [03:38<00:05,  3.92it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  98% 861/879 [03:39<00:04,  3.93it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  98% 863/879 [03:39<00:04,  3.94it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  98% 865/879 [03:39<00:03,  3.94it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  99% 867/879 [03:39<00:03,  3.95it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  99% 869/879 [03:39<00:02,  3.95it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  99% 871/879 [03:39<00:02,  3.96it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1:  99% 873/879 [03:40<00:01,  3.97it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1: 100% 875/879 [03:40<00:01,  3.97it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1: 100% 877/879 [03:40<00:00,  3.98it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Epoch 1: 100% 879/879 [03:40<00:00,  3.98it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1760.0\n",
            "eval_fp: 1422.0\n",
            "eval_fn: 593.0\n",
            "eval_tn: 14810.0\n",
            "eval_precision: 0.5531112551689148\n",
            "eval_recall: 0.7479813098907471\n",
            "eval_f1: 0.6359530091285706\n",
            "eval_mcc: 0.5829561352729797\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 879/879 [03:40<00:00,  3.98it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 879/879 [03:41<00:00,  3.98it/s, loss=0.455, v_num=7520, train_loss=0.594]\n",
            "Computing Input\n",
            "100% 6138/6138 [00:12<00:00, 496.65it/s]\n",
            "100% 885/885 [00:01<00:00, 485.00it/s]\n",
            "100% 77/77 [00:00<00:00, 532.40it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.00it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 121.0\n",
            "fp: 153.0\n",
            "fn: 58.0\n",
            "tn: 1285.0\n",
            "precision: 0.4416058361530304\n",
            "recall: 0.6759776473045349\n",
            "f1: 0.5342163443565369\n",
            "mcc: 0.47637319564819336\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0500), 'test_loss': tensor(0.3224, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.01it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTKTX7nutUyu"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\" \\\n",
        "--summary_type \"cluster\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 3 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--loss_weight 4.0 \\\n",
        "--cls_hidden_size 768 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzitfNyYtK9I",
        "outputId": "0a38a09a-2cb5-4a89-e797-509adff89c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\" \\\n",
        "--summary_type \"cluster\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 2 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--loss_weight 3.5 \\\n",
        "--cls_hidden_size 107520 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-12 13:42:14.229421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"label\": null,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\",\n",
            "    \"cls_hidden_size\": 107520,\n",
            "    \"loss_weight\": 3.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 2,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:11<00:00, 512.77it/s]\n",
            "100% 885/885 [00:01<00:00, 561.77it/s]\n",
            "100% 77/77 [00:00<00:00, 591.77it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 84 M  \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.43it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 82.0\n",
            "eval_fn: 9.0\n",
            "eval_tn: 64.0\n",
            "eval_precision: 0.13684210181236267\n",
            "eval_recall: 0.5909090638160706\n",
            "eval_f1: 0.2222222238779068\n",
            "eval_mcc: 0.019916675984859467\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  87% 724/835 [03:14<00:29,  3.73it/s, loss=0.441, v_num=r-21]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 725/835 [03:14<00:29,  3.73it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  87% 727/835 [03:14<00:28,  3.73it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  87% 729/835 [03:14<00:28,  3.74it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  88% 731/835 [03:15<00:27,  3.75it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  88% 733/835 [03:15<00:27,  3.75it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  88% 735/835 [03:15<00:26,  3.76it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  88% 737/835 [03:15<00:26,  3.77it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  89% 739/835 [03:15<00:25,  3.77it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  89% 741/835 [03:15<00:24,  3.78it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  89% 743/835 [03:16<00:24,  3.79it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  89% 745/835 [03:16<00:23,  3.79it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  89% 747/835 [03:16<00:23,  3.80it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  90% 749/835 [03:16<00:22,  3.81it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  90% 751/835 [03:16<00:22,  3.82it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  90% 753/835 [03:16<00:21,  3.82it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  90% 755/835 [03:17<00:20,  3.83it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  91% 757/835 [03:17<00:20,  3.84it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  91% 759/835 [03:17<00:19,  3.84it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  91% 761/835 [03:17<00:19,  3.85it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  91% 763/835 [03:17<00:18,  3.86it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  92% 765/835 [03:18<00:18,  3.86it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  92% 767/835 [03:18<00:17,  3.87it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  92% 769/835 [03:18<00:17,  3.88it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  92% 771/835 [03:18<00:16,  3.88it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  93% 773/835 [03:18<00:15,  3.89it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  93% 775/835 [03:18<00:15,  3.90it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  93% 777/835 [03:19<00:14,  3.90it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  93% 779/835 [03:19<00:14,  3.91it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  94% 781/835 [03:19<00:13,  3.92it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  94% 783/835 [03:19<00:13,  3.92it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  94% 785/835 [03:19<00:12,  3.93it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  94% 787/835 [03:19<00:12,  3.94it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  94% 789/835 [03:20<00:11,  3.94it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  95% 791/835 [03:20<00:11,  3.95it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  95% 793/835 [03:20<00:10,  3.96it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  95% 795/835 [03:20<00:10,  3.96it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  95% 797/835 [03:20<00:09,  3.97it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  96% 799/835 [03:20<00:09,  3.98it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  96% 801/835 [03:21<00:08,  3.98it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  96% 803/835 [03:21<00:08,  3.99it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  96% 805/835 [03:21<00:07,  4.00it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  97% 807/835 [03:21<00:06,  4.00it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  97% 809/835 [03:21<00:06,  4.01it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  97% 811/835 [03:21<00:05,  4.02it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  97% 813/835 [03:22<00:05,  4.02it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  98% 815/835 [03:22<00:04,  4.03it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  98% 817/835 [03:22<00:04,  4.04it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  98% 819/835 [03:22<00:03,  4.04it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  98% 821/835 [03:22<00:03,  4.05it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  99% 823/835 [03:22<00:02,  4.05it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  99% 825/835 [03:23<00:02,  4.06it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  99% 827/835 [03:23<00:01,  4.07it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0:  99% 829/835 [03:23<00:01,  4.07it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0: 100% 831/835 [03:23<00:00,  4.08it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0: 100% 833/835 [03:23<00:00,  4.09it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0: 100% 835/835 [03:23<00:00,  4.09it/s, loss=0.441, v_num=r-21]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1700.0\n",
            "eval_fp: 1677.0\n",
            "eval_fn: 613.0\n",
            "eval_tn: 14595.0\n",
            "eval_precision: 0.5034053921699524\n",
            "eval_recall: 0.7349762320518494\n",
            "eval_f1: 0.5975395441055298\n",
            "eval_mcc: 0.5409609079360962\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1308090368 bytes == 0x12b60c000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1635115008 bytes == 0x7f9b6d130000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2043895808 bytes == 0x12b60c000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2554871808 bytes == 0x7f9ad4cac000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 3193593856 bytes == 0x7f9a15f06000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 835/835 [03:52<00:00,  3.59it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 0: 100% 835/835 [03:52<00:00,  3.59it/s, loss=0.441, v_num=r-21]\n",
            "Epoch 1:  87% 724/835 [03:18<00:30,  3.65it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 725/835 [03:18<00:30,  3.64it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  87% 727/835 [03:19<00:29,  3.65it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  87% 729/835 [03:19<00:28,  3.66it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  88% 731/835 [03:19<00:28,  3.67it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  88% 733/835 [03:19<00:27,  3.67it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  88% 735/835 [03:19<00:27,  3.68it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  88% 737/835 [03:19<00:26,  3.69it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  89% 739/835 [03:20<00:25,  3.69it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  89% 741/835 [03:20<00:25,  3.70it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  89% 743/835 [03:20<00:24,  3.71it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  89% 745/835 [03:20<00:24,  3.71it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  89% 747/835 [03:20<00:23,  3.72it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  90% 749/835 [03:20<00:23,  3.73it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  90% 751/835 [03:21<00:22,  3.73it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  90% 753/835 [03:21<00:21,  3.74it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  90% 755/835 [03:21<00:21,  3.75it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  91% 757/835 [03:21<00:20,  3.75it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  91% 759/835 [03:21<00:20,  3.76it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  91% 761/835 [03:22<00:19,  3.77it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  91% 763/835 [03:22<00:19,  3.77it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  92% 765/835 [03:22<00:18,  3.78it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  92% 767/835 [03:22<00:17,  3.79it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  92% 769/835 [03:22<00:17,  3.79it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  92% 771/835 [03:22<00:16,  3.80it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  93% 773/835 [03:23<00:16,  3.81it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  93% 775/835 [03:23<00:15,  3.81it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  93% 777/835 [03:23<00:15,  3.82it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  93% 779/835 [03:23<00:14,  3.83it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  94% 781/835 [03:23<00:14,  3.83it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  94% 783/835 [03:23<00:13,  3.84it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  94% 785/835 [03:24<00:12,  3.85it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  94% 787/835 [03:24<00:12,  3.85it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  94% 789/835 [03:24<00:11,  3.86it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  95% 791/835 [03:24<00:11,  3.87it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  95% 793/835 [03:24<00:10,  3.87it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  95% 795/835 [03:24<00:10,  3.88it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  95% 797/835 [03:25<00:09,  3.89it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  96% 799/835 [03:25<00:09,  3.89it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  96% 801/835 [03:25<00:08,  3.90it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  96% 803/835 [03:25<00:08,  3.91it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  96% 805/835 [03:25<00:07,  3.91it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  97% 807/835 [03:25<00:07,  3.92it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  97% 809/835 [03:26<00:06,  3.92it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  97% 811/835 [03:26<00:06,  3.93it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  97% 813/835 [03:26<00:05,  3.94it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  98% 815/835 [03:26<00:05,  3.94it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  98% 817/835 [03:26<00:04,  3.95it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  98% 819/835 [03:26<00:04,  3.96it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  98% 821/835 [03:27<00:03,  3.96it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  99% 823/835 [03:27<00:03,  3.97it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  99% 825/835 [03:27<00:02,  3.98it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  99% 827/835 [03:27<00:02,  3.98it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1:  99% 829/835 [03:27<00:01,  3.99it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1: 100% 831/835 [03:28<00:01,  3.99it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1: 100% 833/835 [03:28<00:00,  4.00it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1: 100% 835/835 [03:28<00:00,  4.01it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1569.0\n",
            "eval_fp: 1216.0\n",
            "eval_fn: 744.0\n",
            "eval_tn: 15056.0\n",
            "eval_precision: 0.5633752346038818\n",
            "eval_recall: 0.6783398389816284\n",
            "eval_f1: 0.6155354976654053\n",
            "eval_mcc: 0.5582435727119446\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 2043895808 bytes == 0x12b60c000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2554871808 bytes == 0x7f9a15f06000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 3193593856 bytes == 0x7f9ad4cac000 @  0x7f9f66bbc615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f9f63019950 0x7f9f6301dbf7 0x7f9f6334e7e8 0x7f9f633041b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 1: 100% 835/835 [03:54<00:00,  3.56it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Epoch 1: 100% 835/835 [03:55<00:00,  3.55it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 1: 100% 835/835 [03:55<00:00,  3.55it/s, loss=0.426, v_num=r-21, train_loss=0.612]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:10<00:00, 549.06it/s]\n",
            "100% 885/885 [00:01<00:00, 538.63it/s]\n",
            "100% 77/77 [00:00<00:00, 619.30it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  4.32it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 125.0\n",
            "fp: 135.0\n",
            "fn: 54.0\n",
            "tn: 1303.0\n",
            "precision: 0.48076921701431274\n",
            "recall: 0.6983240246772766\n",
            "f1: 0.5694760680198669\n",
            "mcc: 0.5162798166275024\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0750), 'test_loss': tensor(0.3060, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:02<00:00,  4.45it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WGZcwOWC9hz"
      },
      "source": [
        "### Economic Development Try Out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEHOCsTuSrmT"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"cluster\" \\\n",
        "--label \"ed\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 8 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo0qHKVESsH-"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--label \"ed\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 8 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYBoBocFrAUm",
        "outputId": "b8d9b80a-d685-4b3c-8244-fa4d21eaaa23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 1 \\\n",
        "--label \"ED\" \\\n",
        "--cls_hidden_size 1536 \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 8 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--loss_weight 4.0 \\\n",
        "--amp_level \"O1\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-14 09:11:04.336293: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"label\": \"ED\",\n",
            "    \"augment\": false,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\",\n",
            "    \"cls_hidden_size\": 1536,\n",
            "    \"loss_weight\": 4.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Loading Data without Augmentation...\n",
            "Computing Input\n",
            "100% 3539/3539 [00:07<00:00, 452.93it/s]\n",
            "100% 885/885 [00:01<00:00, 511.49it/s]\n",
            "100% 77/77 [00:00<00:00, 524.97it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 1 M   \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.88it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 2.0\n",
            "eval_fp: 6.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 0.0\n",
            "eval_precision: 0.25\n",
            "eval_recall: 1.0\n",
            "eval_f1: 0.4000000059604645\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  80% 443/554 [01:57<00:29,  3.77it/s, loss=0.473, v_num=nk-1]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 444/554 [01:57<00:29,  3.76it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  81% 446/554 [01:58<00:28,  3.77it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  81% 448/554 [01:58<00:27,  3.79it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  81% 450/554 [01:58<00:27,  3.80it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  82% 452/554 [01:58<00:26,  3.81it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  82% 454/554 [01:58<00:26,  3.82it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  82% 456/554 [01:58<00:25,  3.83it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  83% 458/554 [01:59<00:24,  3.84it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  83% 460/554 [01:59<00:24,  3.86it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  83% 462/554 [01:59<00:23,  3.87it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  84% 464/554 [01:59<00:23,  3.88it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  84% 466/554 [01:59<00:22,  3.89it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  84% 468/554 [01:59<00:22,  3.90it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  85% 470/554 [02:00<00:21,  3.91it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  85% 472/554 [02:00<00:20,  3.92it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  86% 474/554 [02:00<00:20,  3.93it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  86% 476/554 [02:00<00:19,  3.95it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  86% 478/554 [02:00<00:19,  3.96it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  87% 480/554 [02:00<00:18,  3.97it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  87% 482/554 [02:01<00:18,  3.98it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  87% 484/554 [02:01<00:17,  3.99it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  88% 486/554 [02:01<00:16,  4.00it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  88% 488/554 [02:01<00:16,  4.01it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  88% 490/554 [02:01<00:15,  4.02it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  89% 492/554 [02:02<00:15,  4.03it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  89% 494/554 [02:02<00:14,  4.04it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  90% 496/554 [02:02<00:14,  4.05it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  90% 498/554 [02:02<00:13,  4.07it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  90% 500/554 [02:02<00:13,  4.08it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  91% 502/554 [02:02<00:12,  4.09it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  91% 504/554 [02:03<00:12,  4.10it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  91% 506/554 [02:03<00:11,  4.11it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  92% 508/554 [02:03<00:11,  4.12it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  92% 510/554 [02:03<00:10,  4.13it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  92% 512/554 [02:03<00:10,  4.14it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  93% 514/554 [02:03<00:09,  4.15it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  93% 516/554 [02:04<00:09,  4.16it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  94% 518/554 [02:04<00:08,  4.17it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  94% 520/554 [02:04<00:08,  4.18it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  94% 522/554 [02:04<00:07,  4.19it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  95% 524/554 [02:04<00:07,  4.20it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  95% 526/554 [02:04<00:06,  4.21it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  95% 528/554 [02:05<00:06,  4.22it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  96% 530/554 [02:05<00:05,  4.23it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  96% 532/554 [02:05<00:05,  4.24it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  96% 534/554 [02:05<00:04,  4.25it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  97% 536/554 [02:05<00:04,  4.26it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  97% 538/554 [02:05<00:03,  4.27it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  97% 540/554 [02:06<00:03,  4.28it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  98% 542/554 [02:06<00:02,  4.30it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  98% 544/554 [02:06<00:02,  4.31it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  99% 546/554 [02:06<00:01,  4.32it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  99% 548/554 [02:06<00:01,  4.33it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0:  99% 550/554 [02:06<00:00,  4.34it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0: 100% 552/554 [02:07<00:00,  4.35it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 0: 100% 554/554 [02:07<00:00,  4.36it/s, loss=0.473, v_num=nk-1]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 1.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 93.0\n",
            "eval_tn: 788.0\n",
            "eval_precision: 0.25\n",
            "eval_recall: 0.010638297535479069\n",
            "eval_f1: 0.020408162847161293\n",
            "eval_mcc: 0.03144471347332001\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1283964928 bytes == 0x152b7c000 @  0x7fba1df0d615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fba1a36a950 0x7fba1a36ebf7 0x7fba1a69f7e8 0x7fba1a6551b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1604960256 bytes == 0x7fb67a564000 @  0x7fba1df0d615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fba1a36a950 0x7fba1a36ebf7 0x7fba1a69f7e8 0x7fba1a6551b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 554/554 [02:17<00:00,  4.02it/s, loss=0.473, v_num=nk-1]\n",
            "Epoch 1:  80% 443/554 [01:59<00:29,  3.72it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 444/554 [01:59<00:29,  3.72it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  81% 446/554 [01:59<00:28,  3.73it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  81% 448/554 [01:59<00:28,  3.74it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  81% 450/554 [01:59<00:27,  3.75it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  82% 452/554 [02:00<00:27,  3.76it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  82% 454/554 [02:00<00:26,  3.77it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  82% 456/554 [02:00<00:25,  3.79it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  83% 458/554 [02:00<00:25,  3.80it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  83% 460/554 [02:00<00:24,  3.81it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  83% 462/554 [02:00<00:24,  3.82it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  84% 464/554 [02:01<00:23,  3.83it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  84% 466/554 [02:01<00:22,  3.84it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  84% 468/554 [02:01<00:22,  3.85it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  85% 470/554 [02:01<00:21,  3.86it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  85% 472/554 [02:01<00:21,  3.87it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  86% 474/554 [02:01<00:20,  3.89it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  86% 476/554 [02:02<00:20,  3.90it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  86% 478/554 [02:02<00:19,  3.91it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  87% 480/554 [02:02<00:18,  3.92it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  87% 482/554 [02:02<00:18,  3.93it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  87% 484/554 [02:02<00:17,  3.94it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  88% 486/554 [02:02<00:17,  3.95it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  88% 488/554 [02:03<00:16,  3.96it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  88% 490/554 [02:03<00:16,  3.97it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  89% 492/554 [02:03<00:15,  3.98it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  89% 494/554 [02:03<00:15,  3.99it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  90% 496/554 [02:03<00:14,  4.01it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  90% 498/554 [02:03<00:13,  4.02it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  90% 500/554 [02:04<00:13,  4.03it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  91% 502/554 [02:04<00:12,  4.04it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  91% 504/554 [02:04<00:12,  4.05it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  91% 506/554 [02:04<00:11,  4.06it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  92% 508/554 [02:04<00:11,  4.07it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  92% 510/554 [02:04<00:10,  4.08it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  92% 512/554 [02:05<00:10,  4.09it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  93% 514/554 [02:05<00:09,  4.10it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  93% 516/554 [02:05<00:09,  4.11it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  94% 518/554 [02:05<00:08,  4.12it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  94% 520/554 [02:05<00:08,  4.13it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  94% 522/554 [02:06<00:07,  4.14it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  95% 524/554 [02:06<00:07,  4.15it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  95% 526/554 [02:06<00:06,  4.16it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  95% 528/554 [02:06<00:06,  4.17it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  96% 530/554 [02:06<00:05,  4.18it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  96% 532/554 [02:06<00:05,  4.19it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  96% 534/554 [02:07<00:04,  4.20it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  97% 536/554 [02:07<00:04,  4.21it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  97% 538/554 [02:07<00:03,  4.22it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  97% 540/554 [02:07<00:03,  4.23it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  98% 542/554 [02:07<00:02,  4.25it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  98% 544/554 [02:07<00:02,  4.26it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  99% 546/554 [02:08<00:01,  4.27it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  99% 548/554 [02:08<00:01,  4.28it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1:  99% 550/554 [02:08<00:00,  4.29it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1: 100% 552/554 [02:08<00:00,  4.30it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 1: 100% 554/554 [02:08<00:00,  4.31it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 84.0\n",
            "eval_fp: 247.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 544.0\n",
            "eval_precision: 0.2537764310836792\n",
            "eval_recall: 0.8936170339584351\n",
            "eval_f1: 0.3952941298484802\n",
            "eval_mcc: 0.3701894283294678\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 554/554 [02:08<00:00,  4.30it/s, loss=0.396, v_num=nk-1, train_loss=0.559]\n",
            "Epoch 2:  80% 443/554 [01:58<00:29,  3.74it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 444/554 [01:58<00:29,  3.73it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  81% 446/554 [01:59<00:28,  3.75it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  81% 448/554 [01:59<00:28,  3.76it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  81% 450/554 [01:59<00:27,  3.77it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  82% 452/554 [01:59<00:26,  3.78it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  82% 454/554 [01:59<00:26,  3.79it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  82% 456/554 [01:59<00:25,  3.80it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  83% 458/554 [02:00<00:25,  3.81it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  83% 460/554 [02:00<00:24,  3.82it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  83% 462/554 [02:00<00:23,  3.84it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  84% 464/554 [02:00<00:23,  3.85it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  84% 466/554 [02:00<00:22,  3.86it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  84% 468/554 [02:00<00:22,  3.87it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  85% 470/554 [02:01<00:21,  3.88it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  85% 472/554 [02:01<00:21,  3.89it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  86% 474/554 [02:01<00:20,  3.90it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  86% 476/554 [02:01<00:19,  3.91it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  86% 478/554 [02:01<00:19,  3.93it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  87% 480/554 [02:01<00:18,  3.94it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  87% 482/554 [02:02<00:18,  3.95it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  87% 484/554 [02:02<00:17,  3.96it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  88% 486/554 [02:02<00:17,  3.97it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  88% 488/554 [02:02<00:16,  3.98it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  88% 490/554 [02:02<00:16,  3.99it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  89% 492/554 [02:02<00:15,  4.00it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  89% 494/554 [02:03<00:14,  4.01it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  90% 496/554 [02:03<00:14,  4.02it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  90% 498/554 [02:03<00:13,  4.03it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  90% 500/554 [02:03<00:13,  4.04it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  91% 502/554 [02:03<00:12,  4.05it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  91% 504/554 [02:03<00:12,  4.07it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  91% 506/554 [02:04<00:11,  4.08it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  92% 508/554 [02:04<00:11,  4.09it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  92% 510/554 [02:04<00:10,  4.10it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  92% 512/554 [02:04<00:10,  4.11it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  93% 514/554 [02:04<00:09,  4.12it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  93% 516/554 [02:04<00:09,  4.13it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  94% 518/554 [02:05<00:08,  4.14it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  94% 520/554 [02:05<00:08,  4.15it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  94% 522/554 [02:05<00:07,  4.16it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  95% 524/554 [02:05<00:07,  4.17it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  95% 526/554 [02:05<00:06,  4.18it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  95% 528/554 [02:05<00:06,  4.19it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  96% 530/554 [02:06<00:05,  4.20it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  96% 532/554 [02:06<00:05,  4.21it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  96% 534/554 [02:06<00:04,  4.22it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  97% 536/554 [02:06<00:04,  4.23it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  97% 538/554 [02:06<00:03,  4.24it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  97% 540/554 [02:06<00:03,  4.25it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  98% 542/554 [02:07<00:02,  4.26it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  98% 544/554 [02:07<00:02,  4.27it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  99% 546/554 [02:07<00:01,  4.28it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  99% 548/554 [02:07<00:01,  4.29it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2:  99% 550/554 [02:07<00:00,  4.30it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2: 100% 552/554 [02:07<00:00,  4.31it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2: 100% 554/554 [02:08<00:00,  4.32it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 73.0\n",
            "eval_fp: 166.0\n",
            "eval_fn: 21.0\n",
            "eval_tn: 625.0\n",
            "eval_precision: 0.3054393231868744\n",
            "eval_recall: 0.7765957713127136\n",
            "eval_f1: 0.43843844532966614\n",
            "eval_mcc: 0.393293559551239\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1604960256 bytes == 0x1157e6000 @  0x7fba1df0d615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fba1a36a950 0x7fba1a36ebf7 0x7fba1a69f7e8 0x7fba1a6551b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 2: 100% 554/554 [02:18<00:00,  4.00it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 2: 100% 554/554 [02:18<00:00,  3.99it/s, loss=0.389, v_num=nk-1, train_loss=0.371]\n",
            "Epoch 3:  80% 443/554 [01:59<00:29,  3.71it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 444/554 [01:59<00:29,  3.71it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  81% 446/554 [01:59<00:29,  3.72it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  81% 448/554 [02:00<00:28,  3.73it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  81% 450/554 [02:00<00:27,  3.74it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  82% 452/554 [02:00<00:27,  3.75it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  82% 454/554 [02:00<00:26,  3.77it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  82% 456/554 [02:00<00:25,  3.78it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  83% 458/554 [02:00<00:25,  3.79it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  83% 460/554 [02:01<00:24,  3.80it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  83% 462/554 [02:01<00:24,  3.81it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  84% 464/554 [02:01<00:23,  3.82it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  84% 466/554 [02:01<00:22,  3.83it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  84% 468/554 [02:01<00:22,  3.84it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  85% 470/554 [02:01<00:21,  3.86it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  85% 472/554 [02:02<00:21,  3.87it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  86% 474/554 [02:02<00:20,  3.88it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  86% 476/554 [02:02<00:20,  3.89it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  86% 478/554 [02:02<00:19,  3.90it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  87% 480/554 [02:02<00:18,  3.91it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  87% 482/554 [02:02<00:18,  3.92it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  87% 484/554 [02:03<00:17,  3.93it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  88% 486/554 [02:03<00:17,  3.94it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  88% 488/554 [02:03<00:16,  3.95it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  88% 490/554 [02:03<00:16,  3.96it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  89% 492/554 [02:03<00:15,  3.98it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  89% 494/554 [02:03<00:15,  3.99it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  90% 496/554 [02:04<00:14,  4.00it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  90% 498/554 [02:04<00:13,  4.01it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  90% 500/554 [02:04<00:13,  4.02it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  91% 502/554 [02:04<00:12,  4.03it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  91% 504/554 [02:04<00:12,  4.04it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  91% 506/554 [02:04<00:11,  4.05it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  92% 508/554 [02:05<00:11,  4.06it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  92% 510/554 [02:05<00:10,  4.07it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  92% 512/554 [02:05<00:10,  4.08it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  93% 514/554 [02:05<00:09,  4.09it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  93% 516/554 [02:05<00:09,  4.10it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  94% 518/554 [02:05<00:08,  4.11it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  94% 520/554 [02:06<00:08,  4.12it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  94% 522/554 [02:06<00:07,  4.13it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  95% 524/554 [02:06<00:07,  4.14it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  95% 526/554 [02:06<00:06,  4.15it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  95% 528/554 [02:06<00:06,  4.16it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  96% 530/554 [02:06<00:05,  4.17it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  96% 532/554 [02:07<00:05,  4.18it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  96% 534/554 [02:07<00:04,  4.19it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  97% 536/554 [02:07<00:04,  4.21it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  97% 538/554 [02:07<00:03,  4.22it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  97% 540/554 [02:07<00:03,  4.23it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  98% 542/554 [02:07<00:02,  4.24it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  98% 544/554 [02:08<00:02,  4.25it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  99% 546/554 [02:08<00:01,  4.26it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  99% 548/554 [02:08<00:01,  4.27it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3:  99% 550/554 [02:08<00:00,  4.28it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3: 100% 552/554 [02:08<00:00,  4.29it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 3: 100% 554/554 [02:08<00:00,  4.30it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 73.0\n",
            "eval_fp: 144.0\n",
            "eval_fn: 21.0\n",
            "eval_tn: 647.0\n",
            "eval_precision: 0.33640551567077637\n",
            "eval_recall: 0.7765957713127136\n",
            "eval_f1: 0.469453364610672\n",
            "eval_mcc: 0.42581483721733093\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1604960256 bytes == 0x1157e6000 @  0x7fba1df0d615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fba1a36a950 0x7fba1a36ebf7 0x7fba1a69f7e8 0x7fba1a6551b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 3: 100% 554/554 [02:19<00:00,  3.96it/s, loss=0.323, v_num=nk-1, train_loss=0.337]\n",
            "Epoch 4:  80% 443/554 [01:59<00:29,  3.71it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 444/554 [02:00<00:29,  3.70it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  81% 446/554 [02:00<00:29,  3.71it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  81% 448/554 [02:00<00:28,  3.72it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  81% 450/554 [02:00<00:27,  3.73it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  82% 452/554 [02:00<00:27,  3.75it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  82% 454/554 [02:00<00:26,  3.76it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  82% 456/554 [02:01<00:26,  3.77it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  83% 458/554 [02:01<00:25,  3.78it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  83% 460/554 [02:01<00:24,  3.79it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  83% 462/554 [02:01<00:24,  3.80it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  84% 464/554 [02:01<00:23,  3.81it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  84% 466/554 [02:01<00:23,  3.82it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  84% 468/554 [02:02<00:22,  3.84it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  85% 470/554 [02:02<00:21,  3.85it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  85% 472/554 [02:02<00:21,  3.86it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  86% 474/554 [02:02<00:20,  3.87it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  86% 476/554 [02:02<00:20,  3.88it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  86% 478/554 [02:02<00:19,  3.89it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  87% 480/554 [02:03<00:18,  3.90it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  87% 482/554 [02:03<00:18,  3.91it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  87% 484/554 [02:03<00:17,  3.92it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  88% 486/554 [02:03<00:17,  3.93it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  88% 488/554 [02:03<00:16,  3.94it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  88% 490/554 [02:03<00:16,  3.96it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  89% 492/554 [02:04<00:15,  3.97it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  89% 494/554 [02:04<00:15,  3.98it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  90% 496/554 [02:04<00:14,  3.99it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  90% 498/554 [02:04<00:14,  4.00it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  90% 500/554 [02:04<00:13,  4.01it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  91% 502/554 [02:04<00:12,  4.02it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  91% 504/554 [02:05<00:12,  4.03it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  91% 506/554 [02:05<00:11,  4.04it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  92% 508/554 [02:05<00:11,  4.05it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  92% 510/554 [02:05<00:10,  4.06it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  92% 512/554 [02:05<00:10,  4.07it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  93% 514/554 [02:05<00:09,  4.08it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  93% 516/554 [02:06<00:09,  4.09it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  94% 518/554 [02:06<00:08,  4.10it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  94% 520/554 [02:06<00:08,  4.11it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  94% 522/554 [02:06<00:07,  4.12it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  95% 524/554 [02:06<00:07,  4.13it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  95% 526/554 [02:06<00:06,  4.14it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  95% 528/554 [02:07<00:06,  4.15it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  96% 530/554 [02:07<00:05,  4.16it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  96% 532/554 [02:07<00:05,  4.17it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  96% 534/554 [02:07<00:04,  4.18it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  97% 536/554 [02:07<00:04,  4.19it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  97% 538/554 [02:07<00:03,  4.21it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  97% 540/554 [02:08<00:03,  4.22it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  98% 542/554 [02:08<00:02,  4.23it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  98% 544/554 [02:08<00:02,  4.24it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  99% 546/554 [02:08<00:01,  4.25it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  99% 548/554 [02:08<00:01,  4.26it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4:  99% 550/554 [02:08<00:00,  4.27it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4: 100% 552/554 [02:09<00:00,  4.28it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 4: 100% 554/554 [02:09<00:00,  4.29it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 74.0\n",
            "eval_fp: 161.0\n",
            "eval_fn: 20.0\n",
            "eval_tn: 630.0\n",
            "eval_precision: 0.31489360332489014\n",
            "eval_recall: 0.7872340679168701\n",
            "eval_f1: 0.4498480260372162\n",
            "eval_mcc: 0.4072367250919342\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4: 100% 554/554 [02:09<00:00,  4.28it/s, loss=0.322, v_num=nk-1, train_loss=0.312]\n",
            "Epoch 5:  80% 443/554 [01:58<00:29,  3.72it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 444/554 [01:59<00:29,  3.72it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  81% 446/554 [01:59<00:28,  3.73it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  81% 448/554 [01:59<00:28,  3.74it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  81% 450/554 [01:59<00:27,  3.75it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  82% 452/554 [02:00<00:27,  3.76it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  82% 454/554 [02:00<00:26,  3.78it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  82% 456/554 [02:00<00:25,  3.79it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  83% 458/554 [02:00<00:25,  3.80it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  83% 460/554 [02:00<00:24,  3.81it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  83% 462/554 [02:00<00:24,  3.82it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  84% 464/554 [02:01<00:23,  3.83it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  84% 466/554 [02:01<00:22,  3.84it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  84% 468/554 [02:01<00:22,  3.85it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  85% 470/554 [02:01<00:21,  3.87it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  85% 472/554 [02:01<00:21,  3.88it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  86% 474/554 [02:01<00:20,  3.89it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  86% 476/554 [02:02<00:20,  3.90it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  86% 478/554 [02:02<00:19,  3.91it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  87% 480/554 [02:02<00:18,  3.92it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  87% 482/554 [02:02<00:18,  3.93it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  87% 484/554 [02:02<00:17,  3.94it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  88% 486/554 [02:02<00:17,  3.95it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  88% 488/554 [02:03<00:16,  3.96it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  88% 490/554 [02:03<00:16,  3.98it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  89% 492/554 [02:03<00:15,  3.99it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  89% 494/554 [02:03<00:15,  4.00it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  90% 496/554 [02:03<00:14,  4.01it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  90% 498/554 [02:03<00:13,  4.02it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  90% 500/554 [02:04<00:13,  4.03it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  91% 502/554 [02:04<00:12,  4.04it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  91% 504/554 [02:04<00:12,  4.05it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  91% 506/554 [02:04<00:11,  4.06it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  92% 508/554 [02:04<00:11,  4.07it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  92% 510/554 [02:04<00:10,  4.08it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  92% 512/554 [02:05<00:10,  4.09it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  93% 514/554 [02:05<00:09,  4.10it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  93% 516/554 [02:05<00:09,  4.11it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  94% 518/554 [02:05<00:08,  4.12it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  94% 520/554 [02:05<00:08,  4.13it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  94% 522/554 [02:05<00:07,  4.14it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  95% 524/554 [02:06<00:07,  4.15it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  95% 526/554 [02:06<00:06,  4.16it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  95% 528/554 [02:06<00:06,  4.17it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  96% 530/554 [02:06<00:05,  4.19it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  96% 532/554 [02:06<00:05,  4.20it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  96% 534/554 [02:06<00:04,  4.21it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  97% 536/554 [02:07<00:04,  4.22it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  97% 538/554 [02:07<00:03,  4.23it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  97% 540/554 [02:07<00:03,  4.24it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  98% 542/554 [02:07<00:02,  4.25it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  98% 544/554 [02:07<00:02,  4.26it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  99% 546/554 [02:07<00:01,  4.27it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  99% 548/554 [02:08<00:01,  4.28it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5:  99% 550/554 [02:08<00:00,  4.29it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5: 100% 552/554 [02:08<00:00,  4.30it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 5: 100% 554/554 [02:08<00:00,  4.31it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 67.0\n",
            "eval_fp: 104.0\n",
            "eval_fn: 27.0\n",
            "eval_tn: 687.0\n",
            "eval_precision: 0.39181286096572876\n",
            "eval_recall: 0.7127659320831299\n",
            "eval_f1: 0.505660355091095\n",
            "eval_mcc: 0.4536234438419342\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5: 100% 554/554 [02:19<00:00,  3.97it/s, loss=0.323, v_num=nk-1, train_loss=0.311]\n",
            "Epoch 6:  80% 443/554 [01:59<00:29,  3.71it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 444/554 [01:59<00:29,  3.70it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  81% 446/554 [02:00<00:29,  3.71it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  81% 448/554 [02:00<00:28,  3.72it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  81% 450/554 [02:00<00:27,  3.73it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  82% 452/554 [02:00<00:27,  3.75it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  82% 454/554 [02:00<00:26,  3.76it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  82% 456/554 [02:00<00:26,  3.77it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  83% 458/554 [02:01<00:25,  3.78it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  83% 460/554 [02:01<00:24,  3.79it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  83% 462/554 [02:01<00:24,  3.80it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  84% 464/554 [02:01<00:23,  3.81it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  84% 466/554 [02:01<00:23,  3.82it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  84% 468/554 [02:02<00:22,  3.84it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  85% 470/554 [02:02<00:21,  3.85it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  85% 472/554 [02:02<00:21,  3.86it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  86% 474/554 [02:02<00:20,  3.87it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  86% 476/554 [02:02<00:20,  3.88it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  86% 478/554 [02:02<00:19,  3.89it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  87% 480/554 [02:03<00:18,  3.90it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  87% 482/554 [02:03<00:18,  3.91it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  87% 484/554 [02:03<00:17,  3.92it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  88% 486/554 [02:03<00:17,  3.93it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  88% 488/554 [02:03<00:16,  3.95it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  88% 490/554 [02:03<00:16,  3.96it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  89% 492/554 [02:04<00:15,  3.97it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  89% 494/554 [02:04<00:15,  3.98it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  90% 496/554 [02:04<00:14,  3.99it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  90% 498/554 [02:04<00:14,  4.00it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  90% 500/554 [02:04<00:13,  4.01it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  91% 502/554 [02:04<00:12,  4.02it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  91% 504/554 [02:05<00:12,  4.03it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  91% 506/554 [02:05<00:11,  4.04it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  92% 508/554 [02:05<00:11,  4.05it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  92% 510/554 [02:05<00:10,  4.06it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  92% 512/554 [02:05<00:10,  4.07it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  93% 514/554 [02:05<00:09,  4.08it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  93% 516/554 [02:06<00:09,  4.09it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  94% 518/554 [02:06<00:08,  4.10it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  94% 520/554 [02:06<00:08,  4.12it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  94% 522/554 [02:06<00:07,  4.13it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  95% 524/554 [02:06<00:07,  4.14it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  95% 526/554 [02:06<00:06,  4.15it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  95% 528/554 [02:07<00:06,  4.16it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  96% 530/554 [02:07<00:05,  4.17it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  96% 532/554 [02:07<00:05,  4.18it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  96% 534/554 [02:07<00:04,  4.19it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  97% 536/554 [02:07<00:04,  4.20it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  97% 538/554 [02:07<00:03,  4.21it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  97% 540/554 [02:08<00:03,  4.22it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  98% 542/554 [02:08<00:02,  4.23it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  98% 544/554 [02:08<00:02,  4.24it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  99% 546/554 [02:08<00:01,  4.25it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  99% 548/554 [02:08<00:01,  4.26it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6:  99% 550/554 [02:08<00:00,  4.27it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6: 100% 552/554 [02:09<00:00,  4.28it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 6: 100% 554/554 [02:09<00:00,  4.29it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 29.0\n",
            "eval_fp: 17.0\n",
            "eval_fn: 65.0\n",
            "eval_tn: 774.0\n",
            "eval_precision: 0.6304348111152649\n",
            "eval_recall: 0.3085106313228607\n",
            "eval_f1: 0.41428571939468384\n",
            "eval_mcc: 0.3983847498893738\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6: 100% 554/554 [02:20<00:00,  3.94it/s, loss=0.215, v_num=nk-1, train_loss=0.305]\n",
            "Epoch 7:  80% 443/554 [01:59<00:30,  3.70it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 444/554 [02:00<00:29,  3.69it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  81% 446/554 [02:00<00:29,  3.70it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  81% 448/554 [02:00<00:28,  3.71it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  81% 450/554 [02:00<00:27,  3.72it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  82% 452/554 [02:00<00:27,  3.74it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  82% 454/554 [02:01<00:26,  3.75it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  82% 456/554 [02:01<00:26,  3.76it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  83% 458/554 [02:01<00:25,  3.77it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  83% 460/554 [02:01<00:24,  3.78it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  83% 462/554 [02:01<00:24,  3.79it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  84% 464/554 [02:01<00:23,  3.80it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  84% 466/554 [02:02<00:23,  3.81it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  84% 468/554 [02:02<00:22,  3.83it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  85% 470/554 [02:02<00:21,  3.84it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  85% 472/554 [02:02<00:21,  3.85it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  86% 474/554 [02:02<00:20,  3.86it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  86% 476/554 [02:03<00:20,  3.87it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  86% 478/554 [02:03<00:19,  3.88it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  87% 480/554 [02:03<00:19,  3.89it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  87% 482/554 [02:03<00:18,  3.90it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  87% 484/554 [02:03<00:17,  3.91it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  88% 486/554 [02:03<00:17,  3.92it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  88% 488/554 [02:04<00:16,  3.93it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  88% 490/554 [02:04<00:16,  3.95it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  89% 492/554 [02:04<00:15,  3.96it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  89% 494/554 [02:04<00:15,  3.97it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  90% 496/554 [02:04<00:14,  3.98it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  90% 498/554 [02:04<00:14,  3.99it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  90% 500/554 [02:05<00:13,  4.00it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  91% 502/554 [02:05<00:12,  4.01it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  91% 504/554 [02:05<00:12,  4.02it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  91% 506/554 [02:05<00:11,  4.03it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  92% 508/554 [02:05<00:11,  4.04it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  92% 510/554 [02:05<00:10,  4.05it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  92% 512/554 [02:06<00:10,  4.06it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  93% 514/554 [02:06<00:09,  4.07it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  93% 516/554 [02:06<00:09,  4.08it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  94% 518/554 [02:06<00:08,  4.09it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  94% 520/554 [02:06<00:08,  4.10it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  94% 522/554 [02:06<00:07,  4.11it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  95% 524/554 [02:07<00:07,  4.12it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  95% 526/554 [02:07<00:06,  4.13it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  95% 528/554 [02:07<00:06,  4.14it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  96% 530/554 [02:07<00:05,  4.15it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  96% 532/554 [02:07<00:05,  4.17it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  96% 534/554 [02:07<00:04,  4.18it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  97% 536/554 [02:08<00:04,  4.19it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  97% 538/554 [02:08<00:03,  4.20it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  97% 540/554 [02:08<00:03,  4.21it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  98% 542/554 [02:08<00:02,  4.22it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  98% 544/554 [02:08<00:02,  4.23it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  99% 546/554 [02:08<00:01,  4.24it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  99% 548/554 [02:09<00:01,  4.25it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7:  99% 550/554 [02:09<00:00,  4.26it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7: 100% 552/554 [02:09<00:00,  4.27it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Epoch 7: 100% 554/554 [02:09<00:00,  4.28it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 63.0\n",
            "eval_fp: 89.0\n",
            "eval_fn: 31.0\n",
            "eval_tn: 702.0\n",
            "eval_precision: 0.41447368264198303\n",
            "eval_recall: 0.6702127456665039\n",
            "eval_f1: 0.5121951103210449\n",
            "eval_mcc: 0.4555926024913788\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 554/554 [02:09<00:00,  4.27it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 554/554 [02:09<00:00,  4.26it/s, loss=0.235, v_num=nk-1, train_loss=0.298]\n",
            "Loading Data without Augmentation...\n",
            "Computing Input\n",
            "100% 3539/3539 [00:06<00:00, 535.22it/s]\n",
            "100% 885/885 [00:01<00:00, 526.57it/s]\n",
            "100% 77/77 [00:00<00:00, 570.95it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.27it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 5.0\n",
            "fp: 16.0\n",
            "fn: 3.0\n",
            "tn: 53.0\n",
            "precision: 0.2380952388048172\n",
            "recall: 0.625\n",
            "f1: 0.3448275923728943\n",
            "mcc: 0.2693311274051666\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.7550), 'test_loss': tensor(0.6392, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.14it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj0G857J00o0"
      },
      "source": [
        "### Urban Development Try Out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7rdZOoMkExj",
        "outputId": "9a2eae29-f8fb-4fdb-daef-6475cc7825dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python robsum_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--label \"ud\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 8 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-13 16:08:33.974746: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/0\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"label\": \"ud\",\n",
            "    \"augment\": false,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 768,\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 4367/4367 [00:09<00:00, 468.17it/s]\n",
            "100% 885/885 [00:01<00:00, 507.43it/s]\n",
            "100% 77/77 [00:00<00:00, 543.68it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 592 K \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.67it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 0.0\n",
            "eval_fp: 8.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 0.0\n",
            "eval_precision: 0.0\n",
            "eval_recall: 0\n",
            "eval_f1: 0\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  83% 546/657 [02:26<00:29,  3.73it/s, loss=0.195, v_num=-1-0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  83% 547/657 [02:26<00:29,  3.73it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  84% 549/657 [02:26<00:28,  3.73it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  84% 551/657 [02:27<00:28,  3.74it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  84% 553/657 [02:27<00:27,  3.75it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  84% 555/657 [02:27<00:27,  3.76it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  85% 557/657 [02:27<00:26,  3.77it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  85% 559/657 [02:27<00:25,  3.78it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  85% 561/657 [02:28<00:25,  3.79it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  86% 563/657 [02:28<00:24,  3.80it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  86% 565/657 [02:28<00:24,  3.81it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  86% 567/657 [02:28<00:23,  3.82it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  87% 569/657 [02:28<00:22,  3.83it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  87% 571/657 [02:28<00:22,  3.84it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  87% 573/657 [02:29<00:21,  3.85it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  88% 575/657 [02:29<00:21,  3.85it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  88% 577/657 [02:29<00:20,  3.86it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  88% 579/657 [02:29<00:20,  3.87it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  88% 581/657 [02:29<00:19,  3.88it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  89% 583/657 [02:29<00:19,  3.89it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  89% 585/657 [02:30<00:18,  3.90it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  89% 587/657 [02:30<00:17,  3.91it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  90% 589/657 [02:30<00:17,  3.92it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  90% 591/657 [02:30<00:16,  3.93it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  90% 593/657 [02:30<00:16,  3.93it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  91% 595/657 [02:30<00:15,  3.94it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  91% 597/657 [02:31<00:15,  3.95it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  91% 599/657 [02:31<00:14,  3.96it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  91% 601/657 [02:31<00:14,  3.97it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  92% 603/657 [02:31<00:13,  3.98it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  92% 605/657 [02:31<00:13,  3.99it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  92% 607/657 [02:31<00:12,  4.00it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  93% 609/657 [02:32<00:11,  4.00it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  93% 611/657 [02:32<00:11,  4.01it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  93% 613/657 [02:32<00:10,  4.02it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  94% 615/657 [02:32<00:10,  4.03it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  94% 617/657 [02:32<00:09,  4.04it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  94% 619/657 [02:32<00:09,  4.05it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  95% 621/657 [02:33<00:08,  4.06it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  95% 623/657 [02:33<00:08,  4.06it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  95% 625/657 [02:33<00:07,  4.07it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  95% 627/657 [02:33<00:07,  4.08it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  96% 629/657 [02:33<00:06,  4.09it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  96% 631/657 [02:33<00:06,  4.10it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  96% 633/657 [02:34<00:05,  4.11it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  97% 635/657 [02:34<00:05,  4.12it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  97% 637/657 [02:34<00:04,  4.12it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  97% 639/657 [02:34<00:04,  4.13it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  98% 641/657 [02:34<00:03,  4.14it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  98% 643/657 [02:34<00:03,  4.15it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  98% 645/657 [02:35<00:02,  4.16it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  98% 647/657 [02:35<00:02,  4.17it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  99% 649/657 [02:35<00:01,  4.17it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  99% 651/657 [02:35<00:01,  4.18it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0:  99% 653/657 [02:35<00:00,  4.19it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0: 100% 655/657 [02:35<00:00,  4.20it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 0: 100% 657/657 [02:36<00:00,  4.21it/s, loss=0.195, v_num=-1-0]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 12.0\n",
            "eval_fp: 2.0\n",
            "eval_fn: 5.0\n",
            "eval_tn: 866.0\n",
            "eval_precision: 0.8571428656578064\n",
            "eval_recall: 0.7058823704719543\n",
            "eval_f1: 0.774193525314331\n",
            "eval_mcc: 0.7739690542221069\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1316716544 bytes == 0x186992000 @  0x7f05c9c5e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f05c60bb950 0x7f05c60bfbf7 0x7f05c63f07e8 0x7f05c63a61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1645895680 bytes == 0x115958000 @  0x7f05c9c5e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f05c60bb950 0x7f05c60bfbf7 0x7f05c63f07e8 0x7f05c63a61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2057371648 bytes == 0x7f02055f0000 @  0x7f05c9c5e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f05c60bb950 0x7f05c60bfbf7 0x7f05c63f07e8 0x7f05c63a61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 657/657 [02:47<00:00,  3.91it/s, loss=0.195, v_num=-1-0]\n",
            "Epoch 1:  83% 546/657 [02:27<00:30,  3.69it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  83% 548/657 [02:28<00:29,  3.69it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  84% 550/657 [02:28<00:28,  3.70it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  84% 552/657 [02:28<00:28,  3.71it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  84% 554/657 [02:28<00:27,  3.72it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  85% 556/657 [02:29<00:27,  3.73it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  85% 558/657 [02:29<00:26,  3.74it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  85% 560/657 [02:29<00:25,  3.75it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  86% 562/657 [02:29<00:25,  3.76it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  86% 564/657 [02:29<00:24,  3.77it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  86% 566/657 [02:29<00:24,  3.77it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  86% 568/657 [02:30<00:23,  3.78it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  87% 570/657 [02:30<00:22,  3.79it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  87% 572/657 [02:30<00:22,  3.80it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  87% 574/657 [02:30<00:21,  3.81it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  88% 576/657 [02:30<00:21,  3.82it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  88% 578/657 [02:30<00:20,  3.83it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  88% 580/657 [02:31<00:20,  3.84it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  89% 582/657 [02:31<00:19,  3.85it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  89% 584/657 [02:31<00:18,  3.86it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  89% 586/657 [02:31<00:18,  3.86it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  89% 588/657 [02:31<00:17,  3.87it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  90% 590/657 [02:32<00:17,  3.88it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  90% 592/657 [02:32<00:16,  3.89it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  90% 594/657 [02:32<00:16,  3.90it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  91% 596/657 [02:32<00:15,  3.91it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  91% 598/657 [02:32<00:15,  3.92it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  91% 600/657 [02:32<00:14,  3.93it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  92% 602/657 [02:33<00:13,  3.93it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  92% 604/657 [02:33<00:13,  3.94it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  92% 606/657 [02:33<00:12,  3.95it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  93% 608/657 [02:33<00:12,  3.96it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  93% 610/657 [02:33<00:11,  3.97it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  93% 612/657 [02:33<00:11,  3.98it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  93% 614/657 [02:34<00:10,  3.99it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  94% 616/657 [02:34<00:10,  3.99it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  94% 618/657 [02:34<00:09,  4.00it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  94% 620/657 [02:34<00:09,  4.01it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  95% 622/657 [02:34<00:08,  4.02it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  95% 624/657 [02:34<00:08,  4.03it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  95% 626/657 [02:35<00:07,  4.04it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  96% 628/657 [02:35<00:07,  4.05it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  96% 630/657 [02:35<00:06,  4.05it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  96% 632/657 [02:35<00:06,  4.06it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  96% 634/657 [02:35<00:05,  4.07it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  97% 636/657 [02:35<00:05,  4.08it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  97% 638/657 [02:36<00:04,  4.09it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  97% 640/657 [02:36<00:04,  4.10it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  98% 642/657 [02:36<00:03,  4.10it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  98% 644/657 [02:36<00:03,  4.11it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  98% 646/657 [02:36<00:02,  4.12it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  99% 648/657 [02:36<00:02,  4.13it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  99% 650/657 [02:37<00:01,  4.14it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1:  99% 652/657 [02:37<00:01,  4.15it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1: 100% 654/657 [02:37<00:00,  4.15it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 1: 100% 656/657 [02:37<00:00,  4.16it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.38it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 14.0\n",
            "eval_fp: 14.0\n",
            "eval_fn: 3.0\n",
            "eval_tn: 854.0\n",
            "eval_precision: 0.5\n",
            "eval_recall: 0.8235294222831726\n",
            "eval_f1: 0.6222222447395325\n",
            "eval_mcc: 0.6331459879875183\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 657/657 [02:37<00:00,  4.16it/s, loss=0.192, v_num=-1-0, train_loss=0.571]\n",
            "Epoch 2:  83% 546/657 [02:27<00:29,  3.71it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  83% 548/657 [02:27<00:29,  3.71it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  84% 550/657 [02:27<00:28,  3.72it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  84% 552/657 [02:28<00:28,  3.73it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  84% 554/657 [02:28<00:27,  3.73it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  85% 556/657 [02:28<00:26,  3.74it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  85% 558/657 [02:28<00:26,  3.75it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  85% 560/657 [02:28<00:25,  3.76it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  86% 562/657 [02:29<00:25,  3.77it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  86% 564/657 [02:29<00:24,  3.78it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  86% 566/657 [02:29<00:24,  3.79it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  86% 568/657 [02:29<00:23,  3.80it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  87% 570/657 [02:29<00:22,  3.81it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  87% 572/657 [02:29<00:22,  3.82it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  87% 574/657 [02:30<00:21,  3.83it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  88% 576/657 [02:30<00:21,  3.83it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  88% 578/657 [02:30<00:20,  3.84it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  88% 580/657 [02:30<00:19,  3.85it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  89% 582/657 [02:30<00:19,  3.86it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  89% 584/657 [02:30<00:18,  3.87it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  89% 586/657 [02:31<00:18,  3.88it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  89% 588/657 [02:31<00:17,  3.89it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  90% 590/657 [02:31<00:17,  3.90it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  90% 592/657 [02:31<00:16,  3.91it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  90% 594/657 [02:31<00:16,  3.91it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  91% 596/657 [02:31<00:15,  3.92it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  91% 598/657 [02:32<00:15,  3.93it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  91% 600/657 [02:32<00:14,  3.94it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  92% 602/657 [02:32<00:13,  3.95it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  92% 604/657 [02:32<00:13,  3.96it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  92% 606/657 [02:32<00:12,  3.97it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  93% 608/657 [02:32<00:12,  3.98it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  93% 610/657 [02:33<00:11,  3.98it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  93% 612/657 [02:33<00:11,  3.99it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  93% 614/657 [02:33<00:10,  4.00it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  94% 616/657 [02:33<00:10,  4.01it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  94% 618/657 [02:33<00:09,  4.02it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  94% 620/657 [02:33<00:09,  4.03it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  95% 622/657 [02:34<00:08,  4.04it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  95% 624/657 [02:34<00:08,  4.04it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  95% 626/657 [02:34<00:07,  4.05it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  96% 628/657 [02:34<00:07,  4.06it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  96% 630/657 [02:34<00:06,  4.07it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  96% 632/657 [02:34<00:06,  4.08it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  96% 634/657 [02:35<00:05,  4.09it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  97% 636/657 [02:35<00:05,  4.09it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  97% 638/657 [02:35<00:04,  4.10it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  97% 640/657 [02:35<00:04,  4.11it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  98% 642/657 [02:35<00:03,  4.12it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  98% 644/657 [02:35<00:03,  4.13it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  98% 646/657 [02:36<00:02,  4.14it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  99% 648/657 [02:36<00:02,  4.15it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  99% 650/657 [02:36<00:01,  4.15it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2:  99% 652/657 [02:36<00:01,  4.16it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2: 100% 654/657 [02:36<00:00,  4.17it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 2: 100% 656/657 [02:37<00:00,  4.17it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Validating: 100% 111/111 [00:09<00:00,  8.69it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 16.0\n",
            "eval_fp: 14.0\n",
            "eval_fn: 1.0\n",
            "eval_tn: 854.0\n",
            "eval_precision: 0.5333333611488342\n",
            "eval_recall: 0.9411764740943909\n",
            "eval_f1: 0.6808510422706604\n",
            "eval_mcc: 0.70162433385849\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2: 100% 657/657 [02:37<00:00,  4.17it/s, loss=0.206, v_num=-1-0, train_loss=0.132]\n",
            "Epoch 3:  83% 546/657 [02:27<00:29,  3.71it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  83% 548/657 [02:27<00:29,  3.71it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  84% 550/657 [02:28<00:28,  3.72it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  84% 552/657 [02:28<00:28,  3.72it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  84% 554/657 [02:28<00:27,  3.73it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  85% 556/657 [02:28<00:26,  3.74it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  85% 558/657 [02:28<00:26,  3.75it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  85% 560/657 [02:28<00:25,  3.76it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  86% 562/657 [02:29<00:25,  3.77it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  86% 564/657 [02:29<00:24,  3.78it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  86% 566/657 [02:29<00:24,  3.79it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  86% 568/657 [02:29<00:23,  3.80it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  87% 570/657 [02:29<00:22,  3.81it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  87% 572/657 [02:29<00:22,  3.82it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  87% 574/657 [02:30<00:21,  3.82it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  88% 576/657 [02:30<00:21,  3.83it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  88% 578/657 [02:30<00:20,  3.84it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  88% 580/657 [02:30<00:19,  3.85it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  89% 582/657 [02:30<00:19,  3.86it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  89% 584/657 [02:30<00:18,  3.87it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  89% 586/657 [02:31<00:18,  3.88it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  89% 588/657 [02:31<00:17,  3.89it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  90% 590/657 [02:31<00:17,  3.90it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  90% 592/657 [02:31<00:16,  3.90it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  90% 594/657 [02:31<00:16,  3.91it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  91% 596/657 [02:31<00:15,  3.92it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  91% 598/657 [02:32<00:15,  3.93it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  91% 600/657 [02:32<00:14,  3.94it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  92% 602/657 [02:32<00:13,  3.95it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  92% 604/657 [02:32<00:13,  3.96it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  92% 606/657 [02:32<00:12,  3.97it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  93% 608/657 [02:32<00:12,  3.97it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  93% 610/657 [02:33<00:11,  3.98it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  93% 612/657 [02:33<00:11,  3.99it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  93% 614/657 [02:33<00:10,  4.00it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  94% 616/657 [02:33<00:10,  4.01it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  94% 618/657 [02:33<00:09,  4.02it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  94% 620/657 [02:34<00:09,  4.03it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  95% 622/657 [02:34<00:08,  4.03it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  95% 624/657 [02:34<00:08,  4.04it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  95% 626/657 [02:34<00:07,  4.05it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  96% 628/657 [02:34<00:07,  4.06it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  96% 630/657 [02:34<00:06,  4.07it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  96% 632/657 [02:35<00:06,  4.08it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  96% 634/657 [02:35<00:05,  4.09it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  97% 636/657 [02:35<00:05,  4.09it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  97% 638/657 [02:35<00:04,  4.10it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  97% 640/657 [02:35<00:04,  4.11it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  98% 642/657 [02:35<00:03,  4.12it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  98% 644/657 [02:36<00:03,  4.13it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  98% 646/657 [02:36<00:02,  4.14it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  99% 648/657 [02:36<00:02,  4.14it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  99% 650/657 [02:36<00:01,  4.15it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3:  99% 652/657 [02:36<00:01,  4.16it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3: 100% 654/657 [02:36<00:00,  4.17it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3: 100% 656/657 [02:37<00:00,  4.18it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.17it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 6.0\n",
            "eval_fn: 4.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.6842105388641357\n",
            "eval_recall: 0.7647058963775635\n",
            "eval_f1: 0.7222222089767456\n",
            "eval_mcc: 0.7176280617713928\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1645895680 bytes == 0x164110000 @  0x7f05c9c5e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f05c60bb950 0x7f05c60bfbf7 0x7f05c63f07e8 0x7f05c63a61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2057371648 bytes == 0x7f02055f0000 @  0x7f05c9c5e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f05c60bb950 0x7f05c60bfbf7 0x7f05c63f07e8 0x7f05c63a61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 3: 100% 657/657 [02:48<00:00,  3.90it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 3: 100% 657/657 [02:49<00:00,  3.88it/s, loss=0.070, v_num=-1-0, train_loss=0.0954]\n",
            "Epoch 4:  83% 546/657 [02:28<00:30,  3.69it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  83% 547/657 [02:28<00:29,  3.68it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  84% 549/657 [02:28<00:29,  3.69it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  84% 551/657 [02:28<00:28,  3.70it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  84% 553/657 [02:29<00:28,  3.71it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  84% 555/657 [02:29<00:27,  3.72it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  85% 557/657 [02:29<00:26,  3.73it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  85% 559/657 [02:29<00:26,  3.74it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  85% 561/657 [02:29<00:25,  3.75it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  86% 563/657 [02:29<00:25,  3.76it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  86% 565/657 [02:30<00:24,  3.76it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  86% 567/657 [02:30<00:23,  3.77it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  87% 569/657 [02:30<00:23,  3.78it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  87% 571/657 [02:30<00:22,  3.79it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  87% 573/657 [02:30<00:22,  3.80it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  88% 575/657 [02:30<00:21,  3.81it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  88% 577/657 [02:31<00:20,  3.82it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  88% 579/657 [02:31<00:20,  3.83it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  88% 581/657 [02:31<00:19,  3.84it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  89% 583/657 [02:31<00:19,  3.85it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  89% 585/657 [02:31<00:18,  3.85it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  89% 587/657 [02:31<00:18,  3.86it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  90% 589/657 [02:32<00:17,  3.87it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  90% 591/657 [02:32<00:17,  3.88it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  90% 593/657 [02:32<00:16,  3.89it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  91% 595/657 [02:32<00:15,  3.90it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  91% 597/657 [02:32<00:15,  3.91it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  91% 599/657 [02:32<00:14,  3.92it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  91% 601/657 [02:33<00:14,  3.92it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  92% 603/657 [02:33<00:13,  3.93it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  92% 605/657 [02:33<00:13,  3.94it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  92% 607/657 [02:33<00:12,  3.95it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  93% 609/657 [02:33<00:12,  3.96it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  93% 611/657 [02:33<00:11,  3.97it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  93% 613/657 [02:34<00:11,  3.98it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  94% 615/657 [02:34<00:10,  3.99it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  94% 617/657 [02:34<00:10,  3.99it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  94% 619/657 [02:34<00:09,  4.00it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  95% 621/657 [02:34<00:08,  4.01it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  95% 623/657 [02:35<00:08,  4.02it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  95% 625/657 [02:35<00:07,  4.03it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  95% 627/657 [02:35<00:07,  4.04it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  96% 629/657 [02:35<00:06,  4.04it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  96% 631/657 [02:35<00:06,  4.05it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  96% 633/657 [02:35<00:05,  4.06it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  97% 635/657 [02:36<00:05,  4.07it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  97% 637/657 [02:36<00:04,  4.08it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  97% 639/657 [02:36<00:04,  4.09it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  98% 641/657 [02:36<00:03,  4.09it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  98% 643/657 [02:36<00:03,  4.10it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  98% 645/657 [02:36<00:02,  4.11it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  98% 647/657 [02:37<00:02,  4.12it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  99% 649/657 [02:37<00:01,  4.13it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  99% 651/657 [02:37<00:01,  4.14it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4:  99% 653/657 [02:37<00:00,  4.14it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4: 100% 655/657 [02:37<00:00,  4.15it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4: 100% 657/657 [02:37<00:00,  4.16it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 6.0\n",
            "eval_fn: 4.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.6842105388641357\n",
            "eval_recall: 0.7647058963775635\n",
            "eval_f1: 0.7222222089767456\n",
            "eval_mcc: 0.7176280617713928\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 2057371648 bytes == 0x7f02055f0000 @  0x7f05c9c5e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7f05c60bb950 0x7f05c60bfbf7 0x7f05c63f07e8 0x7f05c63a61b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 4: 100% 657/657 [02:49<00:00,  3.87it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 4: 100% 657/657 [02:49<00:00,  3.87it/s, loss=0.051, v_num=-1-0, train_loss=0.0823]\n",
            "Epoch 5:  83% 546/657 [02:28<00:30,  3.68it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  83% 547/657 [02:28<00:29,  3.68it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  84% 549/657 [02:28<00:29,  3.69it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  84% 551/657 [02:29<00:28,  3.70it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  84% 553/657 [02:29<00:28,  3.70it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  84% 555/657 [02:29<00:27,  3.71it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  85% 557/657 [02:29<00:26,  3.72it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  85% 559/657 [02:29<00:26,  3.73it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  85% 561/657 [02:29<00:25,  3.74it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  86% 563/657 [02:30<00:25,  3.75it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  86% 565/657 [02:30<00:24,  3.76it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  86% 567/657 [02:30<00:23,  3.77it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  87% 569/657 [02:30<00:23,  3.78it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  87% 571/657 [02:30<00:22,  3.79it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  87% 573/657 [02:30<00:22,  3.80it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  88% 575/657 [02:31<00:21,  3.80it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  88% 577/657 [02:31<00:20,  3.81it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  88% 579/657 [02:31<00:20,  3.82it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  88% 581/657 [02:31<00:19,  3.83it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  89% 583/657 [02:31<00:19,  3.84it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  89% 585/657 [02:31<00:18,  3.85it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  89% 587/657 [02:32<00:18,  3.86it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  90% 589/657 [02:32<00:17,  3.87it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  90% 591/657 [02:32<00:17,  3.88it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  90% 593/657 [02:32<00:16,  3.88it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  91% 595/657 [02:32<00:15,  3.89it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  91% 597/657 [02:33<00:15,  3.90it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  91% 599/657 [02:33<00:14,  3.91it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  91% 601/657 [02:33<00:14,  3.92it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  92% 603/657 [02:33<00:13,  3.93it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  92% 605/657 [02:33<00:13,  3.94it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  92% 607/657 [02:33<00:12,  3.95it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  93% 609/657 [02:34<00:12,  3.95it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  93% 611/657 [02:34<00:11,  3.96it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  93% 613/657 [02:34<00:11,  3.97it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  94% 615/657 [02:34<00:10,  3.98it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  94% 617/657 [02:34<00:10,  3.99it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  94% 619/657 [02:34<00:09,  4.00it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  95% 621/657 [02:35<00:08,  4.01it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  95% 623/657 [02:35<00:08,  4.01it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  95% 625/657 [02:35<00:07,  4.02it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  95% 627/657 [02:35<00:07,  4.03it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  96% 629/657 [02:35<00:06,  4.04it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  96% 631/657 [02:35<00:06,  4.05it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  96% 633/657 [02:36<00:05,  4.06it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  97% 635/657 [02:36<00:05,  4.06it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  97% 637/657 [02:36<00:04,  4.07it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  97% 639/657 [02:36<00:04,  4.08it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  98% 641/657 [02:36<00:03,  4.09it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  98% 643/657 [02:36<00:03,  4.10it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  98% 645/657 [02:37<00:02,  4.11it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  98% 647/657 [02:37<00:02,  4.11it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  99% 649/657 [02:37<00:01,  4.12it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  99% 651/657 [02:37<00:01,  4.13it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5:  99% 653/657 [02:37<00:00,  4.14it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5: 100% 655/657 [02:37<00:00,  4.15it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 5: 100% 657/657 [02:38<00:00,  4.16it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 15.0\n",
            "eval_fp: 11.0\n",
            "eval_fn: 2.0\n",
            "eval_tn: 857.0\n",
            "eval_precision: 0.5769230723381042\n",
            "eval_recall: 0.8823529481887817\n",
            "eval_f1: 0.6976743936538696\n",
            "eval_mcc: 0.7069042921066284\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5: 100% 657/657 [02:38<00:00,  4.15it/s, loss=0.160, v_num=-1-0, train_loss=0.077]\n",
            "Epoch 6:  83% 546/657 [02:27<00:30,  3.69it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  83% 548/657 [02:28<00:29,  3.69it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  84% 550/657 [02:28<00:28,  3.70it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  84% 552/657 [02:28<00:28,  3.71it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  84% 554/657 [02:28<00:27,  3.72it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  85% 556/657 [02:29<00:27,  3.73it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  85% 558/657 [02:29<00:26,  3.74it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  85% 560/657 [02:29<00:25,  3.75it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  86% 562/657 [02:29<00:25,  3.76it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  86% 564/657 [02:29<00:24,  3.77it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  86% 566/657 [02:29<00:24,  3.78it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  86% 568/657 [02:30<00:23,  3.78it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  87% 570/657 [02:30<00:22,  3.79it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  87% 572/657 [02:30<00:22,  3.80it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  87% 574/657 [02:30<00:21,  3.81it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  88% 576/657 [02:30<00:21,  3.82it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  88% 578/657 [02:30<00:20,  3.83it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  88% 580/657 [02:31<00:20,  3.84it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  89% 582/657 [02:31<00:19,  3.85it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  89% 584/657 [02:31<00:18,  3.86it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  89% 586/657 [02:31<00:18,  3.86it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  89% 588/657 [02:31<00:17,  3.87it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  90% 590/657 [02:31<00:17,  3.88it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  90% 592/657 [02:32<00:16,  3.89it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  90% 594/657 [02:32<00:16,  3.90it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  91% 596/657 [02:32<00:15,  3.91it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  91% 598/657 [02:32<00:15,  3.92it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  91% 600/657 [02:32<00:14,  3.93it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  92% 602/657 [02:33<00:13,  3.93it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  92% 604/657 [02:33<00:13,  3.94it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  92% 606/657 [02:33<00:12,  3.95it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  93% 608/657 [02:33<00:12,  3.96it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  93% 610/657 [02:33<00:11,  3.97it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  93% 612/657 [02:33<00:11,  3.98it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  93% 614/657 [02:34<00:10,  3.99it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  94% 616/657 [02:34<00:10,  3.99it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  94% 618/657 [02:34<00:09,  4.00it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  94% 620/657 [02:34<00:09,  4.01it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  95% 622/657 [02:34<00:08,  4.02it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  95% 624/657 [02:34<00:08,  4.03it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  95% 626/657 [02:35<00:07,  4.04it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  96% 628/657 [02:35<00:07,  4.05it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  96% 630/657 [02:35<00:06,  4.05it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  96% 632/657 [02:35<00:06,  4.06it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  96% 634/657 [02:35<00:05,  4.07it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  97% 636/657 [02:35<00:05,  4.08it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  97% 638/657 [02:36<00:04,  4.09it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  97% 640/657 [02:36<00:04,  4.10it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  98% 642/657 [02:36<00:03,  4.10it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  98% 644/657 [02:36<00:03,  4.11it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  98% 646/657 [02:36<00:02,  4.12it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  99% 648/657 [02:36<00:02,  4.13it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  99% 650/657 [02:37<00:01,  4.14it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6:  99% 652/657 [02:37<00:01,  4.15it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6: 100% 654/657 [02:37<00:00,  4.15it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 6: 100% 656/657 [02:37<00:00,  4.16it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.39it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 12.0\n",
            "eval_fp: 7.0\n",
            "eval_fn: 5.0\n",
            "eval_tn: 861.0\n",
            "eval_precision: 0.6315789222717285\n",
            "eval_recall: 0.7058823704719543\n",
            "eval_f1: 0.6666666865348816\n",
            "eval_mcc: 0.6608313322067261\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6: 100% 657/657 [02:38<00:00,  4.16it/s, loss=0.030, v_num=-1-0, train_loss=0.0697]\n",
            "Epoch 7:  83% 546/657 [02:27<00:30,  3.69it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  83% 548/657 [02:28<00:29,  3.69it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  84% 550/657 [02:28<00:28,  3.70it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  84% 552/657 [02:28<00:28,  3.71it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  84% 554/657 [02:28<00:27,  3.72it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  85% 556/657 [02:29<00:27,  3.73it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  85% 558/657 [02:29<00:26,  3.74it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  85% 560/657 [02:29<00:25,  3.75it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  86% 562/657 [02:29<00:25,  3.76it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  86% 564/657 [02:29<00:24,  3.76it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  86% 566/657 [02:29<00:24,  3.77it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  86% 568/657 [02:30<00:23,  3.78it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  87% 570/657 [02:30<00:22,  3.79it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  87% 572/657 [02:30<00:22,  3.80it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  87% 574/657 [02:30<00:21,  3.81it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  88% 576/657 [02:30<00:21,  3.82it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  88% 578/657 [02:30<00:20,  3.83it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  88% 580/657 [02:31<00:20,  3.84it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  89% 582/657 [02:31<00:19,  3.85it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  89% 584/657 [02:31<00:18,  3.85it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  89% 586/657 [02:31<00:18,  3.86it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  89% 588/657 [02:31<00:17,  3.87it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  90% 590/657 [02:32<00:17,  3.88it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  90% 592/657 [02:32<00:16,  3.89it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  90% 594/657 [02:32<00:16,  3.90it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  91% 596/657 [02:32<00:15,  3.91it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  91% 598/657 [02:32<00:15,  3.92it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  91% 600/657 [02:32<00:14,  3.92it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  92% 602/657 [02:33<00:13,  3.93it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  92% 604/657 [02:33<00:13,  3.94it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  92% 606/657 [02:33<00:12,  3.95it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  93% 608/657 [02:33<00:12,  3.96it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  93% 610/657 [02:33<00:11,  3.97it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  93% 612/657 [02:33<00:11,  3.98it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  93% 614/657 [02:34<00:10,  3.99it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  94% 616/657 [02:34<00:10,  3.99it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  94% 618/657 [02:34<00:09,  4.00it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  94% 620/657 [02:34<00:09,  4.01it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  95% 622/657 [02:34<00:08,  4.02it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  95% 624/657 [02:34<00:08,  4.03it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  95% 626/657 [02:35<00:07,  4.04it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  96% 628/657 [02:35<00:07,  4.05it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  96% 630/657 [02:35<00:06,  4.05it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  96% 632/657 [02:35<00:06,  4.06it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  96% 634/657 [02:35<00:05,  4.07it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  97% 636/657 [02:35<00:05,  4.08it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  97% 638/657 [02:36<00:04,  4.09it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  97% 640/657 [02:36<00:04,  4.10it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  98% 642/657 [02:36<00:03,  4.10it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  98% 644/657 [02:36<00:03,  4.11it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  98% 646/657 [02:36<00:02,  4.12it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  99% 648/657 [02:36<00:02,  4.13it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  99% 650/657 [02:37<00:01,  4.14it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7:  99% 652/657 [02:37<00:01,  4.15it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7: 100% 654/657 [02:37<00:00,  4.15it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Epoch 7: 100% 656/657 [02:37<00:00,  4.16it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.33it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 7.0\n",
            "eval_fn: 4.0\n",
            "eval_tn: 861.0\n",
            "eval_precision: 0.6499999761581421\n",
            "eval_recall: 0.7647058963775635\n",
            "eval_f1: 0.7027027010917664\n",
            "eval_mcc: 0.6987975239753723\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 657/657 [02:48<00:00,  3.89it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 657/657 [02:49<00:00,  3.88it/s, loss=0.026, v_num=-1-0, train_loss=0.0661]\n",
            "Computing Input\n",
            "100% 4367/4367 [00:08<00:00, 543.21it/s]\n",
            "100% 885/885 [00:01<00:00, 517.92it/s]\n",
            "100% 77/77 [00:00<00:00, 579.08it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.30it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 9.0\n",
            "fp: 3.0\n",
            "fn: 0.0\n",
            "tn: 65.0\n",
            "precision: 0.75\n",
            "recall: 1.0\n",
            "f1: 0.8571428656578064\n",
            "mcc: 0.8467064499855042\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.9625), 'test_loss': tensor(0.0578, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.20it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"label\": \"ud\",\n",
            "    \"augment\": false,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 768,\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 4343/4343 [00:10<00:00, 404.93it/s]\n",
            "100% 885/885 [00:02<00:00, 430.84it/s]\n",
            "100% 77/77 [00:00<00:00, 471.46it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 592 K \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.13it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 0.0\n",
            "eval_fp: 0.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 8.0\n",
            "eval_precision: 0\n",
            "eval_recall: 0\n",
            "eval_f1: 0\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  83% 543/654 [02:27<00:30,  3.69it/s, loss=0.078, v_num=-1-1]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  83% 544/654 [02:27<00:29,  3.69it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  83% 546/654 [02:27<00:29,  3.70it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  84% 548/654 [02:27<00:28,  3.70it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  84% 550/654 [02:28<00:28,  3.71it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  84% 552/654 [02:28<00:27,  3.72it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  85% 554/654 [02:28<00:26,  3.73it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  85% 556/654 [02:28<00:26,  3.74it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  85% 558/654 [02:28<00:25,  3.75it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  86% 560/654 [02:28<00:25,  3.76it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  86% 562/654 [02:29<00:24,  3.77it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  86% 564/654 [02:29<00:23,  3.78it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  87% 566/654 [02:29<00:23,  3.79it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  87% 568/654 [02:29<00:22,  3.80it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  87% 570/654 [02:29<00:22,  3.80it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  87% 572/654 [02:29<00:21,  3.81it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  88% 574/654 [02:30<00:20,  3.82it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  88% 576/654 [02:30<00:20,  3.83it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  88% 578/654 [02:30<00:19,  3.84it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  89% 580/654 [02:30<00:19,  3.85it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  89% 582/654 [02:30<00:18,  3.86it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  89% 584/654 [02:31<00:18,  3.87it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  90% 586/654 [02:31<00:17,  3.88it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  90% 588/654 [02:31<00:16,  3.89it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  90% 590/654 [02:31<00:16,  3.89it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  91% 592/654 [02:31<00:15,  3.90it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  91% 594/654 [02:31<00:15,  3.91it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  91% 596/654 [02:32<00:14,  3.92it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  91% 598/654 [02:32<00:14,  3.93it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  92% 600/654 [02:32<00:13,  3.94it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  92% 602/654 [02:32<00:13,  3.95it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  92% 604/654 [02:32<00:12,  3.96it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  93% 606/654 [02:32<00:12,  3.96it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  93% 608/654 [02:33<00:11,  3.97it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  93% 610/654 [02:33<00:11,  3.98it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  94% 612/654 [02:33<00:10,  3.99it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  94% 614/654 [02:33<00:10,  4.00it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  94% 616/654 [02:33<00:09,  4.01it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  94% 618/654 [02:33<00:08,  4.02it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  95% 620/654 [02:34<00:08,  4.02it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  95% 622/654 [02:34<00:07,  4.03it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  95% 624/654 [02:34<00:07,  4.04it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  96% 626/654 [02:34<00:06,  4.05it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  96% 628/654 [02:34<00:06,  4.06it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  96% 630/654 [02:34<00:05,  4.07it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  97% 632/654 [02:35<00:05,  4.07it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  97% 634/654 [02:35<00:04,  4.08it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  97% 636/654 [02:35<00:04,  4.09it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  98% 638/654 [02:35<00:03,  4.10it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  98% 640/654 [02:35<00:03,  4.11it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  98% 642/654 [02:35<00:02,  4.12it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  98% 644/654 [02:36<00:02,  4.13it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  99% 646/654 [02:36<00:01,  4.13it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  99% 648/654 [02:36<00:01,  4.14it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0:  99% 650/654 [02:36<00:00,  4.15it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0: 100% 652/654 [02:36<00:00,  4.16it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 0: 100% 654/654 [02:36<00:00,  4.17it/s, loss=0.078, v_num=-1-1]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 9.0\n",
            "eval_fp: 4.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.692307710647583\n",
            "eval_recall: 0.4736842215061188\n",
            "eval_f1: 0.5625\n",
            "eval_mcc: 0.5651179552078247\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0: 100% 654/654 [02:47<00:00,  3.89it/s, loss=0.078, v_num=-1-1]\n",
            "Epoch 1:  83% 543/654 [02:27<00:30,  3.67it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  83% 544/654 [02:28<00:29,  3.67it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  83% 546/654 [02:28<00:29,  3.68it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  84% 548/654 [02:28<00:28,  3.69it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  84% 550/654 [02:28<00:28,  3.70it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  84% 552/654 [02:28<00:27,  3.71it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  85% 554/654 [02:29<00:26,  3.72it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  85% 556/654 [02:29<00:26,  3.72it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  85% 558/654 [02:29<00:25,  3.73it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  86% 560/654 [02:29<00:25,  3.74it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  86% 562/654 [02:29<00:24,  3.75it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  86% 564/654 [02:29<00:23,  3.76it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  87% 566/654 [02:30<00:23,  3.77it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  87% 568/654 [02:30<00:22,  3.78it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  87% 570/654 [02:30<00:22,  3.79it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  87% 572/654 [02:30<00:21,  3.80it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  88% 574/654 [02:30<00:21,  3.81it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  88% 576/654 [02:30<00:20,  3.81it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  88% 578/654 [02:31<00:19,  3.82it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  89% 580/654 [02:31<00:19,  3.83it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  89% 582/654 [02:31<00:18,  3.84it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  89% 584/654 [02:31<00:18,  3.85it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  90% 586/654 [02:31<00:17,  3.86it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  90% 588/654 [02:32<00:17,  3.87it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  90% 590/654 [02:32<00:16,  3.88it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  91% 592/654 [02:32<00:15,  3.88it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  91% 594/654 [02:32<00:15,  3.89it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  91% 596/654 [02:32<00:14,  3.90it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  91% 598/654 [02:32<00:14,  3.91it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  92% 600/654 [02:33<00:13,  3.92it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  92% 602/654 [02:33<00:13,  3.93it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  92% 604/654 [02:33<00:12,  3.94it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  93% 606/654 [02:33<00:12,  3.95it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  93% 608/654 [02:33<00:11,  3.95it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  93% 610/654 [02:33<00:11,  3.96it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  94% 612/654 [02:34<00:10,  3.97it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  94% 614/654 [02:34<00:10,  3.98it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  94% 616/654 [02:34<00:09,  3.99it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  94% 618/654 [02:34<00:09,  4.00it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  95% 620/654 [02:34<00:08,  4.01it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  95% 622/654 [02:34<00:07,  4.01it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  95% 624/654 [02:35<00:07,  4.02it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  96% 626/654 [02:35<00:06,  4.03it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  96% 628/654 [02:35<00:06,  4.04it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  96% 630/654 [02:35<00:05,  4.05it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  97% 632/654 [02:35<00:05,  4.06it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  97% 634/654 [02:36<00:04,  4.06it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  97% 636/654 [02:36<00:04,  4.07it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  98% 638/654 [02:36<00:03,  4.08it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  98% 640/654 [02:36<00:03,  4.09it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  98% 642/654 [02:36<00:02,  4.10it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  98% 644/654 [02:36<00:02,  4.10it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  99% 646/654 [02:37<00:01,  4.11it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  99% 648/654 [02:37<00:01,  4.12it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1:  99% 650/654 [02:37<00:00,  4.13it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1: 100% 652/654 [02:37<00:00,  4.14it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 1: 100% 654/654 [02:37<00:00,  4.15it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 9.0\n",
            "eval_fp: 4.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.692307710647583\n",
            "eval_recall: 0.4736842215061188\n",
            "eval_f1: 0.5625\n",
            "eval_mcc: 0.5651179552078247\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 654/654 [02:47<00:00,  3.89it/s, loss=0.065, v_num=-1-1, train_loss=0.481]\n",
            "Epoch 2:  83% 543/654 [02:27<00:30,  3.68it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  83% 544/654 [02:27<00:29,  3.68it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  83% 546/654 [02:28<00:29,  3.69it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  84% 548/654 [02:28<00:28,  3.70it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  84% 550/654 [02:28<00:28,  3.71it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  84% 552/654 [02:28<00:27,  3.72it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  85% 554/654 [02:28<00:26,  3.73it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  85% 556/654 [02:28<00:26,  3.73it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  85% 558/654 [02:29<00:25,  3.74it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  86% 560/654 [02:29<00:25,  3.75it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  86% 562/654 [02:29<00:24,  3.76it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  86% 564/654 [02:29<00:23,  3.77it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  87% 566/654 [02:29<00:23,  3.78it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  87% 568/654 [02:29<00:22,  3.79it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  87% 570/654 [02:30<00:22,  3.80it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  87% 572/654 [02:30<00:21,  3.81it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  88% 574/654 [02:30<00:20,  3.82it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  88% 576/654 [02:30<00:20,  3.83it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  88% 578/654 [02:30<00:19,  3.83it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  89% 580/654 [02:30<00:19,  3.84it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  89% 582/654 [02:31<00:18,  3.85it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  89% 584/654 [02:31<00:18,  3.86it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  90% 586/654 [02:31<00:17,  3.87it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  90% 588/654 [02:31<00:17,  3.88it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  90% 590/654 [02:31<00:16,  3.89it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  91% 592/654 [02:31<00:15,  3.90it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  91% 594/654 [02:32<00:15,  3.90it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  91% 596/654 [02:32<00:14,  3.91it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  91% 598/654 [02:32<00:14,  3.92it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  92% 600/654 [02:32<00:13,  3.93it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  92% 602/654 [02:32<00:13,  3.94it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  92% 604/654 [02:32<00:12,  3.95it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  93% 606/654 [02:33<00:12,  3.96it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  93% 608/654 [02:33<00:11,  3.97it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  93% 610/654 [02:33<00:11,  3.97it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  94% 612/654 [02:33<00:10,  3.98it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  94% 614/654 [02:33<00:10,  3.99it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  94% 616/654 [02:33<00:09,  4.00it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  94% 618/654 [02:34<00:08,  4.01it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  95% 620/654 [02:34<00:08,  4.02it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  95% 622/654 [02:34<00:07,  4.03it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  95% 624/654 [02:34<00:07,  4.03it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  96% 626/654 [02:34<00:06,  4.04it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  96% 628/654 [02:35<00:06,  4.05it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  96% 630/654 [02:35<00:05,  4.06it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  97% 632/654 [02:35<00:05,  4.07it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  97% 634/654 [02:35<00:04,  4.08it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  97% 636/654 [02:35<00:04,  4.08it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  98% 638/654 [02:35<00:03,  4.09it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  98% 640/654 [02:36<00:03,  4.10it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  98% 642/654 [02:36<00:02,  4.11it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  98% 644/654 [02:36<00:02,  4.12it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  99% 646/654 [02:36<00:01,  4.13it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  99% 648/654 [02:36<00:01,  4.13it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2:  99% 650/654 [02:36<00:00,  4.14it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2: 100% 652/654 [02:37<00:00,  4.15it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 2: 100% 654/654 [02:37<00:00,  4.16it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 9.0\n",
            "eval_fp: 1.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 865.0\n",
            "eval_precision: 0.8999999761581421\n",
            "eval_recall: 0.4736842215061188\n",
            "eval_f1: 0.6206896305084229\n",
            "eval_mcc: 0.6479785442352295\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2: 100% 654/654 [02:37<00:00,  4.15it/s, loss=0.073, v_num=-1-1, train_loss=0.103]\n",
            "Epoch 3:  83% 543/654 [02:26<00:30,  3.69it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  83% 544/654 [02:27<00:29,  3.69it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  83% 546/654 [02:27<00:29,  3.70it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  84% 548/654 [02:27<00:28,  3.71it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  84% 550/654 [02:27<00:27,  3.72it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  84% 552/654 [02:28<00:27,  3.73it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  85% 554/654 [02:28<00:26,  3.73it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  85% 556/654 [02:28<00:26,  3.74it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  85% 558/654 [02:28<00:25,  3.75it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  86% 560/654 [02:28<00:24,  3.76it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  86% 562/654 [02:29<00:24,  3.77it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  86% 564/654 [02:29<00:23,  3.78it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  87% 566/654 [02:29<00:23,  3.79it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  87% 568/654 [02:29<00:22,  3.80it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  87% 570/654 [02:29<00:22,  3.81it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  87% 572/654 [02:29<00:21,  3.82it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  88% 574/654 [02:30<00:20,  3.83it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  88% 576/654 [02:30<00:20,  3.84it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  88% 578/654 [02:30<00:19,  3.84it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  89% 580/654 [02:30<00:19,  3.85it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  89% 582/654 [02:30<00:18,  3.86it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  89% 584/654 [02:30<00:18,  3.87it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  90% 586/654 [02:31<00:17,  3.88it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  90% 588/654 [02:31<00:16,  3.89it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  90% 590/654 [02:31<00:16,  3.90it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  91% 592/654 [02:31<00:15,  3.91it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  91% 594/654 [02:31<00:15,  3.91it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  91% 596/654 [02:31<00:14,  3.92it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  91% 598/654 [02:32<00:14,  3.93it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  92% 600/654 [02:32<00:13,  3.94it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  92% 602/654 [02:32<00:13,  3.95it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  92% 604/654 [02:32<00:12,  3.96it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  93% 606/654 [02:32<00:12,  3.97it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  93% 608/654 [02:32<00:11,  3.98it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  93% 610/654 [02:33<00:11,  3.98it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  94% 612/654 [02:33<00:10,  3.99it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  94% 614/654 [02:33<00:09,  4.00it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  94% 616/654 [02:33<00:09,  4.01it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  94% 618/654 [02:33<00:08,  4.02it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  95% 620/654 [02:33<00:08,  4.03it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  95% 622/654 [02:34<00:07,  4.04it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  95% 624/654 [02:34<00:07,  4.04it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  96% 626/654 [02:34<00:06,  4.05it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  96% 628/654 [02:34<00:06,  4.06it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  96% 630/654 [02:34<00:05,  4.07it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  97% 632/654 [02:34<00:05,  4.08it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  97% 634/654 [02:35<00:04,  4.09it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  97% 636/654 [02:35<00:04,  4.09it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  98% 638/654 [02:35<00:03,  4.10it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  98% 640/654 [02:35<00:03,  4.11it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  98% 642/654 [02:35<00:02,  4.12it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  98% 644/654 [02:36<00:02,  4.13it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  99% 646/654 [02:36<00:01,  4.14it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  99% 648/654 [02:36<00:01,  4.14it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3:  99% 650/654 [02:36<00:00,  4.15it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3: 100% 652/654 [02:36<00:00,  4.16it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 3: 100% 654/654 [02:36<00:00,  4.17it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 9.0\n",
            "eval_fp: 0.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 866.0\n",
            "eval_precision: 1.0\n",
            "eval_recall: 0.4736842215061188\n",
            "eval_f1: 0.6428571343421936\n",
            "eval_mcc: 0.6843075752258301\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3: 100% 654/654 [02:37<00:00,  4.16it/s, loss=0.065, v_num=-1-1, train_loss=0.0777]\n",
            "Epoch 4:  83% 543/654 [02:27<00:30,  3.69it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  83% 544/654 [02:27<00:29,  3.68it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  83% 546/654 [02:27<00:29,  3.69it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  84% 548/654 [02:27<00:28,  3.70it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  84% 550/654 [02:28<00:28,  3.71it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  84% 552/654 [02:28<00:27,  3.72it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  85% 554/654 [02:28<00:26,  3.73it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  85% 556/654 [02:28<00:26,  3.74it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  85% 558/654 [02:28<00:25,  3.75it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  86% 560/654 [02:28<00:25,  3.76it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  86% 562/654 [02:29<00:24,  3.77it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  86% 564/654 [02:29<00:23,  3.78it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  87% 566/654 [02:29<00:23,  3.79it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  87% 568/654 [02:29<00:22,  3.79it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  87% 570/654 [02:29<00:22,  3.80it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  87% 572/654 [02:30<00:21,  3.81it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  88% 574/654 [02:30<00:20,  3.82it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  88% 576/654 [02:30<00:20,  3.83it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  88% 578/654 [02:30<00:19,  3.84it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  89% 580/654 [02:30<00:19,  3.85it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  89% 582/654 [02:30<00:18,  3.86it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  89% 584/654 [02:31<00:18,  3.87it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  90% 586/654 [02:31<00:17,  3.88it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  90% 588/654 [02:31<00:16,  3.88it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  90% 590/654 [02:31<00:16,  3.89it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  91% 592/654 [02:31<00:15,  3.90it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  91% 594/654 [02:31<00:15,  3.91it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  91% 596/654 [02:32<00:14,  3.92it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  91% 598/654 [02:32<00:14,  3.93it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  92% 600/654 [02:32<00:13,  3.94it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  92% 602/654 [02:32<00:13,  3.95it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  92% 604/654 [02:32<00:12,  3.95it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  93% 606/654 [02:32<00:12,  3.96it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  93% 608/654 [02:33<00:11,  3.97it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  93% 610/654 [02:33<00:11,  3.98it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  94% 612/654 [02:33<00:10,  3.99it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  94% 614/654 [02:33<00:10,  4.00it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  94% 616/654 [02:33<00:09,  4.01it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  94% 618/654 [02:33<00:08,  4.01it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  95% 620/654 [02:34<00:08,  4.02it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  95% 622/654 [02:34<00:07,  4.03it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  95% 624/654 [02:34<00:07,  4.04it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  96% 626/654 [02:34<00:06,  4.05it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  96% 628/654 [02:34<00:06,  4.06it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  96% 630/654 [02:34<00:05,  4.07it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  97% 632/654 [02:35<00:05,  4.07it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  97% 634/654 [02:35<00:04,  4.08it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  97% 636/654 [02:35<00:04,  4.09it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  98% 638/654 [02:35<00:03,  4.10it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  98% 640/654 [02:35<00:03,  4.11it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  98% 642/654 [02:35<00:02,  4.12it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  98% 644/654 [02:36<00:02,  4.12it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  99% 646/654 [02:36<00:01,  4.13it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  99% 648/654 [02:36<00:01,  4.14it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4:  99% 650/654 [02:36<00:00,  4.15it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4: 100% 652/654 [02:36<00:00,  4.16it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 4: 100% 654/654 [02:36<00:00,  4.17it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 9.0\n",
            "eval_fp: 0.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 866.0\n",
            "eval_precision: 1.0\n",
            "eval_recall: 0.4736842215061188\n",
            "eval_f1: 0.6428571343421936\n",
            "eval_mcc: 0.6843075752258301\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4: 100% 654/654 [02:47<00:00,  3.91it/s, loss=0.069, v_num=-1-1, train_loss=0.0647]\n",
            "Epoch 5:  83% 543/654 [02:27<00:30,  3.68it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  83% 544/654 [02:27<00:29,  3.68it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  83% 546/654 [02:28<00:29,  3.69it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  84% 548/654 [02:28<00:28,  3.70it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  84% 550/654 [02:28<00:28,  3.70it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  84% 552/654 [02:28<00:27,  3.71it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  85% 554/654 [02:28<00:26,  3.72it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  85% 556/654 [02:28<00:26,  3.73it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  85% 558/654 [02:29<00:25,  3.74it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  86% 560/654 [02:29<00:25,  3.75it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  86% 562/654 [02:29<00:24,  3.76it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  86% 564/654 [02:29<00:23,  3.77it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  87% 566/654 [02:29<00:23,  3.78it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  87% 568/654 [02:29<00:22,  3.79it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  87% 570/654 [02:30<00:22,  3.80it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  87% 572/654 [02:30<00:21,  3.80it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  88% 574/654 [02:30<00:20,  3.81it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  88% 576/654 [02:30<00:20,  3.82it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  88% 578/654 [02:30<00:19,  3.83it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  89% 580/654 [02:31<00:19,  3.84it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  89% 582/654 [02:31<00:18,  3.85it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  89% 584/654 [02:31<00:18,  3.86it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  90% 586/654 [02:31<00:17,  3.87it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  90% 588/654 [02:31<00:17,  3.88it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  90% 590/654 [02:31<00:16,  3.88it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  91% 592/654 [02:32<00:15,  3.89it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  91% 594/654 [02:32<00:15,  3.90it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  91% 596/654 [02:32<00:14,  3.91it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  91% 598/654 [02:32<00:14,  3.92it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  92% 600/654 [02:32<00:13,  3.93it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  92% 602/654 [02:32<00:13,  3.94it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  92% 604/654 [02:33<00:12,  3.95it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  93% 606/654 [02:33<00:12,  3.95it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  93% 608/654 [02:33<00:11,  3.96it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  93% 610/654 [02:33<00:11,  3.97it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  94% 612/654 [02:33<00:10,  3.98it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  94% 614/654 [02:33<00:10,  3.99it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  94% 616/654 [02:34<00:09,  4.00it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  94% 618/654 [02:34<00:08,  4.01it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  95% 620/654 [02:34<00:08,  4.01it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  95% 622/654 [02:34<00:07,  4.02it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  95% 624/654 [02:34<00:07,  4.03it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  96% 626/654 [02:34<00:06,  4.04it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  96% 628/654 [02:35<00:06,  4.05it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  96% 630/654 [02:35<00:05,  4.06it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  97% 632/654 [02:35<00:05,  4.07it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  97% 634/654 [02:35<00:04,  4.07it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  97% 636/654 [02:35<00:04,  4.08it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  98% 638/654 [02:35<00:03,  4.09it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  98% 640/654 [02:36<00:03,  4.10it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  98% 642/654 [02:36<00:02,  4.11it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  98% 644/654 [02:36<00:02,  4.11it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  99% 646/654 [02:36<00:01,  4.12it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  99% 648/654 [02:36<00:01,  4.13it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5:  99% 650/654 [02:37<00:00,  4.14it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5: 100% 652/654 [02:37<00:00,  4.15it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 5: 100% 654/654 [02:37<00:00,  4.16it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 9.0\n",
            "eval_tn: 863.0\n",
            "eval_precision: 0.7692307829856873\n",
            "eval_recall: 0.5263158082962036\n",
            "eval_f1: 0.625\n",
            "eval_mcc: 0.6299183368682861\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5: 100% 654/654 [02:37<00:00,  4.15it/s, loss=0.123, v_num=-1-1, train_loss=0.06]\n",
            "Epoch 6:  83% 543/654 [02:27<00:30,  3.69it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  83% 544/654 [02:27<00:29,  3.68it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  83% 546/654 [02:27<00:29,  3.69it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  84% 548/654 [02:27<00:28,  3.70it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  84% 550/654 [02:28<00:28,  3.71it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  84% 552/654 [02:28<00:27,  3.72it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  85% 554/654 [02:28<00:26,  3.73it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  85% 556/654 [02:28<00:26,  3.74it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  85% 558/654 [02:28<00:25,  3.75it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  86% 560/654 [02:29<00:25,  3.76it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  86% 562/654 [02:29<00:24,  3.77it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  86% 564/654 [02:29<00:23,  3.78it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  87% 566/654 [02:29<00:23,  3.79it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  87% 568/654 [02:29<00:22,  3.79it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  87% 570/654 [02:29<00:22,  3.80it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  87% 572/654 [02:30<00:21,  3.81it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  88% 574/654 [02:30<00:20,  3.82it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  88% 576/654 [02:30<00:20,  3.83it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  88% 578/654 [02:30<00:19,  3.84it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  89% 580/654 [02:30<00:19,  3.85it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  89% 582/654 [02:30<00:18,  3.86it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  89% 584/654 [02:31<00:18,  3.87it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  90% 586/654 [02:31<00:17,  3.88it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  90% 588/654 [02:31<00:16,  3.88it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  90% 590/654 [02:31<00:16,  3.89it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  91% 592/654 [02:31<00:15,  3.90it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  91% 594/654 [02:31<00:15,  3.91it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  91% 596/654 [02:32<00:14,  3.92it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  91% 598/654 [02:32<00:14,  3.93it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  92% 600/654 [02:32<00:13,  3.94it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  92% 602/654 [02:32<00:13,  3.94it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  92% 604/654 [02:32<00:12,  3.95it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  93% 606/654 [02:32<00:12,  3.96it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  93% 608/654 [02:33<00:11,  3.97it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  93% 610/654 [02:33<00:11,  3.98it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  94% 612/654 [02:33<00:10,  3.99it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  94% 614/654 [02:33<00:10,  4.00it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  94% 616/654 [02:33<00:09,  4.00it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  94% 618/654 [02:33<00:08,  4.01it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  95% 620/654 [02:34<00:08,  4.02it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  95% 622/654 [02:34<00:07,  4.03it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  95% 624/654 [02:34<00:07,  4.04it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  96% 626/654 [02:34<00:06,  4.05it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  96% 628/654 [02:34<00:06,  4.06it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  96% 630/654 [02:35<00:05,  4.06it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  97% 632/654 [02:35<00:05,  4.07it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  97% 634/654 [02:35<00:04,  4.08it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  97% 636/654 [02:35<00:04,  4.09it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  98% 638/654 [02:35<00:03,  4.10it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  98% 640/654 [02:35<00:03,  4.11it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  98% 642/654 [02:36<00:02,  4.11it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  98% 644/654 [02:36<00:02,  4.12it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  99% 646/654 [02:36<00:01,  4.13it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  99% 648/654 [02:36<00:01,  4.14it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6:  99% 650/654 [02:36<00:00,  4.15it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6: 100% 652/654 [02:36<00:00,  4.16it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 6: 100% 654/654 [02:37<00:00,  4.17it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 1.0\n",
            "eval_fn: 9.0\n",
            "eval_tn: 865.0\n",
            "eval_precision: 0.9090909361839294\n",
            "eval_recall: 0.5263158082962036\n",
            "eval_f1: 0.6666666865348816\n",
            "eval_mcc: 0.6870307922363281\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6: 100% 654/654 [02:37<00:00,  4.16it/s, loss=0.034, v_num=-1-1, train_loss=0.0589]\n",
            "Epoch 7:  83% 543/654 [02:27<00:30,  3.69it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  83% 544/654 [02:27<00:29,  3.68it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  83% 546/654 [02:27<00:29,  3.69it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  84% 548/654 [02:27<00:28,  3.70it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  84% 550/654 [02:28<00:28,  3.71it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  84% 552/654 [02:28<00:27,  3.72it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  85% 554/654 [02:28<00:26,  3.73it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  85% 556/654 [02:28<00:26,  3.74it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  85% 558/654 [02:28<00:25,  3.75it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  86% 560/654 [02:28<00:25,  3.76it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  86% 562/654 [02:29<00:24,  3.77it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  86% 564/654 [02:29<00:23,  3.78it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  87% 566/654 [02:29<00:23,  3.79it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  87% 568/654 [02:29<00:22,  3.80it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  87% 570/654 [02:29<00:22,  3.80it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  87% 572/654 [02:30<00:21,  3.81it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  88% 574/654 [02:30<00:20,  3.82it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  88% 576/654 [02:30<00:20,  3.83it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  88% 578/654 [02:30<00:19,  3.84it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  89% 580/654 [02:30<00:19,  3.85it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  89% 582/654 [02:30<00:18,  3.86it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  89% 584/654 [02:31<00:18,  3.87it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  90% 586/654 [02:31<00:17,  3.88it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  90% 588/654 [02:31<00:16,  3.88it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  90% 590/654 [02:31<00:16,  3.89it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  91% 592/654 [02:31<00:15,  3.90it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  91% 594/654 [02:31<00:15,  3.91it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  91% 596/654 [02:32<00:14,  3.92it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  91% 598/654 [02:32<00:14,  3.93it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  92% 600/654 [02:32<00:13,  3.94it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  92% 602/654 [02:32<00:13,  3.95it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  92% 604/654 [02:32<00:12,  3.95it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  93% 606/654 [02:32<00:12,  3.96it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  93% 608/654 [02:33<00:11,  3.97it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  93% 610/654 [02:33<00:11,  3.98it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  94% 612/654 [02:33<00:10,  3.99it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  94% 614/654 [02:33<00:10,  4.00it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  94% 616/654 [02:33<00:09,  4.01it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  94% 618/654 [02:33<00:08,  4.01it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  95% 620/654 [02:34<00:08,  4.02it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  95% 622/654 [02:34<00:07,  4.03it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  95% 624/654 [02:34<00:07,  4.04it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  96% 626/654 [02:34<00:06,  4.05it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  96% 628/654 [02:34<00:06,  4.06it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  96% 630/654 [02:34<00:05,  4.07it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  97% 632/654 [02:35<00:05,  4.07it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  97% 634/654 [02:35<00:04,  4.08it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  97% 636/654 [02:35<00:04,  4.09it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  98% 638/654 [02:35<00:03,  4.10it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  98% 640/654 [02:35<00:03,  4.11it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  98% 642/654 [02:35<00:02,  4.12it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  98% 644/654 [02:36<00:02,  4.12it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  99% 646/654 [02:36<00:01,  4.13it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  99% 648/654 [02:36<00:01,  4.14it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7:  99% 650/654 [02:36<00:00,  4.15it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7: 100% 652/654 [02:36<00:00,  4.16it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Epoch 7: 100% 654/654 [02:36<00:00,  4.17it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 10.0\n",
            "eval_fn: 9.0\n",
            "eval_tn: 856.0\n",
            "eval_precision: 0.5\n",
            "eval_recall: 0.5263158082962036\n",
            "eval_f1: 0.5128205418586731\n",
            "eval_mcc: 0.5020241737365723\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 654/654 [02:37<00:00,  4.16it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 654/654 [02:37<00:00,  4.15it/s, loss=0.044, v_num=-1-1, train_loss=0.0557]\n",
            "Computing Input\n",
            "100% 4343/4343 [00:07<00:00, 557.75it/s]\n",
            "100% 885/885 [00:01<00:00, 565.77it/s]\n",
            "100% 77/77 [00:00<00:00, 571.75it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.18it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 9.0\n",
            "fp: 5.0\n",
            "fn: 0.0\n",
            "tn: 63.0\n",
            "precision: 0.6428571343421936\n",
            "recall: 1.0\n",
            "f1: 0.782608687877655\n",
            "mcc: 0.771743655204773\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.9375), 'test_loss': tensor(0.1153, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.02it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"label\": \"ud\",\n",
            "    \"augment\": false,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 768,\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 4319/4319 [00:09<00:00, 455.44it/s]\n",
            "100% 885/885 [00:01<00:00, 508.07it/s]\n",
            "100% 77/77 [00:00<00:00, 503.93it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 592 K \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  1.95it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 0.0\n",
            "eval_fp: 8.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 0.0\n",
            "eval_precision: 0.0\n",
            "eval_recall: 0\n",
            "eval_f1: 0\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  83% 540/651 [02:26<00:30,  3.68it/s, loss=0.166, v_num=-1-2]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  83% 541/651 [02:27<00:29,  3.67it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  83% 543/651 [02:27<00:29,  3.68it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  84% 545/651 [02:27<00:28,  3.69it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  84% 547/651 [02:27<00:28,  3.70it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  84% 549/651 [02:28<00:27,  3.71it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  85% 551/651 [02:28<00:26,  3.72it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  85% 553/651 [02:28<00:26,  3.73it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  85% 555/651 [02:28<00:25,  3.74it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  86% 557/651 [02:28<00:25,  3.75it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  86% 559/651 [02:28<00:24,  3.75it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  86% 561/651 [02:29<00:23,  3.76it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  86% 563/651 [02:29<00:23,  3.77it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  87% 565/651 [02:29<00:22,  3.78it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  87% 567/651 [02:29<00:22,  3.79it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  87% 569/651 [02:29<00:21,  3.80it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  88% 571/651 [02:29<00:21,  3.81it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  88% 573/651 [02:30<00:20,  3.82it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  88% 575/651 [02:30<00:19,  3.83it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  89% 577/651 [02:30<00:19,  3.84it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  89% 579/651 [02:30<00:18,  3.84it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  89% 581/651 [02:30<00:18,  3.85it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  90% 583/651 [02:30<00:17,  3.86it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  90% 585/651 [02:31<00:17,  3.87it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  90% 587/651 [02:31<00:16,  3.88it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  90% 589/651 [02:31<00:15,  3.89it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  91% 591/651 [02:31<00:15,  3.90it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  91% 593/651 [02:31<00:14,  3.91it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  91% 595/651 [02:31<00:14,  3.92it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  92% 597/651 [02:32<00:13,  3.92it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  92% 599/651 [02:32<00:13,  3.93it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  92% 601/651 [02:32<00:12,  3.94it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  93% 603/651 [02:32<00:12,  3.95it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  93% 605/651 [02:32<00:11,  3.96it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  93% 607/651 [02:32<00:11,  3.97it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  94% 609/651 [02:33<00:10,  3.98it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  94% 611/651 [02:33<00:10,  3.99it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  94% 613/651 [02:33<00:09,  3.99it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  94% 615/651 [02:33<00:08,  4.00it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  95% 617/651 [02:33<00:08,  4.01it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  95% 619/651 [02:34<00:07,  4.02it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  95% 621/651 [02:34<00:07,  4.03it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  96% 623/651 [02:34<00:06,  4.04it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  96% 625/651 [02:34<00:06,  4.04it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  96% 627/651 [02:34<00:05,  4.05it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  97% 629/651 [02:34<00:05,  4.06it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  97% 631/651 [02:35<00:04,  4.07it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  97% 633/651 [02:35<00:04,  4.08it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  98% 635/651 [02:35<00:03,  4.09it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  98% 637/651 [02:35<00:03,  4.10it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  98% 639/651 [02:35<00:02,  4.10it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  98% 641/651 [02:35<00:02,  4.11it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  99% 643/651 [02:36<00:01,  4.12it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  99% 645/651 [02:36<00:01,  4.13it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0:  99% 647/651 [02:36<00:00,  4.14it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0: 100% 649/651 [02:36<00:00,  4.15it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 0: 100% 651/651 [02:36<00:00,  4.15it/s, loss=0.166, v_num=-1-2]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 12.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 852.0\n",
            "eval_precision: 0.47826087474823\n",
            "eval_recall: 0.523809552192688\n",
            "eval_f1: 0.5\n",
            "eval_mcc: 0.48781096935272217\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0: 100% 651/651 [02:47<00:00,  3.88it/s, loss=0.166, v_num=-1-2]\n",
            "Epoch 1:  83% 540/651 [02:28<00:30,  3.64it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  83% 542/651 [02:29<00:29,  3.64it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  84% 544/651 [02:29<00:29,  3.65it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  84% 546/651 [02:29<00:28,  3.66it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  84% 548/651 [02:29<00:28,  3.66it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  84% 550/651 [02:29<00:27,  3.67it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  85% 552/651 [02:29<00:26,  3.68it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  85% 554/651 [02:30<00:26,  3.69it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  85% 556/651 [02:30<00:25,  3.70it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  86% 558/651 [02:30<00:25,  3.71it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  86% 560/651 [02:30<00:24,  3.72it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  86% 562/651 [02:30<00:23,  3.73it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  87% 564/651 [02:30<00:23,  3.74it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  87% 566/651 [02:31<00:22,  3.75it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  87% 568/651 [02:31<00:22,  3.76it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  88% 570/651 [02:31<00:21,  3.76it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  88% 572/651 [02:31<00:20,  3.77it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  88% 574/651 [02:31<00:20,  3.78it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  88% 576/651 [02:31<00:19,  3.79it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  89% 578/651 [02:32<00:19,  3.80it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  89% 580/651 [02:32<00:18,  3.81it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  89% 582/651 [02:32<00:18,  3.82it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  90% 584/651 [02:32<00:17,  3.83it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  90% 586/651 [02:32<00:16,  3.84it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  90% 588/651 [02:32<00:16,  3.84it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  91% 590/651 [02:33<00:15,  3.85it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  91% 592/651 [02:33<00:15,  3.86it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  91% 594/651 [02:33<00:14,  3.87it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  92% 596/651 [02:33<00:14,  3.88it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  92% 598/651 [02:33<00:13,  3.89it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  92% 600/651 [02:34<00:13,  3.90it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  92% 602/651 [02:34<00:12,  3.90it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  93% 604/651 [02:34<00:12,  3.91it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  93% 606/651 [02:34<00:11,  3.92it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  93% 608/651 [02:34<00:10,  3.93it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  94% 610/651 [02:34<00:10,  3.94it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  94% 612/651 [02:35<00:09,  3.95it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  94% 614/651 [02:35<00:09,  3.96it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  95% 616/651 [02:35<00:08,  3.96it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  95% 618/651 [02:35<00:08,  3.97it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  95% 620/651 [02:35<00:07,  3.98it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  96% 622/651 [02:35<00:07,  3.99it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  96% 624/651 [02:36<00:06,  4.00it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  96% 626/651 [02:36<00:06,  4.01it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  96% 628/651 [02:36<00:05,  4.02it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  97% 630/651 [02:36<00:05,  4.02it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  97% 632/651 [02:36<00:04,  4.03it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  97% 634/651 [02:36<00:04,  4.04it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  98% 636/651 [02:37<00:03,  4.05it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  98% 638/651 [02:37<00:03,  4.06it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  98% 640/651 [02:37<00:02,  4.07it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  99% 642/651 [02:37<00:02,  4.07it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  99% 644/651 [02:37<00:01,  4.08it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1:  99% 646/651 [02:37<00:01,  4.09it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1: 100% 648/651 [02:38<00:00,  4.10it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 1: 100% 650/651 [02:38<00:00,  4.11it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.44it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 9.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 855.0\n",
            "eval_precision: 0.5909090638160706\n",
            "eval_recall: 0.6190476417541504\n",
            "eval_f1: 0.604651153087616\n",
            "eval_mcc: 0.5949819684028625\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 651/651 [02:49<00:00,  3.84it/s, loss=0.075, v_num=-1-2, train_loss=0.563]\n",
            "Epoch 2:  83% 540/651 [02:27<00:30,  3.66it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  83% 542/651 [02:27<00:29,  3.66it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  84% 544/651 [02:28<00:29,  3.67it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  84% 546/651 [02:28<00:28,  3.68it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  84% 548/651 [02:28<00:27,  3.69it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  84% 550/651 [02:28<00:27,  3.70it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  85% 552/651 [02:28<00:26,  3.71it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  85% 554/651 [02:28<00:26,  3.72it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  85% 556/651 [02:29<00:25,  3.73it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  86% 558/651 [02:29<00:24,  3.74it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  86% 560/651 [02:29<00:24,  3.75it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  86% 562/651 [02:29<00:23,  3.76it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  87% 564/651 [02:29<00:23,  3.76it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  87% 566/651 [02:29<00:22,  3.77it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  87% 568/651 [02:30<00:21,  3.78it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  88% 570/651 [02:30<00:21,  3.79it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  88% 572/651 [02:30<00:20,  3.80it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  88% 574/651 [02:30<00:20,  3.81it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  88% 576/651 [02:30<00:19,  3.82it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  89% 578/651 [02:31<00:19,  3.83it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  89% 580/651 [02:31<00:18,  3.84it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  89% 582/651 [02:31<00:17,  3.85it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  90% 584/651 [02:31<00:17,  3.85it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  90% 586/651 [02:31<00:16,  3.86it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  90% 588/651 [02:31<00:16,  3.87it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  91% 590/651 [02:32<00:15,  3.88it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  91% 592/651 [02:32<00:15,  3.89it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  91% 594/651 [02:32<00:14,  3.90it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  92% 596/651 [02:32<00:14,  3.91it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  92% 598/651 [02:32<00:13,  3.92it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  92% 600/651 [02:32<00:12,  3.92it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  92% 602/651 [02:33<00:12,  3.93it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  93% 604/651 [02:33<00:11,  3.94it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  93% 606/651 [02:33<00:11,  3.95it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  93% 608/651 [02:33<00:10,  3.96it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  94% 610/651 [02:33<00:10,  3.97it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  94% 612/651 [02:33<00:09,  3.98it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  94% 614/651 [02:34<00:09,  3.98it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  95% 616/651 [02:34<00:08,  3.99it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  95% 618/651 [02:34<00:08,  4.00it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  95% 620/651 [02:34<00:07,  4.01it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  96% 622/651 [02:34<00:07,  4.02it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  96% 624/651 [02:34<00:06,  4.03it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  96% 626/651 [02:35<00:06,  4.04it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  96% 628/651 [02:35<00:05,  4.04it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  97% 630/651 [02:35<00:05,  4.05it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  97% 632/651 [02:35<00:04,  4.06it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  97% 634/651 [02:35<00:04,  4.07it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  98% 636/651 [02:35<00:03,  4.08it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  98% 638/651 [02:36<00:03,  4.09it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  98% 640/651 [02:36<00:02,  4.09it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  99% 642/651 [02:36<00:02,  4.10it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  99% 644/651 [02:36<00:01,  4.11it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2:  99% 646/651 [02:36<00:01,  4.12it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2: 100% 648/651 [02:37<00:00,  4.13it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 2: 100% 650/651 [02:37<00:00,  4.13it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.33it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 2.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.8461538553237915\n",
            "eval_recall: 0.523809552192688\n",
            "eval_f1: 0.6470588445663452\n",
            "eval_mcc: 0.6597607731819153\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Epoch 2: 100% 651/651 [02:47<00:00,  3.88it/s, loss=0.077, v_num=-1-2, train_loss=0.113]\n",
            "Epoch 3:  83% 540/651 [02:27<00:30,  3.67it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  83% 541/651 [02:27<00:29,  3.67it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  83% 543/651 [02:27<00:29,  3.68it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  84% 545/651 [02:27<00:28,  3.69it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  84% 547/651 [02:28<00:28,  3.70it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  84% 549/651 [02:28<00:27,  3.70it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  85% 551/651 [02:28<00:26,  3.71it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  85% 553/651 [02:28<00:26,  3.72it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  85% 555/651 [02:28<00:25,  3.73it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  86% 557/651 [02:28<00:25,  3.74it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  86% 559/651 [02:29<00:24,  3.75it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  86% 561/651 [02:29<00:23,  3.76it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  86% 563/651 [02:29<00:23,  3.77it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  87% 565/651 [02:29<00:22,  3.78it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  87% 567/651 [02:29<00:22,  3.79it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  87% 569/651 [02:29<00:21,  3.80it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  88% 571/651 [02:30<00:21,  3.81it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  88% 573/651 [02:30<00:20,  3.81it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  88% 575/651 [02:30<00:19,  3.82it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  89% 577/651 [02:30<00:19,  3.83it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  89% 579/651 [02:30<00:18,  3.84it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  89% 581/651 [02:30<00:18,  3.85it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  90% 583/651 [02:31<00:17,  3.86it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  90% 585/651 [02:31<00:17,  3.87it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  90% 587/651 [02:31<00:16,  3.88it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  90% 589/651 [02:31<00:15,  3.89it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  91% 591/651 [02:31<00:15,  3.89it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  91% 593/651 [02:31<00:14,  3.90it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  91% 595/651 [02:32<00:14,  3.91it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  92% 597/651 [02:32<00:13,  3.92it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  92% 599/651 [02:32<00:13,  3.93it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  92% 601/651 [02:32<00:12,  3.94it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  93% 603/651 [02:32<00:12,  3.95it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  93% 605/651 [02:32<00:11,  3.95it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  93% 607/651 [02:33<00:11,  3.96it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  94% 609/651 [02:33<00:10,  3.97it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  94% 611/651 [02:33<00:10,  3.98it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  94% 613/651 [02:33<00:09,  3.99it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  94% 615/651 [02:33<00:09,  4.00it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  95% 617/651 [02:34<00:08,  4.01it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  95% 619/651 [02:34<00:07,  4.01it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  95% 621/651 [02:34<00:07,  4.02it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  96% 623/651 [02:34<00:06,  4.03it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  96% 625/651 [02:34<00:06,  4.04it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  96% 627/651 [02:34<00:05,  4.05it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  97% 629/651 [02:35<00:05,  4.06it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  97% 631/651 [02:35<00:04,  4.07it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  97% 633/651 [02:35<00:04,  4.07it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  98% 635/651 [02:35<00:03,  4.08it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  98% 637/651 [02:35<00:03,  4.09it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  98% 639/651 [02:35<00:02,  4.10it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  98% 641/651 [02:36<00:02,  4.11it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  99% 643/651 [02:36<00:01,  4.12it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  99% 645/651 [02:36<00:01,  4.12it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3:  99% 647/651 [02:36<00:00,  4.13it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3: 100% 649/651 [02:36<00:00,  4.14it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 3: 100% 651/651 [02:36<00:00,  4.15it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 861.0\n",
            "eval_precision: 0.7857142686843872\n",
            "eval_recall: 0.523809552192688\n",
            "eval_f1: 0.6285714507102966\n",
            "eval_mcc: 0.6347144246101379\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3: 100% 651/651 [02:37<00:00,  4.14it/s, loss=0.029, v_num=-1-2, train_loss=0.0854]\n",
            "Epoch 4:  83% 540/651 [02:26<00:30,  3.68it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  83% 542/651 [02:27<00:29,  3.68it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  84% 544/651 [02:27<00:29,  3.69it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  84% 546/651 [02:27<00:28,  3.69it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  84% 548/651 [02:27<00:27,  3.70it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  84% 550/651 [02:28<00:27,  3.71it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  85% 552/651 [02:28<00:26,  3.72it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  85% 554/651 [02:28<00:25,  3.73it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  85% 556/651 [02:28<00:25,  3.74it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  86% 558/651 [02:28<00:24,  3.75it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  86% 560/651 [02:28<00:24,  3.76it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  86% 562/651 [02:29<00:23,  3.77it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  87% 564/651 [02:29<00:23,  3.78it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  87% 566/651 [02:29<00:22,  3.79it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  87% 568/651 [02:29<00:21,  3.80it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  88% 570/651 [02:29<00:21,  3.80it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  88% 572/651 [02:29<00:20,  3.81it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  88% 574/651 [02:30<00:20,  3.82it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  88% 576/651 [02:30<00:19,  3.83it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  89% 578/651 [02:30<00:19,  3.84it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  89% 580/651 [02:30<00:18,  3.85it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  89% 582/651 [02:30<00:17,  3.86it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  90% 584/651 [02:31<00:17,  3.87it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  90% 586/651 [02:31<00:16,  3.88it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  90% 588/651 [02:31<00:16,  3.89it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  91% 590/651 [02:31<00:15,  3.89it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  91% 592/651 [02:31<00:15,  3.90it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  91% 594/651 [02:31<00:14,  3.91it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  92% 596/651 [02:32<00:14,  3.92it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  92% 598/651 [02:32<00:13,  3.93it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  92% 600/651 [02:32<00:12,  3.94it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  92% 602/651 [02:32<00:12,  3.95it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  93% 604/651 [02:32<00:11,  3.96it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  93% 606/651 [02:32<00:11,  3.96it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  93% 608/651 [02:33<00:10,  3.97it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  94% 610/651 [02:33<00:10,  3.98it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  94% 612/651 [02:33<00:09,  3.99it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  94% 614/651 [02:33<00:09,  4.00it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  95% 616/651 [02:33<00:08,  4.01it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  95% 618/651 [02:33<00:08,  4.02it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  95% 620/651 [02:34<00:07,  4.02it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  96% 622/651 [02:34<00:07,  4.03it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  96% 624/651 [02:34<00:06,  4.04it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  96% 626/651 [02:34<00:06,  4.05it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  96% 628/651 [02:34<00:05,  4.06it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  97% 630/651 [02:34<00:05,  4.07it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  97% 632/651 [02:35<00:04,  4.07it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  97% 634/651 [02:35<00:04,  4.08it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  98% 636/651 [02:35<00:03,  4.09it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  98% 638/651 [02:35<00:03,  4.10it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  98% 640/651 [02:35<00:02,  4.11it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  99% 642/651 [02:35<00:02,  4.12it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  99% 644/651 [02:36<00:01,  4.12it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4:  99% 646/651 [02:36<00:01,  4.13it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4: 100% 648/651 [02:36<00:00,  4.14it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 4: 100% 650/651 [02:36<00:00,  4.15it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.33it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 9.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 855.0\n",
            "eval_precision: 0.5909090638160706\n",
            "eval_recall: 0.6190476417541504\n",
            "eval_f1: 0.604651153087616\n",
            "eval_mcc: 0.5949819684028625\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4: 100% 651/651 [02:37<00:00,  4.14it/s, loss=0.059, v_num=-1-2, train_loss=0.0757]\n",
            "Epoch 5:  83% 540/651 [02:27<00:30,  3.67it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  83% 542/651 [02:27<00:29,  3.67it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  84% 544/651 [02:27<00:29,  3.68it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  84% 546/651 [02:28<00:28,  3.69it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  84% 548/651 [02:28<00:27,  3.70it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  84% 550/651 [02:28<00:27,  3.71it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  85% 552/651 [02:28<00:26,  3.71it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  85% 554/651 [02:28<00:26,  3.72it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  85% 556/651 [02:28<00:25,  3.73it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  86% 558/651 [02:29<00:24,  3.74it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  86% 560/651 [02:29<00:24,  3.75it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  86% 562/651 [02:29<00:23,  3.76it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  87% 564/651 [02:29<00:23,  3.77it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  87% 566/651 [02:29<00:22,  3.78it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  87% 568/651 [02:29<00:21,  3.79it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  88% 570/651 [02:30<00:21,  3.80it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  88% 572/651 [02:30<00:20,  3.80it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  88% 574/651 [02:30<00:20,  3.81it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  88% 576/651 [02:30<00:19,  3.82it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  89% 578/651 [02:30<00:19,  3.83it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  89% 580/651 [02:31<00:18,  3.84it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  89% 582/651 [02:31<00:17,  3.85it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  90% 584/651 [02:31<00:17,  3.86it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  90% 586/651 [02:31<00:16,  3.87it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  90% 588/651 [02:31<00:16,  3.88it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  91% 590/651 [02:31<00:15,  3.88it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  91% 592/651 [02:32<00:15,  3.89it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  91% 594/651 [02:32<00:14,  3.90it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  92% 596/651 [02:32<00:14,  3.91it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  92% 598/651 [02:32<00:13,  3.92it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  92% 600/651 [02:32<00:12,  3.93it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  92% 602/651 [02:32<00:12,  3.94it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  93% 604/651 [02:33<00:11,  3.95it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  93% 606/651 [02:33<00:11,  3.95it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  93% 608/651 [02:33<00:10,  3.96it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  94% 610/651 [02:33<00:10,  3.97it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  94% 612/651 [02:33<00:09,  3.98it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  94% 614/651 [02:33<00:09,  3.99it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  95% 616/651 [02:34<00:08,  4.00it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  95% 618/651 [02:34<00:08,  4.01it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  95% 620/651 [02:34<00:07,  4.01it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  96% 622/651 [02:34<00:07,  4.02it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  96% 624/651 [02:34<00:06,  4.03it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  96% 626/651 [02:34<00:06,  4.04it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  96% 628/651 [02:35<00:05,  4.05it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  97% 630/651 [02:35<00:05,  4.06it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  97% 632/651 [02:35<00:04,  4.06it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  97% 634/651 [02:35<00:04,  4.07it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  98% 636/651 [02:35<00:03,  4.08it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  98% 638/651 [02:35<00:03,  4.09it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  98% 640/651 [02:36<00:02,  4.10it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  99% 642/651 [02:36<00:02,  4.11it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  99% 644/651 [02:36<00:01,  4.12it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5:  99% 646/651 [02:36<00:01,  4.12it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5: 100% 648/651 [02:36<00:00,  4.13it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 5: 100% 650/651 [02:37<00:00,  4.14it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.24it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 12.0\n",
            "eval_fp: 4.0\n",
            "eval_fn: 9.0\n",
            "eval_tn: 860.0\n",
            "eval_precision: 0.75\n",
            "eval_recall: 0.5714285969734192\n",
            "eval_f1: 0.6486486196517944\n",
            "eval_mcc: 0.6474789977073669\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Epoch 5: 100% 651/651 [02:47<00:00,  3.88it/s, loss=0.049, v_num=-1-2, train_loss=0.0577]\n",
            "Epoch 6:  83% 540/651 [02:27<00:30,  3.65it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  83% 541/651 [02:28<00:30,  3.64it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  83% 543/651 [02:28<00:29,  3.65it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  84% 545/651 [02:28<00:28,  3.66it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  84% 547/651 [02:28<00:28,  3.67it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  84% 549/651 [02:29<00:27,  3.68it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  85% 551/651 [02:29<00:27,  3.69it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  85% 553/651 [02:29<00:26,  3.70it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  85% 555/651 [02:29<00:25,  3.71it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  86% 557/651 [02:29<00:25,  3.72it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  86% 559/651 [02:29<00:24,  3.73it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  86% 561/651 [02:30<00:24,  3.74it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  86% 563/651 [02:30<00:23,  3.75it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  87% 565/651 [02:30<00:22,  3.75it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  87% 567/651 [02:30<00:22,  3.76it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  87% 569/651 [02:30<00:21,  3.77it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  88% 571/651 [02:31<00:21,  3.78it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  88% 573/651 [02:31<00:20,  3.79it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  88% 575/651 [02:31<00:20,  3.80it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  89% 577/651 [02:31<00:19,  3.81it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  89% 579/651 [02:31<00:18,  3.82it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  89% 581/651 [02:31<00:18,  3.83it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  90% 583/651 [02:32<00:17,  3.83it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  90% 585/651 [02:32<00:17,  3.84it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  90% 587/651 [02:32<00:16,  3.85it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  90% 589/651 [02:32<00:16,  3.86it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  91% 591/651 [02:32<00:15,  3.87it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  91% 593/651 [02:32<00:14,  3.88it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  91% 595/651 [02:33<00:14,  3.89it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  92% 597/651 [02:33<00:13,  3.90it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  92% 599/651 [02:33<00:13,  3.90it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  92% 601/651 [02:33<00:12,  3.91it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  93% 603/651 [02:33<00:12,  3.92it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  93% 605/651 [02:33<00:11,  3.93it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  93% 607/651 [02:34<00:11,  3.94it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  94% 609/651 [02:34<00:10,  3.95it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  94% 611/651 [02:34<00:10,  3.96it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  94% 613/651 [02:34<00:09,  3.96it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  94% 615/651 [02:34<00:09,  3.97it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  95% 617/651 [02:34<00:08,  3.98it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  95% 619/651 [02:35<00:08,  3.99it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  95% 621/651 [02:35<00:07,  4.00it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  96% 623/651 [02:35<00:06,  4.01it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  96% 625/651 [02:35<00:06,  4.02it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  96% 627/651 [02:35<00:05,  4.02it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  97% 629/651 [02:35<00:05,  4.03it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  97% 631/651 [02:36<00:04,  4.04it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  97% 633/651 [02:36<00:04,  4.05it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  98% 635/651 [02:36<00:03,  4.06it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  98% 637/651 [02:36<00:03,  4.07it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  98% 639/651 [02:36<00:02,  4.07it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  98% 641/651 [02:37<00:02,  4.08it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  99% 643/651 [02:37<00:01,  4.09it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  99% 645/651 [02:37<00:01,  4.10it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6:  99% 647/651 [02:37<00:00,  4.11it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6: 100% 649/651 [02:37<00:00,  4.12it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 6: 100% 651/651 [02:37<00:00,  4.12it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 2.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.8461538553237915\n",
            "eval_recall: 0.523809552192688\n",
            "eval_f1: 0.6470588445663452\n",
            "eval_mcc: 0.6597607731819153\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6: 100% 651/651 [02:38<00:00,  4.11it/s, loss=0.018, v_num=-1-2, train_loss=0.0556]\n",
            "Epoch 7:  83% 540/651 [02:27<00:30,  3.67it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  83% 542/651 [02:27<00:29,  3.67it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  84% 544/651 [02:28<00:29,  3.68it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  84% 546/651 [02:28<00:28,  3.68it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  84% 548/651 [02:28<00:27,  3.69it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  84% 550/651 [02:28<00:27,  3.70it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  85% 552/651 [02:28<00:26,  3.71it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  85% 554/651 [02:28<00:26,  3.72it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  85% 556/651 [02:29<00:25,  3.73it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  86% 558/651 [02:29<00:24,  3.74it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  86% 560/651 [02:29<00:24,  3.75it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  86% 562/651 [02:29<00:23,  3.76it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  87% 564/651 [02:29<00:23,  3.77it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  87% 566/651 [02:29<00:22,  3.78it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  87% 568/651 [02:30<00:21,  3.79it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  88% 570/651 [02:30<00:21,  3.79it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  88% 572/651 [02:30<00:20,  3.80it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  88% 574/651 [02:30<00:20,  3.81it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  88% 576/651 [02:30<00:19,  3.82it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  89% 578/651 [02:30<00:19,  3.83it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  89% 580/651 [02:31<00:18,  3.84it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  89% 582/651 [02:31<00:17,  3.85it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  90% 584/651 [02:31<00:17,  3.86it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  90% 586/651 [02:31<00:16,  3.87it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  90% 588/651 [02:31<00:16,  3.87it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  91% 590/651 [02:31<00:15,  3.88it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  91% 592/651 [02:32<00:15,  3.89it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  91% 594/651 [02:32<00:14,  3.90it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  92% 596/651 [02:32<00:14,  3.91it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  92% 598/651 [02:32<00:13,  3.92it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  92% 600/651 [02:32<00:12,  3.93it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  92% 602/651 [02:32<00:12,  3.94it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  93% 604/651 [02:33<00:11,  3.94it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  93% 606/651 [02:33<00:11,  3.95it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  93% 608/651 [02:33<00:10,  3.96it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  94% 610/651 [02:33<00:10,  3.97it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  94% 612/651 [02:33<00:09,  3.98it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  94% 614/651 [02:33<00:09,  3.99it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  95% 616/651 [02:34<00:08,  4.00it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  95% 618/651 [02:34<00:08,  4.00it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  95% 620/651 [02:34<00:07,  4.01it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  96% 622/651 [02:34<00:07,  4.02it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  96% 624/651 [02:34<00:06,  4.03it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  96% 626/651 [02:35<00:06,  4.04it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  96% 628/651 [02:35<00:05,  4.05it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  97% 630/651 [02:35<00:05,  4.06it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  97% 632/651 [02:35<00:04,  4.06it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  97% 634/651 [02:35<00:04,  4.07it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  98% 636/651 [02:35<00:03,  4.08it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  98% 638/651 [02:36<00:03,  4.09it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  98% 640/651 [02:36<00:02,  4.10it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  99% 642/651 [02:36<00:02,  4.11it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  99% 644/651 [02:36<00:01,  4.11it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7:  99% 646/651 [02:36<00:01,  4.12it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7: 100% 648/651 [02:36<00:00,  4.13it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Epoch 7: 100% 650/651 [02:37<00:00,  4.14it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.54it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 2.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 862.0\n",
            "eval_precision: 0.8461538553237915\n",
            "eval_recall: 0.523809552192688\n",
            "eval_f1: 0.6470588445663452\n",
            "eval_mcc: 0.6597607731819153\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 651/651 [02:37<00:00,  4.14it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 651/651 [02:37<00:00,  4.13it/s, loss=0.051, v_num=-1-2, train_loss=0.0549]\n",
            "Computing Input\n",
            "100% 4319/4319 [00:07<00:00, 589.68it/s]\n",
            "100% 885/885 [00:01<00:00, 580.55it/s]\n",
            "100% 77/77 [00:00<00:00, 593.84it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.58it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 9.0\n",
            "fp: 3.0\n",
            "fn: 0.0\n",
            "tn: 65.0\n",
            "precision: 0.75\n",
            "recall: 1.0\n",
            "f1: 0.8571428656578064\n",
            "mcc: 0.8467064499855042\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.9625), 'test_loss': tensor(0.0600, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.68it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/3\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"label\": \"ud\",\n",
            "    \"augment\": false,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 768,\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 4355/4355 [00:09<00:00, 466.83it/s]\n",
            "100% 885/885 [00:01<00:00, 523.21it/s]\n",
            "100% 77/77 [00:00<00:00, 483.15it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 592 K \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.23it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 0.0\n",
            "eval_fp: 8.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 0.0\n",
            "eval_precision: 0.0\n",
            "eval_recall: 0\n",
            "eval_f1: 0\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  83% 545/656 [02:27<00:30,  3.69it/s, loss=0.156, v_num=-1-3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  83% 546/656 [02:27<00:29,  3.69it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  83% 547/656 [02:28<00:29,  3.69it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  84% 549/656 [02:28<00:28,  3.70it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  84% 551/656 [02:28<00:28,  3.71it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  84% 553/656 [02:28<00:27,  3.72it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  85% 555/656 [02:28<00:27,  3.73it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  85% 557/656 [02:29<00:26,  3.74it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  85% 559/656 [02:29<00:25,  3.75it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  86% 561/656 [02:29<00:25,  3.75it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  86% 563/656 [02:29<00:24,  3.76it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  86% 565/656 [02:29<00:24,  3.77it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  86% 567/656 [02:29<00:23,  3.78it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  87% 569/656 [02:30<00:22,  3.79it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  87% 571/656 [02:30<00:22,  3.80it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  87% 573/656 [02:30<00:21,  3.81it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  88% 575/656 [02:30<00:21,  3.82it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  88% 577/656 [02:30<00:20,  3.83it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  88% 579/656 [02:30<00:20,  3.84it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  89% 581/656 [02:31<00:19,  3.84it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  89% 583/656 [02:31<00:18,  3.85it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  89% 585/656 [02:31<00:18,  3.86it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  89% 587/656 [02:31<00:17,  3.87it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  90% 589/656 [02:31<00:17,  3.88it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  90% 591/656 [02:32<00:16,  3.89it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  90% 593/656 [02:32<00:16,  3.90it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  91% 595/656 [02:32<00:15,  3.91it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  91% 597/656 [02:32<00:15,  3.91it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  91% 599/656 [02:32<00:14,  3.92it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  92% 601/656 [02:32<00:13,  3.93it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  92% 603/656 [02:33<00:13,  3.94it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  92% 605/656 [02:33<00:12,  3.95it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  93% 607/656 [02:33<00:12,  3.96it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  93% 609/656 [02:33<00:11,  3.96it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  93% 611/656 [02:33<00:11,  3.97it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  93% 613/656 [02:33<00:10,  3.98it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  94% 615/656 [02:34<00:10,  3.99it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  94% 617/656 [02:34<00:09,  4.00it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  94% 619/656 [02:34<00:09,  4.01it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  95% 621/656 [02:34<00:08,  4.02it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  95% 623/656 [02:34<00:08,  4.02it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  95% 625/656 [02:34<00:07,  4.03it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  96% 627/656 [02:35<00:07,  4.04it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  96% 629/656 [02:35<00:06,  4.05it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  96% 631/656 [02:35<00:06,  4.06it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  96% 633/656 [02:35<00:05,  4.07it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  97% 635/656 [02:35<00:05,  4.08it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  97% 637/656 [02:35<00:04,  4.08it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  97% 639/656 [02:36<00:04,  4.09it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  98% 641/656 [02:36<00:03,  4.10it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  98% 643/656 [02:36<00:03,  4.11it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  98% 645/656 [02:36<00:02,  4.12it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  99% 647/656 [02:36<00:02,  4.13it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  99% 649/656 [02:36<00:01,  4.13it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0:  99% 651/656 [02:37<00:01,  4.14it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0: 100% 653/656 [02:37<00:00,  4.15it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 0: 100% 655/656 [02:37<00:00,  4.16it/s, loss=0.156, v_num=-1-3]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 864.0\n",
            "eval_precision: 0.7692307829856873\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.6451612710952759\n",
            "eval_mcc: 0.647783637046814\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0: 100% 656/656 [02:47<00:00,  3.91it/s, loss=0.156, v_num=-1-3]\n",
            "Epoch 1:  83% 545/656 [02:29<00:30,  3.65it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  83% 546/656 [02:29<00:30,  3.65it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  84% 548/656 [02:29<00:29,  3.66it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  84% 550/656 [02:29<00:28,  3.67it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  84% 552/656 [02:30<00:28,  3.68it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  84% 554/656 [02:30<00:27,  3.69it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  85% 556/656 [02:30<00:27,  3.69it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  85% 558/656 [02:30<00:26,  3.70it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  85% 560/656 [02:30<00:25,  3.71it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  86% 562/656 [02:31<00:25,  3.72it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  86% 564/656 [02:31<00:24,  3.73it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  86% 566/656 [02:31<00:24,  3.74it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  87% 568/656 [02:31<00:23,  3.75it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  87% 570/656 [02:31<00:22,  3.76it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  87% 572/656 [02:31<00:22,  3.77it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  88% 574/656 [02:32<00:21,  3.78it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  88% 576/656 [02:32<00:21,  3.78it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  88% 578/656 [02:32<00:20,  3.79it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  88% 580/656 [02:32<00:19,  3.80it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  89% 582/656 [02:32<00:19,  3.81it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  89% 584/656 [02:32<00:18,  3.82it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  89% 586/656 [02:33<00:18,  3.83it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  90% 588/656 [02:33<00:17,  3.84it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  90% 590/656 [02:33<00:17,  3.85it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  90% 592/656 [02:33<00:16,  3.85it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  91% 594/656 [02:33<00:16,  3.86it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  91% 596/656 [02:33<00:15,  3.87it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  91% 598/656 [02:34<00:14,  3.88it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  91% 600/656 [02:34<00:14,  3.89it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  92% 602/656 [02:34<00:13,  3.90it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  92% 604/656 [02:34<00:13,  3.91it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  92% 606/656 [02:34<00:12,  3.92it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  93% 608/656 [02:34<00:12,  3.92it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  93% 610/656 [02:35<00:11,  3.93it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  93% 612/656 [02:35<00:11,  3.94it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  94% 614/656 [02:35<00:10,  3.95it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  94% 616/656 [02:35<00:10,  3.96it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  94% 618/656 [02:35<00:09,  3.97it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  95% 620/656 [02:35<00:09,  3.98it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  95% 622/656 [02:36<00:08,  3.98it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  95% 624/656 [02:36<00:08,  3.99it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  95% 626/656 [02:36<00:07,  4.00it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  96% 628/656 [02:36<00:06,  4.01it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  96% 630/656 [02:36<00:06,  4.02it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  96% 632/656 [02:37<00:05,  4.03it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  97% 634/656 [02:37<00:05,  4.03it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  97% 636/656 [02:37<00:04,  4.04it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  97% 638/656 [02:37<00:04,  4.05it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  98% 640/656 [02:37<00:03,  4.06it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  98% 642/656 [02:37<00:03,  4.07it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  98% 644/656 [02:38<00:02,  4.08it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  98% 646/656 [02:38<00:02,  4.08it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  99% 648/656 [02:38<00:01,  4.09it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  99% 650/656 [02:38<00:01,  4.10it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1:  99% 652/656 [02:38<00:00,  4.11it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1: 100% 654/656 [02:38<00:00,  4.12it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 1: 100% 656/656 [02:39<00:00,  4.13it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 12.0\n",
            "eval_fp: 13.0\n",
            "eval_fn: 6.0\n",
            "eval_tn: 854.0\n",
            "eval_precision: 0.47999998927116394\n",
            "eval_recall: 0.6666666865348816\n",
            "eval_f1: 0.5581395626068115\n",
            "eval_mcc: 0.5552082657814026\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 656/656 [02:39<00:00,  4.12it/s, loss=0.094, v_num=-1-3, train_loss=0.493]\n",
            "Epoch 2:  83% 545/656 [02:28<00:30,  3.68it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  83% 546/656 [02:28<00:29,  3.67it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  84% 548/656 [02:28<00:29,  3.68it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  84% 550/656 [02:29<00:28,  3.69it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  84% 552/656 [02:29<00:28,  3.70it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  84% 554/656 [02:29<00:27,  3.71it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  85% 556/656 [02:29<00:26,  3.72it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  85% 558/656 [02:29<00:26,  3.73it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  85% 560/656 [02:29<00:25,  3.73it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  86% 562/656 [02:30<00:25,  3.74it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  86% 564/656 [02:30<00:24,  3.75it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  86% 566/656 [02:30<00:23,  3.76it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  87% 568/656 [02:30<00:23,  3.77it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  87% 570/656 [02:30<00:22,  3.78it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  87% 572/656 [02:30<00:22,  3.79it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  88% 574/656 [02:31<00:21,  3.80it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  88% 576/656 [02:31<00:21,  3.81it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  88% 578/656 [02:31<00:20,  3.82it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  88% 580/656 [02:31<00:19,  3.82it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  89% 582/656 [02:31<00:19,  3.83it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  89% 584/656 [02:31<00:18,  3.84it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  89% 586/656 [02:32<00:18,  3.85it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  90% 588/656 [02:32<00:17,  3.86it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  90% 590/656 [02:32<00:17,  3.87it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  90% 592/656 [02:32<00:16,  3.88it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  91% 594/656 [02:32<00:15,  3.89it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  91% 596/656 [02:33<00:15,  3.90it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  91% 598/656 [02:33<00:14,  3.90it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  91% 600/656 [02:33<00:14,  3.91it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  92% 602/656 [02:33<00:13,  3.92it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  92% 604/656 [02:33<00:13,  3.93it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  92% 606/656 [02:33<00:12,  3.94it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  93% 608/656 [02:34<00:12,  3.95it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  93% 610/656 [02:34<00:11,  3.96it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  93% 612/656 [02:34<00:11,  3.96it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  94% 614/656 [02:34<00:10,  3.97it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  94% 616/656 [02:34<00:10,  3.98it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  94% 618/656 [02:34<00:09,  3.99it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  95% 620/656 [02:35<00:09,  4.00it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  95% 622/656 [02:35<00:08,  4.01it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  95% 624/656 [02:35<00:07,  4.01it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  95% 626/656 [02:35<00:07,  4.02it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  96% 628/656 [02:35<00:06,  4.03it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  96% 630/656 [02:35<00:06,  4.04it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  96% 632/656 [02:36<00:05,  4.05it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  97% 634/656 [02:36<00:05,  4.06it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  97% 636/656 [02:36<00:04,  4.06it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  97% 638/656 [02:36<00:04,  4.07it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  98% 640/656 [02:36<00:03,  4.08it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  98% 642/656 [02:36<00:03,  4.09it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  98% 644/656 [02:37<00:02,  4.10it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  98% 646/656 [02:37<00:02,  4.11it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  99% 648/656 [02:37<00:01,  4.11it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  99% 650/656 [02:37<00:01,  4.12it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2:  99% 652/656 [02:37<00:00,  4.13it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2: 100% 654/656 [02:38<00:00,  4.14it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 2: 100% 656/656 [02:38<00:00,  4.15it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 12.0\n",
            "eval_fp: 22.0\n",
            "eval_fn: 6.0\n",
            "eval_tn: 845.0\n",
            "eval_precision: 0.3529411852359772\n",
            "eval_recall: 0.6666666865348816\n",
            "eval_f1: 0.4615384638309479\n",
            "eval_mcc: 0.4709743559360504\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2: 100% 656/656 [02:38<00:00,  4.14it/s, loss=0.144, v_num=-1-3, train_loss=0.0911]\n",
            "Epoch 3:  83% 545/656 [02:28<00:30,  3.67it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  83% 546/656 [02:28<00:29,  3.67it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  84% 548/656 [02:28<00:29,  3.68it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  84% 550/656 [02:29<00:28,  3.69it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  84% 552/656 [02:29<00:28,  3.70it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  84% 554/656 [02:29<00:27,  3.71it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  85% 556/656 [02:29<00:26,  3.72it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  85% 558/656 [02:29<00:26,  3.72it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  85% 560/656 [02:29<00:25,  3.73it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  86% 562/656 [02:30<00:25,  3.74it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  86% 564/656 [02:30<00:24,  3.75it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  86% 566/656 [02:30<00:23,  3.76it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  87% 568/656 [02:30<00:23,  3.77it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  87% 570/656 [02:30<00:22,  3.78it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  87% 572/656 [02:31<00:22,  3.79it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  88% 574/656 [02:31<00:21,  3.80it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  88% 576/656 [02:31<00:21,  3.81it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  88% 578/656 [02:31<00:20,  3.81it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  88% 580/656 [02:31<00:19,  3.82it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  89% 582/656 [02:31<00:19,  3.83it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  89% 584/656 [02:32<00:18,  3.84it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  89% 586/656 [02:32<00:18,  3.85it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  90% 588/656 [02:32<00:17,  3.86it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  90% 590/656 [02:32<00:17,  3.87it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  90% 592/656 [02:32<00:16,  3.88it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  91% 594/656 [02:32<00:15,  3.89it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  91% 596/656 [02:33<00:15,  3.89it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  91% 598/656 [02:33<00:14,  3.90it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  91% 600/656 [02:33<00:14,  3.91it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  92% 602/656 [02:33<00:13,  3.92it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  92% 604/656 [02:33<00:13,  3.93it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  92% 606/656 [02:33<00:12,  3.94it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  93% 608/656 [02:34<00:12,  3.95it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  93% 610/656 [02:34<00:11,  3.95it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  93% 612/656 [02:34<00:11,  3.96it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  94% 614/656 [02:34<00:10,  3.97it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  94% 616/656 [02:34<00:10,  3.98it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  94% 618/656 [02:34<00:09,  3.99it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  95% 620/656 [02:35<00:09,  4.00it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  95% 622/656 [02:35<00:08,  4.01it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  95% 624/656 [02:35<00:07,  4.01it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  95% 626/656 [02:35<00:07,  4.02it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  96% 628/656 [02:35<00:06,  4.03it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  96% 630/656 [02:35<00:06,  4.04it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  96% 632/656 [02:36<00:05,  4.05it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  97% 634/656 [02:36<00:05,  4.06it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  97% 636/656 [02:36<00:04,  4.06it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  97% 638/656 [02:36<00:04,  4.07it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  98% 640/656 [02:36<00:03,  4.08it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  98% 642/656 [02:36<00:03,  4.09it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  98% 644/656 [02:37<00:02,  4.10it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  98% 646/656 [02:37<00:02,  4.11it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  99% 648/656 [02:37<00:01,  4.11it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  99% 650/656 [02:37<00:01,  4.12it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3:  99% 652/656 [02:37<00:00,  4.13it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3: 100% 654/656 [02:37<00:00,  4.14it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3: 100% 656/656 [02:38<00:00,  4.15it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 1.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 866.0\n",
            "eval_precision: 0.9090909361839294\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.6896551847457886\n",
            "eval_mcc: 0.7063478827476501\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3: 100% 656/656 [02:49<00:00,  3.88it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 3: 100% 656/656 [02:49<00:00,  3.88it/s, loss=0.074, v_num=-1-3, train_loss=0.0731]\n",
            "Epoch 4:  83% 545/656 [02:28<00:30,  3.66it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  83% 546/656 [02:29<00:30,  3.65it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  84% 548/656 [02:29<00:29,  3.66it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  84% 550/656 [02:29<00:28,  3.67it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  84% 552/656 [02:30<00:28,  3.68it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  84% 554/656 [02:30<00:27,  3.69it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  85% 556/656 [02:30<00:27,  3.70it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  85% 558/656 [02:30<00:26,  3.71it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  85% 560/656 [02:30<00:25,  3.72it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  86% 562/656 [02:30<00:25,  3.72it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  86% 564/656 [02:31<00:24,  3.73it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  86% 566/656 [02:31<00:24,  3.74it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  87% 568/656 [02:31<00:23,  3.75it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  87% 570/656 [02:31<00:22,  3.76it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  87% 572/656 [02:31<00:22,  3.77it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  88% 574/656 [02:31<00:21,  3.78it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  88% 576/656 [02:32<00:21,  3.79it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  88% 578/656 [02:32<00:20,  3.80it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  88% 580/656 [02:32<00:19,  3.81it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  89% 582/656 [02:32<00:19,  3.81it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  89% 584/656 [02:32<00:18,  3.82it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  89% 586/656 [02:32<00:18,  3.83it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  90% 588/656 [02:33<00:17,  3.84it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  90% 590/656 [02:33<00:17,  3.85it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  90% 592/656 [02:33<00:16,  3.86it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  91% 594/656 [02:33<00:16,  3.87it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  91% 596/656 [02:33<00:15,  3.88it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  91% 598/656 [02:33<00:14,  3.88it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  91% 600/656 [02:34<00:14,  3.89it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  92% 602/656 [02:34<00:13,  3.90it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  92% 604/656 [02:34<00:13,  3.91it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  92% 606/656 [02:34<00:12,  3.92it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  93% 608/656 [02:34<00:12,  3.93it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  93% 610/656 [02:35<00:11,  3.94it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  93% 612/656 [02:35<00:11,  3.94it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  94% 614/656 [02:35<00:10,  3.95it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  94% 616/656 [02:35<00:10,  3.96it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  94% 618/656 [02:35<00:09,  3.97it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  95% 620/656 [02:35<00:09,  3.98it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  95% 622/656 [02:36<00:08,  3.99it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  95% 624/656 [02:36<00:08,  4.00it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  95% 626/656 [02:36<00:07,  4.00it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  96% 628/656 [02:36<00:06,  4.01it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  96% 630/656 [02:36<00:06,  4.02it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  96% 632/656 [02:36<00:05,  4.03it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  97% 634/656 [02:37<00:05,  4.04it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  97% 636/656 [02:37<00:04,  4.05it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  97% 638/656 [02:37<00:04,  4.05it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  98% 640/656 [02:37<00:03,  4.06it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  98% 642/656 [02:37<00:03,  4.07it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  98% 644/656 [02:37<00:02,  4.08it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  98% 646/656 [02:38<00:02,  4.09it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  99% 648/656 [02:38<00:01,  4.10it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  99% 650/656 [02:38<00:01,  4.10it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4:  99% 652/656 [02:38<00:00,  4.11it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4: 100% 654/656 [02:38<00:00,  4.12it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 4: 100% 656/656 [02:38<00:00,  4.13it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 2.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 865.0\n",
            "eval_precision: 0.8333333134651184\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.6666666865348816\n",
            "eval_mcc: 0.675256073474884\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4: 100% 656/656 [02:39<00:00,  4.12it/s, loss=0.084, v_num=-1-3, train_loss=0.0573]\n",
            "Epoch 5:  83% 545/656 [02:28<00:30,  3.68it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  83% 546/656 [02:28<00:29,  3.67it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  84% 548/656 [02:28<00:29,  3.68it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  84% 550/656 [02:28<00:28,  3.69it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  84% 552/656 [02:29<00:28,  3.70it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  84% 554/656 [02:29<00:27,  3.71it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  85% 556/656 [02:29<00:26,  3.72it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  85% 558/656 [02:29<00:26,  3.73it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  85% 560/656 [02:29<00:25,  3.74it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  86% 562/656 [02:29<00:25,  3.75it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  86% 564/656 [02:30<00:24,  3.76it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  86% 566/656 [02:30<00:23,  3.77it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  87% 568/656 [02:30<00:23,  3.77it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  87% 570/656 [02:30<00:22,  3.78it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  87% 572/656 [02:30<00:22,  3.79it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  88% 574/656 [02:31<00:21,  3.80it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  88% 576/656 [02:31<00:20,  3.81it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  88% 578/656 [02:31<00:20,  3.82it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  88% 580/656 [02:31<00:19,  3.83it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  89% 582/656 [02:31<00:19,  3.84it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  89% 584/656 [02:31<00:18,  3.85it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  89% 586/656 [02:32<00:18,  3.85it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  90% 588/656 [02:32<00:17,  3.86it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  90% 590/656 [02:32<00:17,  3.87it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  90% 592/656 [02:32<00:16,  3.88it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  91% 594/656 [02:32<00:15,  3.89it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  91% 596/656 [02:32<00:15,  3.90it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  91% 598/656 [02:33<00:14,  3.91it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  91% 600/656 [02:33<00:14,  3.92it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  92% 602/656 [02:33<00:13,  3.92it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  92% 604/656 [02:33<00:13,  3.93it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  92% 606/656 [02:33<00:12,  3.94it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  93% 608/656 [02:33<00:12,  3.95it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  93% 610/656 [02:34<00:11,  3.96it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  93% 612/656 [02:34<00:11,  3.97it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  94% 614/656 [02:34<00:10,  3.98it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  94% 616/656 [02:34<00:10,  3.99it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  94% 618/656 [02:34<00:09,  3.99it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  95% 620/656 [02:34<00:08,  4.00it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  95% 622/656 [02:35<00:08,  4.01it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  95% 624/656 [02:35<00:07,  4.02it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  95% 626/656 [02:35<00:07,  4.03it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  96% 628/656 [02:35<00:06,  4.04it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  96% 630/656 [02:35<00:06,  4.04it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  96% 632/656 [02:35<00:05,  4.05it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  97% 634/656 [02:36<00:05,  4.06it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  97% 636/656 [02:36<00:04,  4.07it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  97% 638/656 [02:36<00:04,  4.08it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  98% 640/656 [02:36<00:03,  4.09it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  98% 642/656 [02:36<00:03,  4.09it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  98% 644/656 [02:36<00:02,  4.10it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  98% 646/656 [02:37<00:02,  4.11it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  99% 648/656 [02:37<00:01,  4.12it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  99% 650/656 [02:37<00:01,  4.13it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5:  99% 652/656 [02:37<00:00,  4.14it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5: 100% 654/656 [02:37<00:00,  4.14it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 5: 100% 656/656 [02:37<00:00,  4.15it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 4.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 863.0\n",
            "eval_precision: 0.7142857313156128\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.625\n",
            "eval_mcc: 0.6232733130455017\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5: 100% 656/656 [02:38<00:00,  4.15it/s, loss=0.028, v_num=-1-3, train_loss=0.0555]\n",
            "Epoch 6:  83% 545/656 [02:28<00:30,  3.67it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  83% 546/656 [02:28<00:30,  3.67it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  84% 548/656 [02:29<00:29,  3.68it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  84% 550/656 [02:29<00:28,  3.68it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  84% 552/656 [02:29<00:28,  3.69it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  84% 554/656 [02:29<00:27,  3.70it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  85% 556/656 [02:29<00:26,  3.71it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  85% 558/656 [02:29<00:26,  3.72it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  85% 560/656 [02:30<00:25,  3.73it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  86% 562/656 [02:30<00:25,  3.74it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  86% 564/656 [02:30<00:24,  3.75it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  86% 566/656 [02:30<00:23,  3.76it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  87% 568/656 [02:30<00:23,  3.77it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  87% 570/656 [02:30<00:22,  3.78it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  87% 572/656 [02:31<00:22,  3.78it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  88% 574/656 [02:31<00:21,  3.79it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  88% 576/656 [02:31<00:21,  3.80it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  88% 578/656 [02:31<00:20,  3.81it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  88% 580/656 [02:31<00:19,  3.82it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  89% 582/656 [02:31<00:19,  3.83it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  89% 584/656 [02:32<00:18,  3.84it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  89% 586/656 [02:32<00:18,  3.85it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  90% 588/656 [02:32<00:17,  3.86it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  90% 590/656 [02:32<00:17,  3.86it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  90% 592/656 [02:32<00:16,  3.87it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  91% 594/656 [02:33<00:15,  3.88it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  91% 596/656 [02:33<00:15,  3.89it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  91% 598/656 [02:33<00:14,  3.90it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  91% 600/656 [02:33<00:14,  3.91it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  92% 602/656 [02:33<00:13,  3.92it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  92% 604/656 [02:33<00:13,  3.93it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  92% 606/656 [02:34<00:12,  3.93it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  93% 608/656 [02:34<00:12,  3.94it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  93% 610/656 [02:34<00:11,  3.95it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  93% 612/656 [02:34<00:11,  3.96it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  94% 614/656 [02:34<00:10,  3.97it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  94% 616/656 [02:34<00:10,  3.98it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  94% 618/656 [02:35<00:09,  3.99it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  95% 620/656 [02:35<00:09,  3.99it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  95% 622/656 [02:35<00:08,  4.00it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  95% 624/656 [02:35<00:07,  4.01it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  95% 626/656 [02:35<00:07,  4.02it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  96% 628/656 [02:35<00:06,  4.03it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  96% 630/656 [02:36<00:06,  4.04it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  96% 632/656 [02:36<00:05,  4.05it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  97% 634/656 [02:36<00:05,  4.05it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  97% 636/656 [02:36<00:04,  4.06it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  97% 638/656 [02:36<00:04,  4.07it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  98% 640/656 [02:36<00:03,  4.08it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  98% 642/656 [02:37<00:03,  4.09it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  98% 644/656 [02:37<00:02,  4.10it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  98% 646/656 [02:37<00:02,  4.10it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  99% 648/656 [02:37<00:01,  4.11it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  99% 650/656 [02:37<00:01,  4.12it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6:  99% 652/656 [02:37<00:00,  4.13it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6: 100% 654/656 [02:38<00:00,  4.14it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 6: 100% 656/656 [02:38<00:00,  4.15it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 864.0\n",
            "eval_precision: 0.7692307829856873\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.6451612710952759\n",
            "eval_mcc: 0.647783637046814\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6: 100% 656/656 [02:38<00:00,  4.14it/s, loss=0.038, v_num=-1-3, train_loss=0.0462]\n",
            "Epoch 7:  83% 545/656 [02:28<00:30,  3.68it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  83% 546/656 [02:28<00:29,  3.67it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  84% 548/656 [02:28<00:29,  3.68it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  84% 550/656 [02:29<00:28,  3.69it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  84% 552/656 [02:29<00:28,  3.70it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  84% 554/656 [02:29<00:27,  3.71it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  85% 556/656 [02:29<00:26,  3.72it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  85% 558/656 [02:29<00:26,  3.73it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  85% 560/656 [02:29<00:25,  3.74it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  86% 562/656 [02:30<00:25,  3.75it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  86% 564/656 [02:30<00:24,  3.75it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  86% 566/656 [02:30<00:23,  3.76it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  87% 568/656 [02:30<00:23,  3.77it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  87% 570/656 [02:30<00:22,  3.78it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  87% 572/656 [02:30<00:22,  3.79it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  88% 574/656 [02:31<00:21,  3.80it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  88% 576/656 [02:31<00:21,  3.81it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  88% 578/656 [02:31<00:20,  3.82it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  88% 580/656 [02:31<00:19,  3.83it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  89% 582/656 [02:31<00:19,  3.84it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  89% 584/656 [02:31<00:18,  3.84it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  89% 586/656 [02:32<00:18,  3.85it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  90% 588/656 [02:32<00:17,  3.86it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  90% 590/656 [02:32<00:17,  3.87it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  90% 592/656 [02:32<00:16,  3.88it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  91% 594/656 [02:32<00:15,  3.89it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  91% 596/656 [02:32<00:15,  3.90it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  91% 598/656 [02:33<00:14,  3.91it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  91% 600/656 [02:33<00:14,  3.91it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  92% 602/656 [02:33<00:13,  3.92it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  92% 604/656 [02:33<00:13,  3.93it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  92% 606/656 [02:33<00:12,  3.94it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  93% 608/656 [02:33<00:12,  3.95it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  93% 610/656 [02:34<00:11,  3.96it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  93% 612/656 [02:34<00:11,  3.97it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  94% 614/656 [02:34<00:10,  3.97it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  94% 616/656 [02:34<00:10,  3.98it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  94% 618/656 [02:34<00:09,  3.99it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  95% 620/656 [02:34<00:08,  4.00it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  95% 622/656 [02:35<00:08,  4.01it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  95% 624/656 [02:35<00:07,  4.02it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  95% 626/656 [02:35<00:07,  4.03it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  96% 628/656 [02:35<00:06,  4.03it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  96% 630/656 [02:35<00:06,  4.04it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  96% 632/656 [02:36<00:05,  4.05it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  97% 634/656 [02:36<00:05,  4.06it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  97% 636/656 [02:36<00:04,  4.07it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  97% 638/656 [02:36<00:04,  4.08it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  98% 640/656 [02:36<00:03,  4.08it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  98% 642/656 [02:36<00:03,  4.09it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  98% 644/656 [02:37<00:02,  4.10it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  98% 646/656 [02:37<00:02,  4.11it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  99% 648/656 [02:37<00:01,  4.12it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  99% 650/656 [02:37<00:01,  4.13it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7:  99% 652/656 [02:37<00:00,  4.13it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7: 100% 654/656 [02:37<00:00,  4.14it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Epoch 7: 100% 656/656 [02:38<00:00,  4.15it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 864.0\n",
            "eval_precision: 0.7692307829856873\n",
            "eval_recall: 0.5555555820465088\n",
            "eval_f1: 0.6451612710952759\n",
            "eval_mcc: 0.647783637046814\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 656/656 [02:38<00:00,  4.14it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 656/656 [02:38<00:00,  4.14it/s, loss=0.042, v_num=-1-3, train_loss=0.0476]\n",
            "Computing Input\n",
            "100% 4355/4355 [00:07<00:00, 548.95it/s]\n",
            "100% 885/885 [00:01<00:00, 567.36it/s]\n",
            "100% 77/77 [00:00<00:00, 568.88it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.46it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 9.0\n",
            "fp: 3.0\n",
            "fn: 0.0\n",
            "tn: 65.0\n",
            "precision: 0.75\n",
            "recall: 1.0\n",
            "f1: 0.8571428656578064\n",
            "mcc: 0.8467064499855042\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.9625), 'test_loss': tensor(0.0556, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.42it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/4\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"label\": \"ud\",\n",
            "    \"augment\": false,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"cls_hidden_size\": 768,\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 4283/4283 [00:09<00:00, 448.71it/s]\n",
            "100% 885/885 [00:01<00:00, 496.54it/s]\n",
            "100% 77/77 [00:00<00:00, 504.10it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 592 K \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.14it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 0.0\n",
            "eval_fp: 8.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 0.0\n",
            "eval_precision: 0.0\n",
            "eval_recall: 0\n",
            "eval_f1: 0\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  83% 536/647 [02:25<00:30,  3.69it/s, loss=0.116, v_num=-1-4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  83% 537/647 [02:25<00:29,  3.69it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  83% 539/647 [02:25<00:29,  3.70it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  84% 541/647 [02:25<00:28,  3.71it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  84% 543/647 [02:26<00:27,  3.72it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  84% 545/647 [02:26<00:27,  3.73it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  85% 547/647 [02:26<00:26,  3.74it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  85% 549/647 [02:26<00:26,  3.74it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  85% 551/647 [02:26<00:25,  3.75it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  85% 553/647 [02:26<00:24,  3.76it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  86% 555/647 [02:27<00:24,  3.77it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  86% 557/647 [02:27<00:23,  3.78it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  86% 559/647 [02:27<00:23,  3.79it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  87% 561/647 [02:27<00:22,  3.80it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  87% 563/647 [02:27<00:22,  3.81it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  87% 565/647 [02:27<00:21,  3.82it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  88% 567/647 [02:28<00:20,  3.83it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  88% 569/647 [02:28<00:20,  3.84it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  88% 571/647 [02:28<00:19,  3.85it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  89% 573/647 [02:28<00:19,  3.85it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  89% 575/647 [02:28<00:18,  3.86it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  89% 577/647 [02:28<00:18,  3.87it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  89% 579/647 [02:29<00:17,  3.88it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  90% 581/647 [02:29<00:16,  3.89it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  90% 583/647 [02:29<00:16,  3.90it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  90% 585/647 [02:29<00:15,  3.91it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  91% 587/647 [02:29<00:15,  3.92it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  91% 589/647 [02:30<00:14,  3.93it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  91% 591/647 [02:30<00:14,  3.93it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  92% 593/647 [02:30<00:13,  3.94it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  92% 595/647 [02:30<00:13,  3.95it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  92% 597/647 [02:30<00:12,  3.96it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  93% 599/647 [02:30<00:12,  3.97it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  93% 601/647 [02:31<00:11,  3.98it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  93% 603/647 [02:31<00:11,  3.99it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  94% 605/647 [02:31<00:10,  4.00it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  94% 607/647 [02:31<00:09,  4.00it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  94% 609/647 [02:31<00:09,  4.01it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  94% 611/647 [02:31<00:08,  4.02it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  95% 613/647 [02:32<00:08,  4.03it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  95% 615/647 [02:32<00:07,  4.04it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  95% 617/647 [02:32<00:07,  4.05it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  96% 619/647 [02:32<00:06,  4.06it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  96% 621/647 [02:32<00:06,  4.06it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  96% 623/647 [02:32<00:05,  4.07it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  97% 625/647 [02:33<00:05,  4.08it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  97% 627/647 [02:33<00:04,  4.09it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  97% 629/647 [02:33<00:04,  4.10it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  98% 631/647 [02:33<00:03,  4.11it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  98% 633/647 [02:33<00:03,  4.12it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  98% 635/647 [02:33<00:02,  4.12it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  98% 637/647 [02:34<00:02,  4.13it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  99% 639/647 [02:34<00:01,  4.14it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  99% 641/647 [02:34<00:01,  4.15it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0:  99% 643/647 [02:34<00:00,  4.16it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0: 100% 645/647 [02:34<00:00,  4.17it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0: 100% 647/647 [02:34<00:00,  4.18it/s, loss=0.116, v_num=-1-4]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 10.0\n",
            "eval_fn: 13.0\n",
            "eval_tn: 851.0\n",
            "eval_precision: 0.523809552192688\n",
            "eval_recall: 0.4583333432674408\n",
            "eval_f1: 0.4888888895511627\n",
            "eval_mcc: 0.4767327904701233\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0: 100% 647/647 [02:45<00:00,  3.91it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 0: 100% 647/647 [02:45<00:00,  3.91it/s, loss=0.116, v_num=-1-4]\n",
            "Epoch 1:  83% 536/647 [02:26<00:30,  3.66it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  83% 537/647 [02:26<00:30,  3.66it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  83% 539/647 [02:27<00:29,  3.67it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  84% 541/647 [02:27<00:28,  3.68it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  84% 543/647 [02:27<00:28,  3.68it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  84% 545/647 [02:27<00:27,  3.69it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  85% 547/647 [02:27<00:27,  3.70it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  85% 549/647 [02:27<00:26,  3.71it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  85% 551/647 [02:28<00:25,  3.72it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  85% 553/647 [02:28<00:25,  3.73it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  86% 555/647 [02:28<00:24,  3.74it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  86% 557/647 [02:28<00:24,  3.75it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  86% 559/647 [02:28<00:23,  3.76it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  87% 561/647 [02:28<00:22,  3.77it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  87% 563/647 [02:29<00:22,  3.78it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  87% 565/647 [02:29<00:21,  3.79it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  88% 567/647 [02:29<00:21,  3.79it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  88% 569/647 [02:29<00:20,  3.80it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  88% 571/647 [02:29<00:19,  3.81it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  89% 573/647 [02:29<00:19,  3.82it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  89% 575/647 [02:30<00:18,  3.83it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  89% 577/647 [02:30<00:18,  3.84it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  89% 579/647 [02:30<00:17,  3.85it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  90% 581/647 [02:30<00:17,  3.86it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  90% 583/647 [02:30<00:16,  3.87it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  90% 585/647 [02:30<00:15,  3.88it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  91% 587/647 [02:31<00:15,  3.88it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  91% 589/647 [02:31<00:14,  3.89it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  91% 591/647 [02:31<00:14,  3.90it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  92% 593/647 [02:31<00:13,  3.91it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  92% 595/647 [02:31<00:13,  3.92it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  92% 597/647 [02:31<00:12,  3.93it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  93% 599/647 [02:32<00:12,  3.94it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  93% 601/647 [02:32<00:11,  3.95it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  93% 603/647 [02:32<00:11,  3.95it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  94% 605/647 [02:32<00:10,  3.96it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  94% 607/647 [02:32<00:10,  3.97it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  94% 609/647 [02:32<00:09,  3.98it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  94% 611/647 [02:33<00:09,  3.99it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  95% 613/647 [02:33<00:08,  4.00it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  95% 615/647 [02:33<00:07,  4.01it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  95% 617/647 [02:33<00:07,  4.02it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  96% 619/647 [02:33<00:06,  4.02it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  96% 621/647 [02:34<00:06,  4.03it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  96% 623/647 [02:34<00:05,  4.04it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  97% 625/647 [02:34<00:05,  4.05it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  97% 627/647 [02:34<00:04,  4.06it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  97% 629/647 [02:34<00:04,  4.07it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  98% 631/647 [02:34<00:03,  4.08it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  98% 633/647 [02:35<00:03,  4.08it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  98% 635/647 [02:35<00:02,  4.09it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  98% 637/647 [02:35<00:02,  4.10it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  99% 639/647 [02:35<00:01,  4.11it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  99% 641/647 [02:35<00:01,  4.12it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1:  99% 643/647 [02:35<00:00,  4.13it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1: 100% 645/647 [02:36<00:00,  4.13it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 1: 100% 647/647 [02:36<00:00,  4.14it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 16.0\n",
            "eval_fp: 13.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 848.0\n",
            "eval_precision: 0.5517241358757019\n",
            "eval_recall: 0.6666666865348816\n",
            "eval_f1: 0.6037735939025879\n",
            "eval_mcc: 0.5944714546203613\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1: 100% 647/647 [02:46<00:00,  3.88it/s, loss=0.078, v_num=-1-4, train_loss=0.508]\n",
            "Epoch 2:  83% 536/647 [02:26<00:30,  3.66it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  83% 538/647 [02:26<00:29,  3.66it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  83% 540/647 [02:27<00:29,  3.67it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  84% 542/647 [02:27<00:28,  3.68it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  84% 544/647 [02:27<00:27,  3.69it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  84% 546/647 [02:27<00:27,  3.70it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  85% 548/647 [02:27<00:26,  3.71it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  85% 550/647 [02:27<00:26,  3.72it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  85% 552/647 [02:28<00:25,  3.73it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  86% 554/647 [02:28<00:24,  3.74it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  86% 556/647 [02:28<00:24,  3.75it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  86% 558/647 [02:28<00:23,  3.76it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  87% 560/647 [02:28<00:23,  3.76it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  87% 562/647 [02:28<00:22,  3.77it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  87% 564/647 [02:29<00:21,  3.78it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  87% 566/647 [02:29<00:21,  3.79it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  88% 568/647 [02:29<00:20,  3.80it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  88% 570/647 [02:29<00:20,  3.81it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  88% 572/647 [02:29<00:19,  3.82it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  89% 574/647 [02:29<00:19,  3.83it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  89% 576/647 [02:30<00:18,  3.84it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  89% 578/647 [02:30<00:17,  3.85it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  90% 580/647 [02:30<00:17,  3.85it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  90% 582/647 [02:30<00:16,  3.86it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  90% 584/647 [02:30<00:16,  3.87it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  91% 586/647 [02:30<00:15,  3.88it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  91% 588/647 [02:31<00:15,  3.89it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  91% 590/647 [02:31<00:14,  3.90it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  91% 592/647 [02:31<00:14,  3.91it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  92% 594/647 [02:31<00:13,  3.92it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  92% 596/647 [02:31<00:12,  3.93it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  92% 598/647 [02:32<00:12,  3.93it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  93% 600/647 [02:32<00:11,  3.94it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  93% 602/647 [02:32<00:11,  3.95it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  93% 604/647 [02:32<00:10,  3.96it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  94% 606/647 [02:32<00:10,  3.97it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  94% 608/647 [02:32<00:09,  3.98it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  94% 610/647 [02:33<00:09,  3.99it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  95% 612/647 [02:33<00:08,  3.99it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  95% 614/647 [02:33<00:08,  4.00it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  95% 616/647 [02:33<00:07,  4.01it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  96% 618/647 [02:33<00:07,  4.02it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  96% 620/647 [02:33<00:06,  4.03it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  96% 622/647 [02:34<00:06,  4.04it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  96% 624/647 [02:34<00:05,  4.05it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  97% 626/647 [02:34<00:05,  4.06it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  97% 628/647 [02:34<00:04,  4.06it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  97% 630/647 [02:34<00:04,  4.07it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  98% 632/647 [02:34<00:03,  4.08it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  98% 634/647 [02:35<00:03,  4.09it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  98% 636/647 [02:35<00:02,  4.10it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  99% 638/647 [02:35<00:02,  4.11it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  99% 640/647 [02:35<00:01,  4.11it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2:  99% 642/647 [02:35<00:01,  4.12it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2: 100% 644/647 [02:35<00:00,  4.13it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2: 100% 646/647 [02:36<00:00,  4.14it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.46it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 15.0\n",
            "eval_fp: 10.0\n",
            "eval_fn: 9.0\n",
            "eval_tn: 851.0\n",
            "eval_precision: 0.6000000238418579\n",
            "eval_recall: 0.625\n",
            "eval_f1: 0.6122449040412903\n",
            "eval_mcc: 0.6013420224189758\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2: 100% 647/647 [02:46<00:00,  3.88it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 2: 100% 647/647 [02:46<00:00,  3.88it/s, loss=0.028, v_num=-1-4, train_loss=0.102]\n",
            "Epoch 3:  83% 536/647 [02:26<00:30,  3.67it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  83% 537/647 [02:26<00:30,  3.66it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  83% 539/647 [02:26<00:29,  3.67it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  84% 541/647 [02:26<00:28,  3.68it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  84% 543/647 [02:27<00:28,  3.69it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  84% 545/647 [02:27<00:27,  3.70it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  85% 547/647 [02:27<00:26,  3.71it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  85% 549/647 [02:27<00:26,  3.72it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  85% 551/647 [02:27<00:25,  3.73it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  85% 553/647 [02:27<00:25,  3.74it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  86% 555/647 [02:28<00:24,  3.75it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  86% 557/647 [02:28<00:23,  3.76it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  86% 559/647 [02:28<00:23,  3.77it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  87% 561/647 [02:28<00:22,  3.77it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  87% 563/647 [02:28<00:22,  3.78it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  87% 565/647 [02:28<00:21,  3.79it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  88% 567/647 [02:29<00:21,  3.80it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  88% 569/647 [02:29<00:20,  3.81it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  88% 571/647 [02:29<00:19,  3.82it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  89% 573/647 [02:29<00:19,  3.83it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  89% 575/647 [02:29<00:18,  3.84it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  89% 577/647 [02:29<00:18,  3.85it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  89% 579/647 [02:30<00:17,  3.86it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  90% 581/647 [02:30<00:17,  3.86it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  90% 583/647 [02:30<00:16,  3.87it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  90% 585/647 [02:30<00:15,  3.88it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  91% 587/647 [02:30<00:15,  3.89it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  91% 589/647 [02:31<00:14,  3.90it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  91% 591/647 [02:31<00:14,  3.91it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  92% 593/647 [02:31<00:13,  3.92it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  92% 595/647 [02:31<00:13,  3.93it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  92% 597/647 [02:31<00:12,  3.94it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  93% 599/647 [02:31<00:12,  3.94it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  93% 601/647 [02:32<00:11,  3.95it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  93% 603/647 [02:32<00:11,  3.96it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  94% 605/647 [02:32<00:10,  3.97it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  94% 607/647 [02:32<00:10,  3.98it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  94% 609/647 [02:32<00:09,  3.99it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  94% 611/647 [02:32<00:09,  4.00it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  95% 613/647 [02:33<00:08,  4.00it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  95% 615/647 [02:33<00:07,  4.01it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  95% 617/647 [02:33<00:07,  4.02it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  96% 619/647 [02:33<00:06,  4.03it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  96% 621/647 [02:33<00:06,  4.04it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  96% 623/647 [02:33<00:05,  4.05it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  97% 625/647 [02:34<00:05,  4.06it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  97% 627/647 [02:34<00:04,  4.06it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  97% 629/647 [02:34<00:04,  4.07it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  98% 631/647 [02:34<00:03,  4.08it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  98% 633/647 [02:34<00:03,  4.09it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  98% 635/647 [02:34<00:02,  4.10it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  98% 637/647 [02:35<00:02,  4.11it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  99% 639/647 [02:35<00:01,  4.11it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  99% 641/647 [02:35<00:01,  4.12it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3:  99% 643/647 [02:35<00:00,  4.13it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3: 100% 645/647 [02:35<00:00,  4.14it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 3: 100% 647/647 [02:35<00:00,  4.15it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 17.0\n",
            "eval_fp: 12.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 849.0\n",
            "eval_precision: 0.5862069129943848\n",
            "eval_recall: 0.7083333134651184\n",
            "eval_f1: 0.6415094137191772\n",
            "eval_mcc: 0.6335465908050537\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3: 100% 647/647 [02:36<00:00,  4.14it/s, loss=0.064, v_num=-1-4, train_loss=0.075]\n",
            "Epoch 4:  83% 536/647 [02:25<00:30,  3.68it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  83% 538/647 [02:26<00:29,  3.68it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  83% 540/647 [02:26<00:28,  3.69it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  84% 542/647 [02:26<00:28,  3.70it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  84% 544/647 [02:26<00:27,  3.71it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  84% 546/647 [02:26<00:27,  3.72it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  85% 548/647 [02:26<00:26,  3.73it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  85% 550/647 [02:27<00:25,  3.74it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  85% 552/647 [02:27<00:25,  3.75it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  86% 554/647 [02:27<00:24,  3.76it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  86% 556/647 [02:27<00:24,  3.77it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  86% 558/647 [02:27<00:23,  3.78it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  87% 560/647 [02:27<00:22,  3.79it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  87% 562/647 [02:28<00:22,  3.79it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  87% 564/647 [02:28<00:21,  3.80it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  87% 566/647 [02:28<00:21,  3.81it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  88% 568/647 [02:28<00:20,  3.82it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  88% 570/647 [02:28<00:20,  3.83it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  88% 572/647 [02:28<00:19,  3.84it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  89% 574/647 [02:29<00:18,  3.85it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  89% 576/647 [02:29<00:18,  3.86it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  89% 578/647 [02:29<00:17,  3.87it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  90% 580/647 [02:29<00:17,  3.88it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  90% 582/647 [02:29<00:16,  3.89it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  90% 584/647 [02:29<00:16,  3.89it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  91% 586/647 [02:30<00:15,  3.90it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  91% 588/647 [02:30<00:15,  3.91it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  91% 590/647 [02:30<00:14,  3.92it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  91% 592/647 [02:30<00:13,  3.93it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  92% 594/647 [02:30<00:13,  3.94it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  92% 596/647 [02:30<00:12,  3.95it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  92% 598/647 [02:31<00:12,  3.96it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  93% 600/647 [02:31<00:11,  3.97it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  93% 602/647 [02:31<00:11,  3.97it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  93% 604/647 [02:31<00:10,  3.98it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  94% 606/647 [02:31<00:10,  3.99it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  94% 608/647 [02:31<00:09,  4.00it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  94% 610/647 [02:32<00:09,  4.01it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  95% 612/647 [02:32<00:08,  4.02it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  95% 614/647 [02:32<00:08,  4.03it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  95% 616/647 [02:32<00:07,  4.03it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  96% 618/647 [02:32<00:07,  4.04it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  96% 620/647 [02:33<00:06,  4.05it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  96% 622/647 [02:33<00:06,  4.06it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  96% 624/647 [02:33<00:05,  4.07it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  97% 626/647 [02:33<00:05,  4.08it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  97% 628/647 [02:33<00:04,  4.09it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  97% 630/647 [02:33<00:04,  4.09it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  98% 632/647 [02:34<00:03,  4.10it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  98% 634/647 [02:34<00:03,  4.11it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  98% 636/647 [02:34<00:02,  4.12it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  99% 638/647 [02:34<00:02,  4.13it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  99% 640/647 [02:34<00:01,  4.14it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4:  99% 642/647 [02:34<00:01,  4.14it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4: 100% 644/647 [02:35<00:00,  4.15it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 4: 100% 646/647 [02:35<00:00,  4.16it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.53it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 17.0\n",
            "eval_fp: 13.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 848.0\n",
            "eval_precision: 0.5666666626930237\n",
            "eval_recall: 0.7083333134651184\n",
            "eval_f1: 0.6296296119689941\n",
            "eval_mcc: 0.6222196817398071\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4: 100% 647/647 [02:35<00:00,  4.16it/s, loss=0.057, v_num=-1-4, train_loss=0.0658]\n",
            "Epoch 5:  83% 536/647 [02:25<00:30,  3.69it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  83% 538/647 [02:25<00:29,  3.69it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  83% 540/647 [02:26<00:28,  3.70it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  84% 542/647 [02:26<00:28,  3.71it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  84% 544/647 [02:26<00:27,  3.71it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  84% 546/647 [02:26<00:27,  3.72it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  85% 548/647 [02:26<00:26,  3.73it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  85% 550/647 [02:26<00:25,  3.74it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  85% 552/647 [02:27<00:25,  3.75it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  86% 554/647 [02:27<00:24,  3.76it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  86% 556/647 [02:27<00:24,  3.77it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  86% 558/647 [02:27<00:23,  3.78it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  87% 560/647 [02:27<00:22,  3.79it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  87% 562/647 [02:27<00:22,  3.80it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  87% 564/647 [02:28<00:21,  3.81it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  87% 566/647 [02:28<00:21,  3.82it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  88% 568/647 [02:28<00:20,  3.83it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  88% 570/647 [02:28<00:20,  3.83it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  88% 572/647 [02:28<00:19,  3.84it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  89% 574/647 [02:28<00:18,  3.85it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  89% 576/647 [02:29<00:18,  3.86it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  89% 578/647 [02:29<00:17,  3.87it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  90% 580/647 [02:29<00:17,  3.88it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  90% 582/647 [02:29<00:16,  3.89it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  90% 584/647 [02:29<00:16,  3.90it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  91% 586/647 [02:29<00:15,  3.91it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  91% 588/647 [02:30<00:15,  3.92it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  91% 590/647 [02:30<00:14,  3.92it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  91% 592/647 [02:30<00:13,  3.93it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  92% 594/647 [02:30<00:13,  3.94it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  92% 596/647 [02:30<00:12,  3.95it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  92% 598/647 [02:31<00:12,  3.96it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  93% 600/647 [02:31<00:11,  3.97it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  93% 602/647 [02:31<00:11,  3.98it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  93% 604/647 [02:31<00:10,  3.99it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  94% 606/647 [02:31<00:10,  3.99it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  94% 608/647 [02:31<00:09,  4.00it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  94% 610/647 [02:32<00:09,  4.01it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  95% 612/647 [02:32<00:08,  4.02it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  95% 614/647 [02:32<00:08,  4.03it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  95% 616/647 [02:32<00:07,  4.04it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  96% 618/647 [02:32<00:07,  4.05it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  96% 620/647 [02:32<00:06,  4.06it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  96% 622/647 [02:33<00:06,  4.06it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  96% 624/647 [02:33<00:05,  4.07it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  97% 626/647 [02:33<00:05,  4.08it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  97% 628/647 [02:33<00:04,  4.09it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  97% 630/647 [02:33<00:04,  4.10it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  98% 632/647 [02:33<00:03,  4.11it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  98% 634/647 [02:34<00:03,  4.11it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  98% 636/647 [02:34<00:02,  4.12it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  99% 638/647 [02:34<00:02,  4.13it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  99% 640/647 [02:34<00:01,  4.14it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5:  99% 642/647 [02:34<00:01,  4.15it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5: 100% 644/647 [02:34<00:00,  4.16it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 5: 100% 646/647 [02:35<00:00,  4.17it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.45it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 14.0\n",
            "eval_fp: 6.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 855.0\n",
            "eval_precision: 0.699999988079071\n",
            "eval_recall: 0.5833333134651184\n",
            "eval_f1: 0.6363636255264282\n",
            "eval_mcc: 0.6299143433570862\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5: 100% 647/647 [02:35<00:00,  4.16it/s, loss=0.028, v_num=-1-4, train_loss=0.0526]\n",
            "Epoch 6:  83% 536/647 [02:25<00:30,  3.68it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  83% 538/647 [02:26<00:29,  3.68it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  83% 540/647 [02:26<00:28,  3.69it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  84% 542/647 [02:26<00:28,  3.70it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  84% 544/647 [02:26<00:27,  3.71it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  84% 546/647 [02:26<00:27,  3.72it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  85% 548/647 [02:26<00:26,  3.73it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  85% 550/647 [02:27<00:25,  3.74it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  85% 552/647 [02:27<00:25,  3.75it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  86% 554/647 [02:27<00:24,  3.76it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  86% 556/647 [02:27<00:24,  3.77it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  86% 558/647 [02:27<00:23,  3.78it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  87% 560/647 [02:27<00:22,  3.79it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  87% 562/647 [02:28<00:22,  3.79it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  87% 564/647 [02:28<00:21,  3.80it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  87% 566/647 [02:28<00:21,  3.81it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  88% 568/647 [02:28<00:20,  3.82it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  88% 570/647 [02:28<00:20,  3.83it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  88% 572/647 [02:28<00:19,  3.84it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  89% 574/647 [02:29<00:18,  3.85it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  89% 576/647 [02:29<00:18,  3.86it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  89% 578/647 [02:29<00:17,  3.87it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  90% 580/647 [02:29<00:17,  3.88it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  90% 582/647 [02:29<00:16,  3.89it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  90% 584/647 [02:29<00:16,  3.89it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  91% 586/647 [02:30<00:15,  3.90it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  91% 588/647 [02:30<00:15,  3.91it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  91% 590/647 [02:30<00:14,  3.92it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  91% 592/647 [02:30<00:13,  3.93it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  92% 594/647 [02:30<00:13,  3.94it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  92% 596/647 [02:30<00:12,  3.95it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  92% 598/647 [02:31<00:12,  3.96it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  93% 600/647 [02:31<00:11,  3.97it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  93% 602/647 [02:31<00:11,  3.97it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  93% 604/647 [02:31<00:10,  3.98it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  94% 606/647 [02:31<00:10,  3.99it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  94% 608/647 [02:32<00:09,  4.00it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  94% 610/647 [02:32<00:09,  4.01it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  95% 612/647 [02:32<00:08,  4.02it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  95% 614/647 [02:32<00:08,  4.03it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  95% 616/647 [02:32<00:07,  4.03it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  96% 618/647 [02:32<00:07,  4.04it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  96% 620/647 [02:33<00:06,  4.05it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  96% 622/647 [02:33<00:06,  4.06it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  96% 624/647 [02:33<00:05,  4.07it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  97% 626/647 [02:33<00:05,  4.08it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  97% 628/647 [02:33<00:04,  4.09it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  97% 630/647 [02:33<00:04,  4.09it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  98% 632/647 [02:34<00:03,  4.10it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  98% 634/647 [02:34<00:03,  4.11it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  98% 636/647 [02:34<00:02,  4.12it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  99% 638/647 [02:34<00:02,  4.13it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  99% 640/647 [02:34<00:01,  4.14it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6:  99% 642/647 [02:34<00:01,  4.14it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6: 100% 644/647 [02:35<00:00,  4.15it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6: 100% 646/647 [02:35<00:00,  4.16it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.49it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 17.0\n",
            "eval_fp: 8.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 853.0\n",
            "eval_precision: 0.6800000071525574\n",
            "eval_recall: 0.7083333134651184\n",
            "eval_f1: 0.6938775777816772\n",
            "eval_mcc: 0.6853163838386536\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6: 100% 647/647 [02:45<00:00,  3.91it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 6: 100% 647/647 [02:45<00:00,  3.90it/s, loss=0.033, v_num=-1-4, train_loss=0.0514]\n",
            "Epoch 7:  83% 536/647 [02:25<00:30,  3.68it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  83% 537/647 [02:26<00:29,  3.67it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  83% 539/647 [02:26<00:29,  3.68it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  84% 541/647 [02:26<00:28,  3.69it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  84% 543/647 [02:26<00:28,  3.70it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  84% 545/647 [02:26<00:27,  3.71it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  85% 547/647 [02:27<00:26,  3.72it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  85% 549/647 [02:27<00:26,  3.73it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  85% 551/647 [02:27<00:25,  3.74it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  85% 553/647 [02:27<00:25,  3.75it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  86% 555/647 [02:27<00:24,  3.76it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  86% 557/647 [02:27<00:23,  3.77it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  86% 559/647 [02:28<00:23,  3.78it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  87% 561/647 [02:28<00:22,  3.78it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  87% 563/647 [02:28<00:22,  3.79it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  87% 565/647 [02:28<00:21,  3.80it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  88% 567/647 [02:28<00:20,  3.81it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  88% 569/647 [02:28<00:20,  3.82it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  88% 571/647 [02:29<00:19,  3.83it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  89% 573/647 [02:29<00:19,  3.84it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  89% 575/647 [02:29<00:18,  3.85it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  89% 577/647 [02:29<00:18,  3.86it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  89% 579/647 [02:29<00:17,  3.87it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  90% 581/647 [02:29<00:17,  3.88it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  90% 583/647 [02:30<00:16,  3.88it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  90% 585/647 [02:30<00:15,  3.89it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  91% 587/647 [02:30<00:15,  3.90it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  91% 589/647 [02:30<00:14,  3.91it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  91% 591/647 [02:30<00:14,  3.92it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  92% 593/647 [02:30<00:13,  3.93it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  92% 595/647 [02:31<00:13,  3.94it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  92% 597/647 [02:31<00:12,  3.95it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  93% 599/647 [02:31<00:12,  3.95it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  93% 601/647 [02:31<00:11,  3.96it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  93% 603/647 [02:31<00:11,  3.97it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  94% 605/647 [02:31<00:10,  3.98it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  94% 607/647 [02:32<00:10,  3.99it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  94% 609/647 [02:32<00:09,  4.00it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  94% 611/647 [02:32<00:08,  4.01it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  95% 613/647 [02:32<00:08,  4.02it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  95% 615/647 [02:32<00:07,  4.02it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  95% 617/647 [02:32<00:07,  4.03it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  96% 619/647 [02:33<00:06,  4.04it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  96% 621/647 [02:33<00:06,  4.05it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  96% 623/647 [02:33<00:05,  4.06it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  97% 625/647 [02:33<00:05,  4.07it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  97% 627/647 [02:33<00:04,  4.08it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  97% 629/647 [02:33<00:04,  4.08it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  98% 631/647 [02:34<00:03,  4.09it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  98% 633/647 [02:34<00:03,  4.10it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  98% 635/647 [02:34<00:02,  4.11it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  98% 637/647 [02:34<00:02,  4.12it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  99% 639/647 [02:34<00:01,  4.13it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  99% 641/647 [02:35<00:01,  4.14it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7:  99% 643/647 [02:35<00:00,  4.14it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7: 100% 645/647 [02:35<00:00,  4.15it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Epoch 7: 100% 647/647 [02:35<00:00,  4.16it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 16.0\n",
            "eval_fp: 9.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 852.0\n",
            "eval_precision: 0.6399999856948853\n",
            "eval_recall: 0.6666666865348816\n",
            "eval_f1: 0.6530612111091614\n",
            "eval_mcc: 0.6433292031288147\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 647/647 [02:45<00:00,  3.90it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 647/647 [02:46<00:00,  3.89it/s, loss=0.018, v_num=-1-4, train_loss=0.0437]\n",
            "Computing Input\n",
            "100% 4283/4283 [00:07<00:00, 557.42it/s]\n",
            "100% 885/885 [00:01<00:00, 502.52it/s]\n",
            "100% 77/77 [00:00<00:00, 567.22it/s]\n",
            "Testing:  90% 9/10 [00:01<00:00,  5.28it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 9.0\n",
            "fp: 4.0\n",
            "fn: 0.0\n",
            "tn: 64.0\n",
            "precision: 0.692307710647583\n",
            "recall: 1.0\n",
            "f1: 0.8181818127632141\n",
            "mcc: 0.8072073459625244\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.9500), 'test_loss': tensor(0.0979, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.44it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsFLFmoE0ynJ",
        "outputId": "3ae0b9df-d1d2-4731-c330-8b593960fe85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\" \\\n",
        "--ud \\\n",
        "--summary_type \"cluster\" \\\n",
        "--amount_labels 1 \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 8 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--loss_weight 3.0 \\\n",
        "--amp_level \"O1\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-08 05:17:25.507123: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"summary_type\": \"cluster\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"ud\": true,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output\",\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 8,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 4319/4319 [00:08<00:00, 525.02it/s]\n",
            "100% 885/885 [00:01<00:00, 575.20it/s]\n",
            "100% 77/77 [00:00<00:00, 510.52it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 592 K \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.70it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 0.0\n",
            "eval_fp: 8.0\n",
            "eval_fn: 0.0\n",
            "eval_tn: 0.0\n",
            "eval_precision: 0.0\n",
            "eval_recall: 0\n",
            "eval_f1: 0\n",
            "eval_mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0:  83% 540/651 [02:22<00:29,  3.80it/s, loss=0.071, v_num=er-1]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  83% 541/651 [02:22<00:28,  3.80it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  83% 543/651 [02:22<00:28,  3.81it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  84% 545/651 [02:22<00:27,  3.82it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  84% 547/651 [02:22<00:27,  3.83it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  84% 549/651 [02:23<00:26,  3.84it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  85% 551/651 [02:23<00:26,  3.85it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  85% 553/651 [02:23<00:25,  3.85it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  85% 555/651 [02:23<00:24,  3.86it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  86% 557/651 [02:23<00:24,  3.87it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  86% 559/651 [02:23<00:23,  3.88it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  86% 561/651 [02:24<00:23,  3.89it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  86% 563/651 [02:24<00:22,  3.90it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  87% 565/651 [02:24<00:21,  3.91it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  87% 567/651 [02:24<00:21,  3.92it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  87% 569/651 [02:24<00:20,  3.93it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  88% 571/651 [02:24<00:20,  3.94it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  88% 573/651 [02:25<00:19,  3.95it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  88% 575/651 [02:25<00:19,  3.96it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  89% 577/651 [02:25<00:18,  3.97it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  89% 579/651 [02:25<00:18,  3.98it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  89% 581/651 [02:25<00:17,  3.98it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  90% 583/651 [02:25<00:17,  3.99it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  90% 585/651 [02:26<00:16,  4.00it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  90% 587/651 [02:26<00:15,  4.01it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  90% 589/651 [02:26<00:15,  4.02it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  91% 591/651 [02:26<00:14,  4.03it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  91% 593/651 [02:26<00:14,  4.04it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  91% 595/651 [02:26<00:13,  4.05it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  92% 597/651 [02:27<00:13,  4.06it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  92% 599/651 [02:27<00:12,  4.07it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  92% 601/651 [02:27<00:12,  4.08it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  93% 603/651 [02:27<00:11,  4.08it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  93% 605/651 [02:27<00:11,  4.09it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  93% 607/651 [02:27<00:10,  4.10it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  94% 609/651 [02:28<00:10,  4.11it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  94% 611/651 [02:28<00:09,  4.12it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  94% 613/651 [02:28<00:09,  4.13it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  94% 615/651 [02:28<00:08,  4.14it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  95% 617/651 [02:28<00:08,  4.15it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  95% 619/651 [02:28<00:07,  4.15it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  95% 621/651 [02:29<00:07,  4.16it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  96% 623/651 [02:29<00:06,  4.17it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  96% 625/651 [02:29<00:06,  4.18it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  96% 627/651 [02:29<00:05,  4.19it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  97% 629/651 [02:29<00:05,  4.20it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  97% 631/651 [02:29<00:04,  4.21it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  97% 633/651 [02:30<00:04,  4.22it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  98% 635/651 [02:30<00:03,  4.22it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  98% 637/651 [02:30<00:03,  4.23it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  98% 639/651 [02:30<00:02,  4.24it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  98% 641/651 [02:30<00:02,  4.25it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  99% 643/651 [02:30<00:01,  4.26it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  99% 645/651 [02:31<00:01,  4.27it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0:  99% 647/651 [02:31<00:00,  4.28it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0: 100% 649/651 [02:31<00:00,  4.28it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0: 100% 651/651 [02:31<00:00,  4.29it/s, loss=0.071, v_num=er-1]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 10.0\n",
            "eval_fp: 1.0\n",
            "eval_fn: 11.0\n",
            "eval_tn: 863.0\n",
            "eval_precision: 0.9090909361839294\n",
            "eval_recall: 0.4761904776096344\n",
            "eval_f1: 0.625\n",
            "eval_mcc: 0.652586817741394\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1316716544 bytes == 0x1873e4000 @  0x7fe7711c1615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fe76d61e950 0x7fe76d622bf7 0x7fe76d9537e8 0x7fe76d9091b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1645895680 bytes == 0x1163aa000 @  0x7fe7711c1615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fe76d61e950 0x7fe76d622bf7 0x7fe76d9537e8 0x7fe76d9091b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2057371648 bytes == 0x7fe3abe7e000 @  0x7fe7711c1615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fe76d61e950 0x7fe76d622bf7 0x7fe76d9537e8 0x7fe76d9091b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 0: 100% 651/651 [02:43<00:00,  3.98it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 0: 100% 651/651 [02:43<00:00,  3.98it/s, loss=0.071, v_num=er-1]\n",
            "Epoch 1:  83% 540/651 [02:23<00:29,  3.75it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  83% 541/651 [02:24<00:29,  3.75it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  83% 543/651 [02:24<00:28,  3.76it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  84% 545/651 [02:24<00:28,  3.77it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  84% 547/651 [02:24<00:27,  3.78it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  84% 549/651 [02:25<00:26,  3.79it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  85% 551/651 [02:25<00:26,  3.80it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  85% 553/651 [02:25<00:25,  3.80it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  85% 555/651 [02:25<00:25,  3.81it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  86% 557/651 [02:25<00:24,  3.82it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  86% 559/651 [02:25<00:24,  3.83it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  86% 561/651 [02:26<00:23,  3.84it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  86% 563/651 [02:26<00:22,  3.85it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  87% 565/651 [02:26<00:22,  3.86it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  87% 567/651 [02:26<00:21,  3.87it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  87% 569/651 [02:26<00:21,  3.88it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  88% 571/651 [02:26<00:20,  3.89it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  88% 573/651 [02:27<00:20,  3.90it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  88% 575/651 [02:27<00:19,  3.91it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  89% 577/651 [02:27<00:18,  3.91it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  89% 579/651 [02:27<00:18,  3.92it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  89% 581/651 [02:27<00:17,  3.93it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  90% 583/651 [02:27<00:17,  3.94it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  90% 585/651 [02:28<00:16,  3.95it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  90% 587/651 [02:28<00:16,  3.96it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  90% 589/651 [02:28<00:15,  3.97it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  91% 591/651 [02:28<00:15,  3.98it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  91% 593/651 [02:28<00:14,  3.99it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  91% 595/651 [02:28<00:14,  4.00it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  92% 597/651 [02:29<00:13,  4.01it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  92% 599/651 [02:29<00:12,  4.01it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  92% 601/651 [02:29<00:12,  4.02it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  93% 603/651 [02:29<00:11,  4.03it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  93% 605/651 [02:29<00:11,  4.04it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  93% 607/651 [02:29<00:10,  4.05it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  94% 609/651 [02:30<00:10,  4.06it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  94% 611/651 [02:30<00:09,  4.07it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  94% 613/651 [02:30<00:09,  4.08it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  94% 615/651 [02:30<00:08,  4.08it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  95% 617/651 [02:30<00:08,  4.09it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  95% 619/651 [02:30<00:07,  4.10it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  95% 621/651 [02:31<00:07,  4.11it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  96% 623/651 [02:31<00:06,  4.12it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  96% 625/651 [02:31<00:06,  4.13it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  96% 627/651 [02:31<00:05,  4.14it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  97% 629/651 [02:31<00:05,  4.15it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  97% 631/651 [02:31<00:04,  4.15it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  97% 633/651 [02:32<00:04,  4.16it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  98% 635/651 [02:32<00:03,  4.17it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  98% 637/651 [02:32<00:03,  4.18it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  98% 639/651 [02:32<00:02,  4.19it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  98% 641/651 [02:32<00:02,  4.20it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  99% 643/651 [02:32<00:01,  4.21it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  99% 645/651 [02:33<00:01,  4.21it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1:  99% 647/651 [02:33<00:00,  4.22it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1: 100% 649/651 [02:33<00:00,  4.23it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 1: 100% 651/651 [02:33<00:00,  4.24it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 14.0\n",
            "eval_fp: 6.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 858.0\n",
            "eval_precision: 0.699999988079071\n",
            "eval_recall: 0.6666666865348816\n",
            "eval_f1: 0.6829268336296082\n",
            "eval_mcc: 0.6756232380867004\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 1645895680 bytes == 0x164b62000 @  0x7fe7711c1615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fe76d61e950 0x7fe76d622bf7 0x7fe76d9537e8 0x7fe76d9091b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 2057371648 bytes == 0x7fe3abe7e000 @  0x7fe7711c1615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fe76d61e950 0x7fe76d622bf7 0x7fe76d9537e8 0x7fe76d9091b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 1: 100% 651/651 [02:45<00:00,  3.93it/s, loss=0.123, v_num=er-1, train_loss=0.58]\n",
            "Epoch 2:  83% 540/651 [02:23<00:29,  3.75it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  83% 542/651 [02:24<00:29,  3.75it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  84% 544/651 [02:24<00:28,  3.76it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  84% 546/651 [02:24<00:27,  3.77it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  84% 548/651 [02:24<00:27,  3.78it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  84% 550/651 [02:25<00:26,  3.79it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  85% 552/651 [02:25<00:26,  3.80it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  85% 554/651 [02:25<00:25,  3.81it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  85% 556/651 [02:25<00:24,  3.82it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  86% 558/651 [02:25<00:24,  3.83it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  86% 560/651 [02:25<00:23,  3.84it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  86% 562/651 [02:26<00:23,  3.85it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  87% 564/651 [02:26<00:22,  3.86it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  87% 566/651 [02:26<00:21,  3.87it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  87% 568/651 [02:26<00:21,  3.87it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  88% 570/651 [02:26<00:20,  3.88it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  88% 572/651 [02:26<00:20,  3.89it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  88% 574/651 [02:27<00:19,  3.90it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  88% 576/651 [02:27<00:19,  3.91it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  89% 578/651 [02:27<00:18,  3.92it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  89% 580/651 [02:27<00:18,  3.93it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  89% 582/651 [02:27<00:17,  3.94it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  90% 584/651 [02:27<00:16,  3.95it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  90% 586/651 [02:28<00:16,  3.96it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  90% 588/651 [02:28<00:15,  3.97it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  91% 590/651 [02:28<00:15,  3.97it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  91% 592/651 [02:28<00:14,  3.98it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  91% 594/651 [02:28<00:14,  3.99it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  92% 596/651 [02:28<00:13,  4.00it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  92% 598/651 [02:29<00:13,  4.01it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  92% 600/651 [02:29<00:12,  4.02it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  92% 602/651 [02:29<00:12,  4.03it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  93% 604/651 [02:29<00:11,  4.04it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  93% 606/651 [02:29<00:11,  4.05it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  93% 608/651 [02:29<00:10,  4.05it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  94% 610/651 [02:30<00:10,  4.06it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  94% 612/651 [02:30<00:09,  4.07it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  94% 614/651 [02:30<00:09,  4.08it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  95% 616/651 [02:30<00:08,  4.09it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  95% 618/651 [02:30<00:08,  4.10it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  95% 620/651 [02:30<00:07,  4.11it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  96% 622/651 [02:31<00:07,  4.11it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  96% 624/651 [02:31<00:06,  4.12it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  96% 626/651 [02:31<00:06,  4.13it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  96% 628/651 [02:31<00:05,  4.14it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  97% 630/651 [02:32<00:05,  4.14it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  97% 632/651 [02:32<00:04,  4.15it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  97% 634/651 [02:32<00:04,  4.16it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  98% 636/651 [02:32<00:03,  4.17it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  98% 638/651 [02:32<00:03,  4.18it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  98% 640/651 [02:32<00:02,  4.19it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  99% 642/651 [02:33<00:02,  4.20it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  99% 644/651 [02:33<00:01,  4.20it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2:  99% 646/651 [02:33<00:01,  4.21it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2: 100% 648/651 [02:33<00:00,  4.22it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 2: 100% 650/651 [02:33<00:00,  4.23it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.52it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 11.0\n",
            "eval_fp: 1.0\n",
            "eval_fn: 10.0\n",
            "eval_tn: 863.0\n",
            "eval_precision: 0.9166666865348816\n",
            "eval_recall: 0.523809552192688\n",
            "eval_f1: 0.6666666865348816\n",
            "eval_mcc: 0.6878305673599243\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2: 100% 651/651 [02:34<00:00,  4.23it/s, loss=0.040, v_num=er-1, train_loss=0.129]\n",
            "Epoch 3:  83% 540/651 [02:23<00:29,  3.77it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  83% 542/651 [02:23<00:28,  3.77it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  84% 544/651 [02:23<00:28,  3.78it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  84% 546/651 [02:24<00:27,  3.79it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  84% 548/651 [02:24<00:27,  3.80it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  84% 550/651 [02:24<00:26,  3.81it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  85% 552/651 [02:24<00:25,  3.82it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  85% 554/651 [02:24<00:25,  3.83it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  85% 556/651 [02:24<00:24,  3.84it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  86% 558/651 [02:25<00:24,  3.85it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  86% 560/651 [02:25<00:23,  3.86it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  86% 562/651 [02:25<00:23,  3.87it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  87% 564/651 [02:25<00:22,  3.87it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  87% 566/651 [02:25<00:21,  3.88it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  87% 568/651 [02:25<00:21,  3.89it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  88% 570/651 [02:26<00:20,  3.90it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  88% 572/651 [02:26<00:20,  3.91it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  88% 574/651 [02:26<00:19,  3.92it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  88% 576/651 [02:26<00:19,  3.93it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  89% 578/651 [02:26<00:18,  3.94it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  89% 580/651 [02:26<00:17,  3.95it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  89% 582/651 [02:27<00:17,  3.96it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  90% 584/651 [02:27<00:16,  3.97it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  90% 586/651 [02:27<00:16,  3.98it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  90% 588/651 [02:27<00:15,  3.98it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  91% 590/651 [02:27<00:15,  3.99it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  91% 592/651 [02:27<00:14,  4.00it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  91% 594/651 [02:28<00:14,  4.01it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  92% 596/651 [02:28<00:13,  4.02it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  92% 598/651 [02:28<00:13,  4.03it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  92% 600/651 [02:28<00:12,  4.04it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  92% 602/651 [02:28<00:12,  4.05it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  93% 604/651 [02:28<00:11,  4.06it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  93% 606/651 [02:29<00:11,  4.07it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  93% 608/651 [02:29<00:10,  4.07it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  94% 610/651 [02:29<00:10,  4.08it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  94% 612/651 [02:29<00:09,  4.09it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  94% 614/651 [02:29<00:09,  4.10it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  95% 616/651 [02:29<00:08,  4.11it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  95% 618/651 [02:30<00:08,  4.12it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  95% 620/651 [02:30<00:07,  4.13it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  96% 622/651 [02:30<00:07,  4.14it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  96% 624/651 [02:30<00:06,  4.14it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  96% 626/651 [02:30<00:06,  4.15it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  96% 628/651 [02:30<00:05,  4.16it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  97% 630/651 [02:31<00:05,  4.17it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  97% 632/651 [02:31<00:04,  4.18it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  97% 634/651 [02:31<00:04,  4.19it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  98% 636/651 [02:31<00:03,  4.20it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  98% 638/651 [02:31<00:03,  4.20it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  98% 640/651 [02:31<00:02,  4.21it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  99% 642/651 [02:32<00:02,  4.22it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  99% 644/651 [02:32<00:01,  4.23it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3:  99% 646/651 [02:32<00:01,  4.24it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3: 100% 648/651 [02:32<00:00,  4.25it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 3: 100% 650/651 [02:32<00:00,  4.26it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.56it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 4.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 860.0\n",
            "eval_precision: 0.7647058963775635\n",
            "eval_recall: 0.6190476417541504\n",
            "eval_f1: 0.6842105388641357\n",
            "eval_mcc: 0.6813121438026428\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3: 100% 651/651 [02:33<00:00,  4.25it/s, loss=0.051, v_num=er-1, train_loss=0.0906]\n",
            "Epoch 4:  83% 540/651 [02:23<00:29,  3.77it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  83% 542/651 [02:23<00:28,  3.77it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  84% 544/651 [02:23<00:28,  3.78it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  84% 546/651 [02:23<00:27,  3.79it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  84% 548/651 [02:24<00:27,  3.80it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  84% 550/651 [02:24<00:26,  3.81it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  85% 552/651 [02:24<00:25,  3.82it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  85% 554/651 [02:24<00:25,  3.83it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  85% 556/651 [02:24<00:24,  3.84it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  86% 558/651 [02:24<00:24,  3.85it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  86% 560/651 [02:25<00:23,  3.86it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  86% 562/651 [02:25<00:23,  3.87it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  87% 564/651 [02:25<00:22,  3.88it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  87% 566/651 [02:25<00:21,  3.89it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  87% 568/651 [02:25<00:21,  3.90it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  88% 570/651 [02:25<00:20,  3.90it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  88% 572/651 [02:26<00:20,  3.91it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  88% 574/651 [02:26<00:19,  3.92it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  88% 576/651 [02:26<00:19,  3.93it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  89% 578/651 [02:26<00:18,  3.94it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  89% 580/651 [02:26<00:17,  3.95it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  89% 582/651 [02:26<00:17,  3.96it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  90% 584/651 [02:27<00:16,  3.97it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  90% 586/651 [02:27<00:16,  3.98it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  90% 588/651 [02:27<00:15,  3.99it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  91% 590/651 [02:27<00:15,  4.00it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  91% 592/651 [02:27<00:14,  4.01it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  91% 594/651 [02:27<00:14,  4.01it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  92% 596/651 [02:28<00:13,  4.02it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  92% 598/651 [02:28<00:13,  4.03it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  92% 600/651 [02:28<00:12,  4.04it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  92% 602/651 [02:28<00:12,  4.05it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  93% 604/651 [02:28<00:11,  4.06it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  93% 606/651 [02:28<00:11,  4.07it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  93% 608/651 [02:29<00:10,  4.08it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  94% 610/651 [02:29<00:10,  4.09it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  94% 612/651 [02:29<00:09,  4.09it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  94% 614/651 [02:29<00:09,  4.10it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  95% 616/651 [02:29<00:08,  4.11it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  95% 618/651 [02:29<00:08,  4.12it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  95% 620/651 [02:30<00:07,  4.13it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  96% 622/651 [02:30<00:07,  4.14it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  96% 624/651 [02:30<00:06,  4.15it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  96% 626/651 [02:30<00:06,  4.16it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  96% 628/651 [02:30<00:05,  4.16it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  97% 630/651 [02:30<00:05,  4.17it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  97% 632/651 [02:31<00:04,  4.18it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  97% 634/651 [02:31<00:04,  4.19it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  98% 636/651 [02:31<00:03,  4.20it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  98% 638/651 [02:31<00:03,  4.21it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  98% 640/651 [02:31<00:02,  4.22it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  99% 642/651 [02:31<00:02,  4.22it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  99% 644/651 [02:32<00:01,  4.23it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4:  99% 646/651 [02:32<00:01,  4.24it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4: 100% 648/651 [02:32<00:00,  4.25it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 4: 100% 650/651 [02:32<00:00,  4.26it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.61it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 13.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 8.0\n",
            "eval_tn: 861.0\n",
            "eval_precision: 0.8125\n",
            "eval_recall: 0.6190476417541504\n",
            "eval_f1: 0.7027027010917664\n",
            "eval_mcc: 0.7031984329223633\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4: 100% 651/651 [02:32<00:00,  4.26it/s, loss=0.058, v_num=er-1, train_loss=0.0731]\n",
            "Epoch 5:  83% 540/651 [02:23<00:29,  3.78it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  83% 542/651 [02:23<00:28,  3.78it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  84% 544/651 [02:23<00:28,  3.79it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  84% 546/651 [02:23<00:27,  3.79it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  84% 548/651 [02:24<00:27,  3.80it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  84% 550/651 [02:24<00:26,  3.81it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  85% 552/651 [02:24<00:25,  3.82it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  85% 554/651 [02:24<00:25,  3.83it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  85% 556/651 [02:24<00:24,  3.84it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  86% 558/651 [02:24<00:24,  3.85it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  86% 560/651 [02:25<00:23,  3.86it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  86% 562/651 [02:25<00:22,  3.87it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  87% 564/651 [02:25<00:22,  3.88it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  87% 566/651 [02:25<00:21,  3.89it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  87% 568/651 [02:25<00:21,  3.90it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  88% 570/651 [02:25<00:20,  3.91it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  88% 572/651 [02:26<00:20,  3.92it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  88% 574/651 [02:26<00:19,  3.93it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  88% 576/651 [02:26<00:19,  3.93it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  89% 578/651 [02:26<00:18,  3.94it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  89% 580/651 [02:26<00:17,  3.95it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  89% 582/651 [02:26<00:17,  3.96it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  90% 584/651 [02:27<00:16,  3.97it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  90% 586/651 [02:27<00:16,  3.98it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  90% 588/651 [02:27<00:15,  3.99it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  91% 590/651 [02:27<00:15,  4.00it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  91% 592/651 [02:27<00:14,  4.01it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  91% 594/651 [02:27<00:14,  4.02it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  92% 596/651 [02:28<00:13,  4.03it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  92% 598/651 [02:28<00:13,  4.03it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  92% 600/651 [02:28<00:12,  4.04it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  92% 602/651 [02:28<00:12,  4.05it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  93% 604/651 [02:28<00:11,  4.06it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  93% 606/651 [02:28<00:11,  4.07it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  93% 608/651 [02:29<00:10,  4.08it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  94% 610/651 [02:29<00:10,  4.09it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  94% 612/651 [02:29<00:09,  4.10it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  94% 614/651 [02:29<00:09,  4.11it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  95% 616/651 [02:29<00:08,  4.11it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  95% 618/651 [02:29<00:08,  4.12it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  95% 620/651 [02:30<00:07,  4.13it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  96% 622/651 [02:30<00:07,  4.14it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  96% 624/651 [02:30<00:06,  4.15it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  96% 626/651 [02:30<00:06,  4.16it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  96% 628/651 [02:30<00:05,  4.17it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  97% 630/651 [02:30<00:05,  4.17it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  97% 632/651 [02:31<00:04,  4.18it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  97% 634/651 [02:31<00:04,  4.19it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  98% 636/651 [02:31<00:03,  4.20it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  98% 638/651 [02:31<00:03,  4.21it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  98% 640/651 [02:31<00:02,  4.22it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  99% 642/651 [02:31<00:02,  4.23it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  99% 644/651 [02:32<00:01,  4.23it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5:  99% 646/651 [02:32<00:01,  4.24it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5: 100% 648/651 [02:32<00:00,  4.25it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 5: 100% 650/651 [02:32<00:00,  4.26it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.67it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 17.0\n",
            "eval_fp: 12.0\n",
            "eval_fn: 4.0\n",
            "eval_tn: 852.0\n",
            "eval_precision: 0.5862069129943848\n",
            "eval_recall: 0.8095238208770752\n",
            "eval_f1: 0.6800000071525574\n",
            "eval_mcc: 0.6802122592926025\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5: 100% 651/651 [02:32<00:00,  4.26it/s, loss=0.058, v_num=er-1, train_loss=0.0634]\n",
            "Epoch 6:  83% 540/651 [02:23<00:29,  3.77it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  83% 542/651 [02:23<00:28,  3.77it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  84% 544/651 [02:23<00:28,  3.78it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  84% 546/651 [02:24<00:27,  3.79it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  84% 548/651 [02:24<00:27,  3.80it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  84% 550/651 [02:24<00:26,  3.81it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  85% 552/651 [02:24<00:25,  3.82it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  85% 554/651 [02:24<00:25,  3.83it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  85% 556/651 [02:24<00:24,  3.84it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  86% 558/651 [02:25<00:24,  3.85it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  86% 560/651 [02:25<00:23,  3.85it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  86% 562/651 [02:25<00:23,  3.86it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  87% 564/651 [02:25<00:22,  3.87it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  87% 566/651 [02:25<00:21,  3.88it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  87% 568/651 [02:25<00:21,  3.89it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  88% 570/651 [02:26<00:20,  3.90it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  88% 572/651 [02:26<00:20,  3.91it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  88% 574/651 [02:26<00:19,  3.92it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  88% 576/651 [02:26<00:19,  3.93it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  89% 578/651 [02:26<00:18,  3.94it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  89% 580/651 [02:26<00:17,  3.95it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  89% 582/651 [02:27<00:17,  3.96it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  90% 584/651 [02:27<00:16,  3.97it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  90% 586/651 [02:27<00:16,  3.97it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  90% 588/651 [02:27<00:15,  3.98it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  91% 590/651 [02:27<00:15,  3.99it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  91% 592/651 [02:27<00:14,  4.00it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  91% 594/651 [02:28<00:14,  4.01it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  92% 596/651 [02:28<00:13,  4.02it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  92% 598/651 [02:28<00:13,  4.03it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  92% 600/651 [02:28<00:12,  4.04it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  92% 602/651 [02:28<00:12,  4.05it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  93% 604/651 [02:28<00:11,  4.05it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  93% 606/651 [02:29<00:11,  4.06it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  93% 608/651 [02:29<00:10,  4.07it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  94% 610/651 [02:29<00:10,  4.08it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  94% 612/651 [02:29<00:09,  4.09it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  94% 614/651 [02:29<00:09,  4.10it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  95% 616/651 [02:29<00:08,  4.11it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  95% 618/651 [02:30<00:08,  4.12it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  95% 620/651 [02:30<00:07,  4.13it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  96% 622/651 [02:30<00:07,  4.13it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  96% 624/651 [02:30<00:06,  4.14it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  96% 626/651 [02:30<00:06,  4.15it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  96% 628/651 [02:30<00:05,  4.16it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  97% 630/651 [02:31<00:05,  4.17it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  97% 632/651 [02:31<00:04,  4.18it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  97% 634/651 [02:31<00:04,  4.19it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  98% 636/651 [02:31<00:03,  4.19it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  98% 638/651 [02:31<00:03,  4.20it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  98% 640/651 [02:31<00:02,  4.21it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  99% 642/651 [02:32<00:02,  4.22it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  99% 644/651 [02:32<00:01,  4.23it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6:  99% 646/651 [02:32<00:01,  4.24it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6: 100% 648/651 [02:32<00:00,  4.25it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 6: 100% 650/651 [02:32<00:00,  4.25it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Validating: 100% 111/111 [00:09<00:00, 12.55it/s]\u001b[A\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 14.0\n",
            "eval_fp: 5.0\n",
            "eval_fn: 7.0\n",
            "eval_tn: 859.0\n",
            "eval_precision: 0.7368420958518982\n",
            "eval_recall: 0.6666666865348816\n",
            "eval_f1: 0.699999988079071\n",
            "eval_mcc: 0.6939898729324341\n",
            "--------------------------------------------------------------------------------\n",
            "tcmalloc: large alloc 2057371648 bytes == 0x7fe3abe7e000 @  0x7fe7711c1615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fe76d61e950 0x7fe76d622bf7 0x7fe76d9537e8 0x7fe76d9091b3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "\n",
            "Epoch 6: 100% 651/651 [02:44<00:00,  3.95it/s, loss=0.075, v_num=er-1, train_loss=0.0592]\n",
            "Epoch 7:  83% 540/651 [02:23<00:29,  3.75it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  83% 541/651 [02:24<00:29,  3.75it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  83% 543/651 [02:24<00:28,  3.75it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  84% 545/651 [02:24<00:28,  3.76it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  84% 547/651 [02:24<00:27,  3.77it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  84% 549/651 [02:25<00:26,  3.78it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  85% 551/651 [02:25<00:26,  3.79it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  85% 553/651 [02:25<00:25,  3.80it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  85% 555/651 [02:25<00:25,  3.81it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  86% 557/651 [02:25<00:24,  3.82it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  86% 559/651 [02:25<00:24,  3.83it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  86% 561/651 [02:26<00:23,  3.84it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  86% 563/651 [02:26<00:22,  3.85it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  87% 565/651 [02:26<00:22,  3.86it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  87% 567/651 [02:26<00:21,  3.87it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  87% 569/651 [02:26<00:21,  3.88it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  88% 571/651 [02:26<00:20,  3.89it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  88% 573/651 [02:27<00:20,  3.89it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  88% 575/651 [02:27<00:19,  3.90it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  89% 577/651 [02:27<00:18,  3.91it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  89% 579/651 [02:27<00:18,  3.92it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  89% 581/651 [02:27<00:17,  3.93it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  90% 583/651 [02:27<00:17,  3.94it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  90% 585/651 [02:28<00:16,  3.95it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  90% 587/651 [02:28<00:16,  3.96it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  90% 589/651 [02:28<00:15,  3.97it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  91% 591/651 [02:28<00:15,  3.98it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  91% 593/651 [02:28<00:14,  3.99it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  91% 595/651 [02:28<00:14,  3.99it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  92% 597/651 [02:29<00:13,  4.00it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  92% 599/651 [02:29<00:12,  4.01it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  92% 601/651 [02:29<00:12,  4.02it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  93% 603/651 [02:29<00:11,  4.03it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  93% 605/651 [02:29<00:11,  4.04it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  93% 607/651 [02:29<00:10,  4.05it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  94% 609/651 [02:30<00:10,  4.06it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  94% 611/651 [02:30<00:09,  4.06it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  94% 613/651 [02:30<00:09,  4.07it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  94% 615/651 [02:30<00:08,  4.08it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  95% 617/651 [02:30<00:08,  4.09it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  95% 619/651 [02:30<00:07,  4.10it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  95% 621/651 [02:31<00:07,  4.11it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  96% 623/651 [02:31<00:06,  4.12it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  96% 625/651 [02:31<00:06,  4.13it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  96% 627/651 [02:31<00:05,  4.13it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  97% 629/651 [02:31<00:05,  4.14it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  97% 631/651 [02:31<00:04,  4.15it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  97% 633/651 [02:32<00:04,  4.16it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  98% 635/651 [02:32<00:03,  4.17it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  98% 637/651 [02:32<00:03,  4.18it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  98% 639/651 [02:32<00:02,  4.19it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  98% 641/651 [02:32<00:02,  4.19it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  99% 643/651 [02:32<00:01,  4.20it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  99% 645/651 [02:33<00:01,  4.21it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7:  99% 647/651 [02:33<00:00,  4.22it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7: 100% 649/651 [02:33<00:00,  4.23it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Epoch 7: 100% 651/651 [02:33<00:00,  4.24it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON EVALSET\n",
            "eval_tp: 15.0\n",
            "eval_fp: 3.0\n",
            "eval_fn: 6.0\n",
            "eval_tn: 861.0\n",
            "eval_precision: 0.8333333134651184\n",
            "eval_recall: 0.7142857313156128\n",
            "eval_f1: 0.7692307829856873\n",
            "eval_mcc: 0.7664368748664856\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7: 100% 651/651 [02:33<00:00,  4.23it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 7: 100% 651/651 [02:34<00:00,  4.23it/s, loss=0.023, v_num=er-1, train_loss=0.0579]\n",
            "Computing Input\n",
            "100% 4319/4319 [00:06<00:00, 671.25it/s]\n",
            "100% 885/885 [00:01<00:00, 650.14it/s]\n",
            "100% 77/77 [00:00<00:00, 587.61it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.68it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 9.0\n",
            "fp: 0.0\n",
            "fn: 0.0\n",
            "tn: 68.0\n",
            "precision: 1.0\n",
            "recall: 1.0\n",
            "f1: 1.0\n",
            "mcc: 1.0\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(1.), 'test_loss': tensor(0.0356, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.52it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk3SyoiaBbKA"
      },
      "source": [
        "### Hyperparameter Loss Weight for 21-Label Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hBGYwg3Y8FZ",
        "outputId": "63d78d6e-f861-4186-cd60-24509062973b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 2.0 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 10:54:31.013033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 2.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:14<00:00, 405.92it/s]\n",
            "100% 885/885 [00:02<00:00, 418.77it/s]\n",
            "100% 77/77 [00:00<00:00, 448.36it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 606 K \n",
            "Epoch 4: 100% 724/724 [17:16<00:00,  1.43s/it, loss=0.466, v_num=k-21, train_loss=0.472]Saving latest checkpoint..\n",
            "Epoch 4: 100% 724/724 [17:16<00:00,  1.43s/it, loss=0.466, v_num=k-21, train_loss=0.472]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:12<00:00, 478.14it/s]\n",
            "100% 885/885 [00:01<00:00, 470.09it/s]\n",
            "100% 77/77 [00:00<00:00, 492.25it/s]\n",
            "Testing: 100% 10/10 [00:05<00:00,  2.19it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 29.0\n",
            "fp: 36.0\n",
            "fn: 150.0\n",
            "tn: 1402.0\n",
            "precision: 0.446153849363327\n",
            "recall: 0.16201117634773254\n",
            "f1: 0.23770491778850555\n",
            "mcc: 0.21880093216896057\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0125), 'test_loss': tensor(0.3432, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:05<00:00,  1.92it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLrsLZwNY74K",
        "outputId": "1240d639-b578-451f-a9ff-b827098ddac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 2.5 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 12:21:46.291206: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 2.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:14<00:00, 392.86it/s]\n",
            "100% 885/885 [00:02<00:00, 413.50it/s]\n",
            "100% 77/77 [00:00<00:00, 444.67it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 606 K \n",
            "Epoch 4: 100% 724/724 [17:16<00:00,  1.43s/it, loss=0.506, v_num=k-21, train_loss=0.512]Saving latest checkpoint..\n",
            "Epoch 4: 100% 724/724 [17:16<00:00,  1.43s/it, loss=0.506, v_num=k-21, train_loss=0.512]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:12<00:00, 475.21it/s]\n",
            "100% 885/885 [00:01<00:00, 478.14it/s]\n",
            "100% 77/77 [00:00<00:00, 466.04it/s]\n",
            "Testing: 100% 10/10 [00:05<00:00,  2.19it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 54.0\n",
            "fp: 69.0\n",
            "fn: 125.0\n",
            "tn: 1369.0\n",
            "precision: 0.4390243887901306\n",
            "recall: 0.3016759753227234\n",
            "f1: 0.3576158881187439\n",
            "mcc: 0.3002520799636841\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0500), 'test_loss': tensor(0.3549, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:05<00:00,  1.91it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUrcM8AOY7uR",
        "outputId": "85906ac8-842a-41bd-927e-ae59e2a81546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 3.0 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 13:49:00.008995: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:14<00:00, 392.99it/s]\n",
            "100% 885/885 [00:02<00:00, 418.53it/s]\n",
            "100% 77/77 [00:00<00:00, 432.36it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 606 K \n",
            "Epoch 4: 100% 724/724 [17:15<00:00,  1.43s/it, loss=0.544, v_num=k-21, train_loss=0.549]Saving latest checkpoint..\n",
            "Epoch 4: 100% 724/724 [17:16<00:00,  1.43s/it, loss=0.544, v_num=k-21, train_loss=0.549]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:12<00:00, 461.01it/s]\n",
            "100% 885/885 [00:01<00:00, 453.01it/s]\n",
            "100% 77/77 [00:00<00:00, 470.04it/s]\n",
            "Testing: 100% 10/10 [00:05<00:00,  2.20it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 75.0\n",
            "fp: 105.0\n",
            "fn: 104.0\n",
            "tn: 1333.0\n",
            "precision: 0.4166666567325592\n",
            "recall: 0.4189944267272949\n",
            "f1: 0.417827308177948\n",
            "mcc: 0.3451339602470398\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0125), 'test_loss': tensor(0.3707, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:05<00:00,  1.92it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEa_kG9_3dqE",
        "outputId": "9a95b367-19b3-47e5-97b1-8179f931aa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 4.0 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 15:16:12.400110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 4.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:15<00:00, 383.47it/s]\n",
            "100% 885/885 [00:02<00:00, 410.36it/s]\n",
            "100% 77/77 [00:00<00:00, 447.50it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 606 K \n",
            "Epoch 4: 100% 724/724 [17:15<00:00,  1.43s/it, loss=0.606, v_num=k-21, train_loss=0.613]Saving latest checkpoint..\n",
            "Epoch 4: 100% 724/724 [17:15<00:00,  1.43s/it, loss=0.606, v_num=k-21, train_loss=0.613]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:12<00:00, 467.38it/s]\n",
            "100% 885/885 [00:01<00:00, 460.06it/s]\n",
            "100% 77/77 [00:00<00:00, 462.19it/s]\n",
            "Testing: 100% 10/10 [00:05<00:00,  2.18it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 98.0\n",
            "fp: 182.0\n",
            "fn: 81.0\n",
            "tn: 1256.0\n",
            "precision: 0.3499999940395355\n",
            "recall: 0.5474860072135925\n",
            "f1: 0.4270152449607849\n",
            "mcc: 0.349029541015625\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0125), 'test_loss': tensor(0.4038, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:05<00:00,  1.93it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlA0n9aN-uwB",
        "outputId": "eb8368d4-1188-4673-d201-94d3f59e4082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 21 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 4.5 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 18:29:31.019219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 21,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 4.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5789/5789 [00:11<00:00, 522.16it/s]\n",
            "100% 885/885 [00:01<00:00, 565.60it/s]\n",
            "100% 77/77 [00:00<00:00, 569.29it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 606 K \n",
            "Epoch 4: 100% 724/724 [05:26<00:00,  2.22it/s, loss=0.630, v_num=k-21, train_loss=0.642]Saving latest checkpoint..\n",
            "Epoch 4: 100% 724/724 [05:26<00:00,  2.22it/s, loss=0.630, v_num=k-21, train_loss=0.642]\n",
            "Computing Input\n",
            "100% 5789/5789 [00:09<00:00, 630.11it/s]\n",
            "100% 885/885 [00:01<00:00, 625.48it/s]\n",
            "100% 77/77 [00:00<00:00, 655.16it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.87it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 113.0\n",
            "fp: 217.0\n",
            "fn: 66.0\n",
            "tn: 1221.0\n",
            "precision: 0.3424242436885834\n",
            "recall: 0.6312848925590515\n",
            "f1: 0.44400784373283386\n",
            "mcc: 0.3739774525165558\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.0125), 'test_loss': tensor(0.4260, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.65it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYqxK6ogBUJH"
      },
      "source": [
        "### Hyperparameter Loss Weight for 5-Label Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVF6S9xOl02k",
        "outputId": "002d9d8d-d365-478e-df82-3c2b64791e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 5 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 1.5 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 17:13:48.594509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.74MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.81MB/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 396kB/s]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 5,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 1.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 3887/3887 [00:07<00:00, 493.93it/s]\n",
            "100% 885/885 [00:01<00:00, 566.80it/s]\n",
            "100% 77/77 [00:00<00:00, 518.11it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 594 K \n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.435, v_num=nk-5, train_loss=0.447]Saving latest checkpoint..\n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.435, v_num=nk-5, train_loss=0.447]\n",
            "Computing Input\n",
            "100% 3887/3887 [00:06<00:00, 614.79it/s]\n",
            "100% 885/885 [00:01<00:00, 606.00it/s]\n",
            "100% 77/77 [00:00<00:00, 591.78it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.94it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 26.0\n",
            "fp: 18.0\n",
            "fn: 17.0\n",
            "tn: 324.0\n",
            "precision: 0.5909090638160706\n",
            "recall: 0.604651153087616\n",
            "f1: 0.5977011322975159\n",
            "mcc: 0.5465101599693298\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.6225), 'test_loss': tensor(0.2981, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.76it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxedAXg9l0io",
        "outputId": "4c64d370-e36c-4c95-a654-a19d7b43cb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 5 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 2.0 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 17:33:04.626659: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 5,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 2.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 3887/3887 [00:08<00:00, 483.26it/s]\n",
            "100% 885/885 [00:01<00:00, 549.13it/s]\n",
            "100% 77/77 [00:00<00:00, 578.75it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 594 K \n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.502, v_num=nk-5, train_loss=0.515]Saving latest checkpoint..\n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.502, v_num=nk-5, train_loss=0.515]\n",
            "Computing Input\n",
            "100% 3887/3887 [00:06<00:00, 601.27it/s]\n",
            "100% 885/885 [00:01<00:00, 610.18it/s]\n",
            "100% 77/77 [00:00<00:00, 639.36it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.94it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 29.0\n",
            "fp: 22.0\n",
            "fn: 14.0\n",
            "tn: 320.0\n",
            "precision: 0.5686274766921997\n",
            "recall: 0.6744186282157898\n",
            "f1: 0.6170212626457214\n",
            "mcc: 0.5668702721595764\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.6100), 'test_loss': tensor(0.3294, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.75it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4MjQpdkCVPr",
        "outputId": "53fca277-a462-4fa5-d233-9083c28641b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 5 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 2.5 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 17:51:51.039427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 5,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 2.5,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 3887/3887 [00:07<00:00, 493.15it/s]\n",
            "100% 885/885 [00:01<00:00, 557.68it/s]\n",
            "100% 77/77 [00:00<00:00, 524.70it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 594 K \n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.559, v_num=nk-5, train_loss=0.573]Saving latest checkpoint..\n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.559, v_num=nk-5, train_loss=0.573]\n",
            "Computing Input\n",
            "100% 3887/3887 [00:06<00:00, 603.65it/s]\n",
            "100% 885/885 [00:01<00:00, 606.35it/s]\n",
            "100% 77/77 [00:00<00:00, 650.60it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.90it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 29.0\n",
            "fp: 39.0\n",
            "fn: 14.0\n",
            "tn: 303.0\n",
            "precision: 0.4264705777168274\n",
            "recall: 0.6744186282157898\n",
            "f1: 0.522522509098053\n",
            "mcc: 0.46285882592201233\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.4650), 'test_loss': tensor(0.3609, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.64it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n54RJGroSnIp",
        "outputId": "434dcaa9-2300-4841-f205-ae673312080c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_roberta.py \\\n",
        "--data_path \"/content/drive/My Drive/cls_data/train-eval-split/1\" \\\n",
        "--summary_type \"textrank\" \\\n",
        "--amount_labels 5 \\\n",
        "--lm_path \"/content/drive/My Drive/language_model/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/My Drive/training_output\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 8 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 5 \\\n",
        "--check_val_every_n_epoch 10 \\\n",
        "--loss_weight 3.0 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-09-23 18:10:37.283815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/My Drive/cls_data/train-eval-split/1\",\n",
            "    \"summary_type\": \"textrank\",\n",
            "    \"amount_labels\": 5,\n",
            "    \"lm_path\": \"/content/drive/My Drive/language_model/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/My Drive/training_output\",\n",
            "    \"loss_weight\": 3.0,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"max_learning_rate\": null,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 5,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 10,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 3887/3887 [00:08<00:00, 482.53it/s]\n",
            "100% 885/885 [00:01<00:00, 552.48it/s]\n",
            "100% 77/77 [00:00<00:00, 546.76it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | classifier | ClassifierExtension | 594 K \n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.607, v_num=nk-5, train_loss=0.622]Saving latest checkpoint..\n",
            "Epoch 4: 100% 486/486 [03:38<00:00,  2.22it/s, loss=0.607, v_num=nk-5, train_loss=0.622]\n",
            "Computing Input\n",
            "100% 3887/3887 [00:06<00:00, 603.47it/s]\n",
            "100% 885/885 [00:01<00:00, 619.96it/s]\n",
            "100% 77/77 [00:00<00:00, 578.02it/s]\n",
            "Testing: 100% 10/10 [00:01<00:00,  6.94it/s]--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 30.0\n",
            "fp: 51.0\n",
            "fn: 13.0\n",
            "tn: 291.0\n",
            "precision: 0.37037035822868347\n",
            "recall: 0.6976743936538696\n",
            "f1: 0.4838709533214569\n",
            "mcc: 0.42392152547836304\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.3900), 'test_loss': tensor(0.3919, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 10/10 [00:01<00:00,  5.74it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ivTB6oTvPF"
      },
      "source": [
        "## RecRob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gHiIUuKvcGl"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_RecRob.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZUOKHshqPBL",
        "outputId": "c4abc6f3-6150-4ddd-f9cb-3a15c61f291f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python recrob_experiments.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--gpus 1 \\\n",
        "--batch_size 1 \\\n",
        "--num_workers 4 \\\n",
        "--max_epochs 10 \\\n",
        "--scheduler \\\n",
        "--learning_rate_ext 0.001 \\\n",
        "--check_val_every_n_epoch 12 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-11 09:28:40.496043: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 975kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 496kB/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 355kB/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/0\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"rnn_hidden_size\": 384,\n",
            "    \"cls_hidden_size\": 192,\n",
            "    \"split_size\": 200,\n",
            "    \"shift\": 50,\n",
            "    \"scheduler\": true,\n",
            "    \"loss_weight\": 1.0,\n",
            "    \"batch_size\": 1,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate_ext\": 0.001,\n",
            "    \"learning_rate_lm\": 1e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 10,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 12,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5242/5242 [00:30<00:00, 174.47it/s]\n",
            "100% 885/885 [00:05<00:00, 174.77it/s]\n",
            "100% 77/77 [00:00<00:00, 187.33it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | rnn        | RNNExtension        | 1 M   \n",
            "3 | classifier | ClassifierExtension | 74 K  \n",
            "Epoch 2: 100% 5242/5242 [09:45<00:00,  8.95it/s, loss=0.693, v_num=1-0, train_loss=0.693]Epoch     3: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 4: 100% 5242/5242 [09:45<00:00,  8.96it/s, loss=0.693, v_num=1-0, train_loss=0.693]Epoch     5: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 6: 100% 5242/5242 [09:45<00:00,  8.96it/s, loss=0.693, v_num=1-0, train_loss=0.693]Epoch     7: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 8: 100% 5242/5242 [09:44<00:00,  8.97it/s, loss=0.693, v_num=1-0, train_loss=0.693]Epoch     9: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 9: 100% 5242/5242 [09:44<00:00,  8.97it/s, loss=0.693, v_num=1-0, train_loss=0.693]Saving latest checkpoint..\n",
            "Epoch 9: 100% 5242/5242 [09:45<00:00,  8.95it/s, loss=0.693, v_num=1-0, train_loss=0.693]\n",
            "Computing Input\n",
            "100% 5242/5242 [00:30<00:00, 174.71it/s]\n",
            "100% 885/885 [00:05<00:00, 175.24it/s]\n",
            "100% 77/77 [00:00<00:00, 180.56it/s]\n",
            "Testing:  99% 76/77 [00:03<00:00, 26.66it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 0.0\n",
            "fp: 0.0\n",
            "fn: 10.0\n",
            "tn: 67.0\n",
            "precision: 0\n",
            "recall: 0.0\n",
            "f1: 0\n",
            "mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.8701), 'test_loss': tensor(0.6927, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 77/77 [00:03<00:00, 20.94it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/1\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"rnn_hidden_size\": 384,\n",
            "    \"cls_hidden_size\": 192,\n",
            "    \"split_size\": 200,\n",
            "    \"shift\": 50,\n",
            "    \"scheduler\": true,\n",
            "    \"loss_weight\": 1.0,\n",
            "    \"batch_size\": 1,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate_ext\": 0.001,\n",
            "    \"learning_rate_lm\": 1e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 10,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 12,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5258/5258 [00:30<00:00, 172.94it/s]\n",
            "100% 885/885 [00:04<00:00, 179.82it/s]\n",
            "100% 77/77 [00:00<00:00, 183.11it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | rnn        | RNNExtension        | 1 M   \n",
            "3 | classifier | ClassifierExtension | 74 K  \n",
            "Epoch 2: 100% 5258/5258 [09:55<00:00,  8.83it/s, loss=0.695, v_num=1-1, train_loss=0.693]Epoch     3: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 4: 100% 5258/5258 [09:56<00:00,  8.82it/s, loss=0.693, v_num=1-1, train_loss=0.693]Epoch     5: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 6: 100% 5258/5258 [09:54<00:00,  8.84it/s, loss=0.693, v_num=1-1, train_loss=0.693]Epoch     7: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 8: 100% 5258/5258 [09:53<00:00,  8.86it/s, loss=0.693, v_num=1-1, train_loss=0.693]Epoch     9: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 9: 100% 5258/5258 [09:54<00:00,  8.85it/s, loss=0.693, v_num=1-1, train_loss=0.693]Saving latest checkpoint..\n",
            "Epoch 9: 100% 5258/5258 [09:55<00:00,  8.83it/s, loss=0.693, v_num=1-1, train_loss=0.693]\n",
            "Computing Input\n",
            "100% 5258/5258 [00:30<00:00, 171.30it/s]\n",
            "100% 885/885 [00:05<00:00, 162.94it/s]\n",
            "100% 77/77 [00:00<00:00, 179.22it/s]\n",
            "Testing:  99% 76/77 [00:03<00:00, 26.03it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 10.0\n",
            "fp: 67.0\n",
            "fn: 0.0\n",
            "tn: 0.0\n",
            "precision: 0.12987013161182404\n",
            "recall: 1.0\n",
            "f1: 0.2298850566148758\n",
            "mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.1299), 'test_loss': tensor(0.6935, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 77/77 [00:03<00:00, 20.90it/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"rnn_hidden_size\": 384,\n",
            "    \"cls_hidden_size\": 192,\n",
            "    \"split_size\": 200,\n",
            "    \"shift\": 50,\n",
            "    \"scheduler\": true,\n",
            "    \"loss_weight\": 1.0,\n",
            "    \"batch_size\": 1,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate_ext\": 0.001,\n",
            "    \"learning_rate_lm\": 1e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 10,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 12,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5295/5295 [00:30<00:00, 174.58it/s]\n",
            "100% 885/885 [00:04<00:00, 183.16it/s]\n",
            "100% 77/77 [00:00<00:00, 182.28it/s]\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | rnn        | RNNExtension        | 1 M   \n",
            "3 | classifier | ClassifierExtension | 74 K  \n",
            "Epoch 2: 100% 5295/5295 [09:57<00:00,  8.87it/s, loss=0.691, v_num=1-2, train_loss=0.693]Epoch     3: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 4: 100% 5295/5295 [09:57<00:00,  8.86it/s, loss=0.694, v_num=1-2, train_loss=0.693]Epoch     5: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 6:  24% 1284/5295 [02:24<07:31,  8.89it/s, loss=0.693, v_num=1-2, train_loss=0.693]Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZsK_QHqPq5"
      },
      "source": [
        "### Hyperparameter Epochs & Size & Loss Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OaQck57FJXx"
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_RecRob.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--amount_labels 5 \\\n",
        "--num_workers 4 \\\n",
        "--batch_size 1 \\\n",
        "--scheduler \\\n",
        "--learning_rate_ext 0.001 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--loss_weight 3.25 \\\n",
        "--gpus 1 \\\n",
        "--max_epochs 3 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOGRTH30L09e",
        "outputId": "918a4feb-ff83-41f1-fb88-1b16152fc6c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\"\n",
        "!python train_RecRob.py \\\n",
        "--data_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\" \\\n",
        "--lm_path \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\" \\\n",
        "--output_dir \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\" \\\n",
        "--amount_labels 1 \\\n",
        "--num_workers 4 \\\n",
        "--batch_size 1 \\\n",
        "--scheduler \\\n",
        "--learning_rate_ext 0.001 \\\n",
        "--check_val_every_n_epoch 1 \\\n",
        "--loss_weight 1.05 \\\n",
        "--gpus 1 \\\n",
        "--max_epochs 3 \\\n",
        "--amp_level \"O1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model/news-topic-cls/core\n",
            "2020-10-07 15:40:43.659468: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Hyperparameter:\n",
            "_______________\n",
            "{\n",
            "    \"data_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/cls-data/train-eval-split/2\",\n",
            "    \"amount_labels\": 1,\n",
            "    \"lm_path\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/language-models/roberta-lm\",\n",
            "    \"output_dir\": \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/classifier-output/\",\n",
            "    \"rnn_hidden_size\": 384,\n",
            "    \"cls_hidden_size\": 192,\n",
            "    \"split_size\": 200,\n",
            "    \"shift\": 50,\n",
            "    \"scheduler\": true,\n",
            "    \"loss_weight\": 1.05,\n",
            "    \"batch_size\": 1,\n",
            "    \"num_workers\": 4,\n",
            "    \"learning_rate_ext\": 0.001,\n",
            "    \"learning_rate_lm\": 1e-05,\n",
            "    \"lr_decay\": 0.95,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"max_epochs\": 3,\n",
            "    \"gradient_clip_val\": 0.0,\n",
            "    \"accumulate_grad_batches\": 1,\n",
            "    \"gpus\": 1,\n",
            "    \"tpu_cores\": null,\n",
            "    \"amp_level\": \"O1\",\n",
            "    \"auto_lr_find\": false,\n",
            "    \"check_val_every_n_epoch\": 1,\n",
            "    \"fast_dev_run\": false\n",
            "}\n",
            "Computing Input\n",
            "100% 5295/5295 [00:31<00:00, 169.24it/s]\n",
            "100% 885/885 [00:05<00:00, 174.91it/s]\n",
            "100% 77/77 [00:00<00:00, 183.89it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name       | Type                | Params\n",
            "---------------------------------------------------\n",
            "0 | roberta    | RobertaModel        | 124 M \n",
            "1 | dropout    | Dropout             | 0     \n",
            "2 | rnn        | RNNExtension        | 1 M   \n",
            "3 | classifier | ClassifierExtension | 74 K  \n",
            "Epoch 0:  86% 5295/6180 [09:52<01:39,  8.93it/s, loss=0.709, v_num=2-1]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 5296/6180 [09:53<01:39,  8.92it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5300/6180 [09:53<01:38,  8.93it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5304/6180 [09:53<01:38,  8.93it/s, loss=0.709, v_num=2-1]\n",
            "Validating:   1% 10/885 [00:00<03:02,  4.80it/s]\u001b[A\n",
            "Epoch 0:  86% 5308/6180 [09:53<01:37,  8.94it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5312/6180 [09:54<01:37,  8.94it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5316/6180 [09:54<01:36,  8.95it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5320/6180 [09:54<01:36,  8.95it/s, loss=0.709, v_num=2-1]\n",
            "Validating:   3% 26/885 [00:01<01:00, 14.20it/s]\u001b[A\n",
            "Epoch 0:  86% 5324/6180 [09:54<01:35,  8.95it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5328/6180 [09:54<01:35,  8.96it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5332/6180 [09:54<01:34,  8.96it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5336/6180 [09:54<01:34,  8.97it/s, loss=0.709, v_num=2-1]\n",
            "Validating:   5% 42/885 [00:02<00:36, 22.91it/s]\u001b[A\n",
            "Epoch 0:  86% 5340/6180 [09:55<01:33,  8.97it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  86% 5344/6180 [09:55<01:33,  8.98it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5348/6180 [09:55<01:32,  8.98it/s, loss=0.709, v_num=2-1]\n",
            "Validating:   6% 54/885 [00:02<00:34, 24.44it/s]\u001b[A\n",
            "Epoch 0:  87% 5352/6180 [09:55<01:32,  8.99it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5356/6180 [09:55<01:31,  8.99it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5360/6180 [09:55<01:31,  8.99it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5364/6180 [09:56<01:30,  9.00it/s, loss=0.709, v_num=2-1]\n",
            "Validating:   8% 70/885 [00:03<00:31, 26.19it/s]\u001b[A\n",
            "Epoch 0:  87% 5368/6180 [09:56<01:30,  9.00it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5372/6180 [09:56<01:29,  9.01it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5376/6180 [09:56<01:29,  9.01it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5380/6180 [09:56<01:28,  9.02it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5384/6180 [09:56<01:28,  9.02it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5388/6180 [09:56<01:27,  9.03it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5392/6180 [09:57<01:27,  9.03it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5396/6180 [09:57<01:26,  9.03it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  12% 102/885 [00:04<00:28, 27.74it/s]\u001b[A\n",
            "Epoch 0:  87% 5400/6180 [09:57<01:26,  9.04it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  87% 5404/6180 [09:57<01:25,  9.04it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5408/6180 [09:57<01:25,  9.05it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  13% 114/885 [00:04<00:30, 25.07it/s]\u001b[A\n",
            "Epoch 0:  88% 5412/6180 [09:57<01:24,  9.05it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5416/6180 [09:58<01:24,  9.06it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5420/6180 [09:58<01:23,  9.06it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  14% 126/885 [00:05<00:28, 26.24it/s]\u001b[A\n",
            "Epoch 0:  88% 5424/6180 [09:58<01:23,  9.06it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5428/6180 [09:58<01:22,  9.07it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5432/6180 [09:58<01:22,  9.07it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5436/6180 [09:58<01:21,  9.08it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5440/6180 [09:58<01:21,  9.08it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  16% 146/885 [00:06<00:28, 25.80it/s]\u001b[A\n",
            "Epoch 0:  88% 5444/6180 [09:59<01:21,  9.09it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5448/6180 [09:59<01:20,  9.09it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5452/6180 [09:59<01:20,  9.09it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5456/6180 [09:59<01:19,  9.10it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  18% 162/885 [00:06<00:28, 25.09it/s]\u001b[A\n",
            "Epoch 0:  88% 5460/6180 [09:59<01:19,  9.10it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5464/6180 [09:59<01:18,  9.11it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  88% 5468/6180 [10:00<01:18,  9.11it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5472/6180 [10:00<01:17,  9.12it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  20% 178/885 [00:07<00:28, 25.07it/s]\u001b[A\n",
            "Epoch 0:  89% 5476/6180 [10:00<01:17,  9.12it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5480/6180 [10:00<01:16,  9.12it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5484/6180 [10:00<01:16,  9.13it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5488/6180 [10:00<01:15,  9.13it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5492/6180 [10:01<01:15,  9.14it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5496/6180 [10:01<01:14,  9.14it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  23% 202/885 [00:08<00:27, 24.50it/s]\u001b[A\n",
            "Epoch 0:  89% 5500/6180 [10:01<01:14,  9.15it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5504/6180 [10:01<01:13,  9.15it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5508/6180 [10:01<01:13,  9.16it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5512/6180 [10:01<01:12,  9.16it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  25% 218/885 [00:09<00:28, 23.41it/s]\u001b[A\n",
            "Epoch 0:  89% 5516/6180 [10:01<01:12,  9.16it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5520/6180 [10:02<01:11,  9.17it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  89% 5524/6180 [10:02<01:11,  9.17it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  26% 230/885 [00:09<00:26, 25.13it/s]\u001b[A\n",
            "Epoch 0:  89% 5528/6180 [10:02<01:11,  9.18it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5532/6180 [10:02<01:10,  9.18it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5536/6180 [10:02<01:10,  9.19it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5540/6180 [10:02<01:09,  9.19it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5544/6180 [10:03<01:09,  9.19it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  28% 250/885 [00:10<00:24, 26.14it/s]\u001b[A\n",
            "Epoch 0:  90% 5548/6180 [10:03<01:08,  9.20it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5552/6180 [10:03<01:08,  9.20it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5556/6180 [10:03<01:07,  9.21it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  30% 262/885 [00:10<00:27, 23.00it/s]\u001b[A\n",
            "Epoch 0:  90% 5560/6180 [10:03<01:07,  9.21it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5564/6180 [10:03<01:06,  9.21it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5568/6180 [10:04<01:06,  9.22it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  31% 274/885 [00:11<00:24, 24.88it/s]\u001b[A\n",
            "Epoch 0:  90% 5572/6180 [10:04<01:05,  9.22it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5576/6180 [10:04<01:05,  9.23it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5580/6180 [10:04<01:04,  9.23it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5584/6180 [10:04<01:04,  9.24it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5588/6180 [10:04<01:04,  9.24it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  90% 5592/6180 [10:04<01:03,  9.24it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  34% 298/885 [00:12<00:26, 22.00it/s]\u001b[A\n",
            "Epoch 0:  91% 5596/6180 [10:05<01:03,  9.25it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5600/6180 [10:05<01:02,  9.25it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5604/6180 [10:05<01:02,  9.26it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  35% 310/885 [00:12<00:22, 25.84it/s]\u001b[A\n",
            "Epoch 0:  91% 5608/6180 [10:05<01:01,  9.26it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5612/6180 [10:05<01:01,  9.26it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5616/6180 [10:05<01:00,  9.27it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  36% 322/885 [00:13<00:28, 20.08it/s]\u001b[A\n",
            "Epoch 0:  91% 5620/6180 [10:06<01:00,  9.27it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5624/6180 [10:06<00:59,  9.27it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5628/6180 [10:06<00:59,  9.28it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5632/6180 [10:06<00:59,  9.28it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5636/6180 [10:06<00:58,  9.29it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  39% 342/885 [00:14<00:26, 20.85it/s]\u001b[A\n",
            "Epoch 0:  91% 5640/6180 [10:07<00:58,  9.29it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5644/6180 [10:07<00:57,  9.29it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5648/6180 [10:07<00:57,  9.30it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  91% 5652/6180 [10:07<00:56,  9.30it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  40% 358/885 [00:14<00:21, 25.07it/s]\u001b[A\n",
            "Epoch 0:  92% 5656/6180 [10:07<00:56,  9.31it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5660/6180 [10:07<00:55,  9.31it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5664/6180 [10:07<00:55,  9.32it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5668/6180 [10:08<00:54,  9.32it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5672/6180 [10:08<00:54,  9.32it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  43% 378/885 [00:15<00:20, 25.17it/s]\u001b[A\n",
            "Epoch 0:  92% 5676/6180 [10:08<00:54,  9.33it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5680/6180 [10:08<00:53,  9.33it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5684/6180 [10:08<00:53,  9.34it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5688/6180 [10:08<00:52,  9.34it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  45% 394/885 [00:16<00:19, 25.27it/s]\u001b[A\n",
            "Epoch 0:  92% 5692/6180 [10:09<00:52,  9.34it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5696/6180 [10:09<00:51,  9.35it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5700/6180 [10:09<00:51,  9.35it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5704/6180 [10:09<00:50,  9.36it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5708/6180 [10:09<00:50,  9.36it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  92% 5712/6180 [10:09<00:49,  9.37it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  47% 418/885 [00:17<00:16, 27.71it/s]\u001b[A\n",
            "Epoch 0:  92% 5716/6180 [10:10<00:49,  9.37it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5720/6180 [10:10<00:49,  9.37it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5724/6180 [10:10<00:48,  9.38it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5728/6180 [10:10<00:48,  9.38it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  49% 434/885 [00:17<00:17, 26.17it/s]\u001b[A\n",
            "Epoch 0:  93% 5732/6180 [10:10<00:47,  9.39it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5736/6180 [10:10<00:47,  9.39it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5740/6180 [10:10<00:46,  9.40it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5744/6180 [10:11<00:46,  9.40it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  51% 450/885 [00:18<00:16, 27.01it/s]\u001b[A\n",
            "Epoch 0:  93% 5748/6180 [10:11<00:45,  9.40it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5752/6180 [10:11<00:45,  9.41it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5756/6180 [10:11<00:45,  9.41it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  52% 462/885 [00:18<00:17, 24.54it/s]\u001b[A\n",
            "Epoch 0:  93% 5760/6180 [10:11<00:44,  9.42it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5764/6180 [10:11<00:44,  9.42it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5768/6180 [10:12<00:43,  9.42it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  54% 474/885 [00:19<00:17, 23.90it/s]\u001b[A\n",
            "Epoch 0:  93% 5772/6180 [10:12<00:43,  9.43it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  93% 5776/6180 [10:12<00:42,  9.43it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5780/6180 [10:12<00:42,  9.43it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5784/6180 [10:12<00:41,  9.44it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5788/6180 [10:12<00:41,  9.44it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5792/6180 [10:13<00:41,  9.45it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5796/6180 [10:13<00:40,  9.45it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  57% 502/885 [00:20<00:13, 28.48it/s]\u001b[A\n",
            "Epoch 0:  94% 5800/6180 [10:13<00:40,  9.46it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5804/6180 [10:13<00:39,  9.46it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5808/6180 [10:13<00:39,  9.46it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  58% 514/885 [00:20<00:13, 26.86it/s]\u001b[A\n",
            "Epoch 0:  94% 5812/6180 [10:13<00:38,  9.47it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5816/6180 [10:13<00:38,  9.47it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5820/6180 [10:14<00:37,  9.48it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5824/6180 [10:14<00:37,  9.48it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  60% 530/885 [00:21<00:13, 25.49it/s]\u001b[A\n",
            "Epoch 0:  94% 5828/6180 [10:14<00:37,  9.48it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5832/6180 [10:14<00:36,  9.49it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  94% 5836/6180 [10:14<00:36,  9.49it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  61% 542/885 [00:21<00:14, 23.68it/s]\u001b[A\n",
            "Epoch 0:  94% 5840/6180 [10:15<00:35,  9.50it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5844/6180 [10:15<00:35,  9.50it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5848/6180 [10:15<00:34,  9.50it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5852/6180 [10:15<00:34,  9.51it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  63% 558/885 [00:22<00:12, 26.24it/s]\u001b[A\n",
            "Epoch 0:  95% 5856/6180 [10:15<00:34,  9.51it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5860/6180 [10:15<00:33,  9.52it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5864/6180 [10:15<00:33,  9.52it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5868/6180 [10:16<00:32,  9.53it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  65% 574/885 [00:23<00:11, 27.01it/s]\u001b[A\n",
            "Epoch 0:  95% 5872/6180 [10:16<00:32,  9.53it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5876/6180 [10:16<00:31,  9.53it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5880/6180 [10:16<00:31,  9.54it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5884/6180 [10:16<00:31,  9.54it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  67% 590/885 [00:23<00:11, 25.12it/s]\u001b[A\n",
            "Epoch 0:  95% 5888/6180 [10:16<00:30,  9.54it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5892/6180 [10:17<00:30,  9.55it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5896/6180 [10:17<00:29,  9.55it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  95% 5900/6180 [10:17<00:29,  9.56it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  68% 606/885 [00:24<00:10, 25.55it/s]\u001b[A\n",
            "Epoch 0:  96% 5904/6180 [10:17<00:28,  9.56it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5908/6180 [10:17<00:28,  9.57it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5912/6180 [10:17<00:28,  9.57it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5916/6180 [10:17<00:27,  9.57it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5920/6180 [10:18<00:27,  9.58it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5924/6180 [10:18<00:26,  9.58it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  71% 630/885 [00:25<00:09, 25.97it/s]\u001b[A\n",
            "Epoch 0:  96% 5928/6180 [10:18<00:26,  9.59it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5932/6180 [10:18<00:25,  9.59it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5936/6180 [10:18<00:25,  9.59it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  73% 642/885 [00:25<00:10, 22.73it/s]\u001b[A\n",
            "Epoch 0:  96% 5940/6180 [10:18<00:25,  9.60it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5944/6180 [10:19<00:24,  9.60it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5948/6180 [10:19<00:24,  9.60it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5952/6180 [10:19<00:23,  9.61it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  74% 658/885 [00:26<00:11, 20.37it/s]\u001b[A\n",
            "Epoch 0:  96% 5956/6180 [10:19<00:23,  9.61it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  96% 5960/6180 [10:19<00:22,  9.61it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5964/6180 [10:20<00:22,  9.62it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  76% 670/885 [00:27<00:10, 20.42it/s]\u001b[A\n",
            "Epoch 0:  97% 5968/6180 [10:20<00:22,  9.62it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5972/6180 [10:20<00:21,  9.63it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5976/6180 [10:20<00:21,  9.63it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5980/6180 [10:20<00:20,  9.63it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5984/6180 [10:20<00:20,  9.64it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  78% 690/885 [00:28<00:07, 24.90it/s]\u001b[A\n",
            "Epoch 0:  97% 5988/6180 [10:21<00:19,  9.64it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5992/6180 [10:21<00:19,  9.65it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 5996/6180 [10:21<00:19,  9.65it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 6000/6180 [10:21<00:18,  9.65it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 6004/6180 [10:21<00:18,  9.66it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  80% 710/885 [00:28<00:06, 25.60it/s]\u001b[A\n",
            "Epoch 0:  97% 6008/6180 [10:21<00:17,  9.66it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 6012/6180 [10:22<00:17,  9.67it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 6016/6180 [10:22<00:16,  9.67it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  82% 722/885 [00:29<00:06, 25.47it/s]\u001b[A\n",
            "Epoch 0:  97% 6020/6180 [10:22<00:16,  9.67it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  97% 6024/6180 [10:22<00:16,  9.68it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6028/6180 [10:22<00:15,  9.68it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6032/6180 [10:22<00:15,  9.69it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  83% 738/885 [00:29<00:06, 24.16it/s]\u001b[A\n",
            "Epoch 0:  98% 6036/6180 [10:22<00:14,  9.69it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6040/6180 [10:23<00:14,  9.69it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6044/6180 [10:23<00:14,  9.70it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  85% 750/885 [00:30<00:05, 23.90it/s]\u001b[A\n",
            "Epoch 0:  98% 6048/6180 [10:23<00:13,  9.70it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6052/6180 [10:23<00:13,  9.70it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6056/6180 [10:23<00:12,  9.71it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6060/6180 [10:23<00:12,  9.71it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  87% 766/885 [00:31<00:05, 23.27it/s]\u001b[A\n",
            "Epoch 0:  98% 6064/6180 [10:24<00:11,  9.72it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6068/6180 [10:24<00:11,  9.72it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6072/6180 [10:24<00:11,  9.72it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6076/6180 [10:24<00:10,  9.73it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  98% 6080/6180 [10:24<00:10,  9.73it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  89% 786/885 [00:31<00:03, 25.82it/s]\u001b[A\n",
            "Epoch 0:  98% 6084/6180 [10:24<00:09,  9.74it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6088/6180 [10:25<00:09,  9.74it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6092/6180 [10:25<00:09,  9.74it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6096/6180 [10:25<00:08,  9.75it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6100/6180 [10:25<00:08,  9.75it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6104/6180 [10:25<00:07,  9.76it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6108/6180 [10:25<00:07,  9.76it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6112/6180 [10:25<00:06,  9.77it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6116/6180 [10:26<00:06,  9.77it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  93% 822/885 [00:33<00:02, 25.83it/s]\u001b[A\n",
            "Epoch 0:  99% 6120/6180 [10:26<00:06,  9.77it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6124/6180 [10:26<00:05,  9.78it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6128/6180 [10:26<00:05,  9.78it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6132/6180 [10:26<00:04,  9.78it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6136/6180 [10:26<00:04,  9.79it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6140/6180 [10:26<00:04,  9.79it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  96% 846/885 [00:34<00:01, 27.85it/s]\u001b[A\n",
            "Epoch 0:  99% 6144/6180 [10:27<00:03,  9.80it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0:  99% 6148/6180 [10:27<00:03,  9.80it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0: 100% 6152/6180 [10:27<00:02,  9.80it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0: 100% 6156/6180 [10:27<00:02,  9.81it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0: 100% 6160/6180 [10:27<00:02,  9.81it/s, loss=0.709, v_num=2-1]\n",
            "Validating:  98% 866/885 [00:34<00:00, 27.05it/s]\u001b[A\n",
            "Epoch 0: 100% 6164/6180 [10:27<00:01,  9.82it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0: 100% 6168/6180 [10:28<00:01,  9.82it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0: 100% 6172/6180 [10:28<00:00,  9.82it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 0: 100% 6176/6180 [10:28<00:00,  9.83it/s, loss=0.709, v_num=2-1]\n",
            "Validating: 100% 882/885 [00:35<00:00, 26.39it/s]\u001b[A\n",
            "Epoch 0: 100% 6180/6180 [10:32<00:00,  9.77it/s, loss=0.709, v_num=2-1]\n",
            "Epoch 1:  86% 5295/6180 [09:55<01:39,  8.90it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 5296/6180 [09:55<01:39,  8.89it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5300/6180 [09:55<01:38,  8.89it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5304/6180 [09:56<01:38,  8.90it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:   1% 10/885 [00:00<03:21,  4.34it/s]\u001b[A\n",
            "Epoch 1:  86% 5308/6180 [09:56<01:37,  8.90it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5312/6180 [09:56<01:37,  8.91it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5316/6180 [09:56<01:36,  8.91it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5320/6180 [09:56<01:36,  8.92it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:   3% 26/885 [00:01<01:04, 13.27it/s]\u001b[A\n",
            "Epoch 1:  86% 5324/6180 [09:56<01:35,  8.92it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5328/6180 [09:57<01:35,  8.92it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5332/6180 [09:57<01:34,  8.93it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5336/6180 [09:57<01:34,  8.93it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:   5% 42/885 [00:02<00:37, 22.21it/s]\u001b[A\n",
            "Epoch 1:  86% 5340/6180 [09:57<01:33,  8.94it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  86% 5344/6180 [09:57<01:33,  8.94it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5348/6180 [09:57<01:33,  8.95it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:   6% 54/885 [00:02<00:35, 23.43it/s]\u001b[A\n",
            "Epoch 1:  87% 5352/6180 [09:57<01:32,  8.95it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5356/6180 [09:58<01:32,  8.95it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5360/6180 [09:58<01:31,  8.96it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5364/6180 [09:58<01:31,  8.96it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5368/6180 [09:58<01:30,  8.97it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:   8% 74/885 [00:03<00:31, 25.60it/s]\u001b[A\n",
            "Epoch 1:  87% 5372/6180 [09:58<01:30,  8.97it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5376/6180 [09:58<01:29,  8.98it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5380/6180 [09:59<01:29,  8.98it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5384/6180 [09:59<01:28,  8.99it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5388/6180 [09:59<01:28,  8.99it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5392/6180 [09:59<01:27,  8.99it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5396/6180 [09:59<01:27,  9.00it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  12% 102/885 [00:04<00:28, 27.70it/s]\u001b[A\n",
            "Epoch 1:  87% 5400/6180 [09:59<01:26,  9.00it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  87% 5404/6180 [09:59<01:26,  9.01it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5408/6180 [10:00<01:25,  9.01it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  13% 114/885 [00:05<00:31, 24.64it/s]\u001b[A\n",
            "Epoch 1:  88% 5412/6180 [10:00<01:25,  9.02it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5416/6180 [10:00<01:24,  9.02it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5420/6180 [10:00<01:24,  9.02it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  14% 126/885 [00:05<00:29, 26.00it/s]\u001b[A\n",
            "Epoch 1:  88% 5424/6180 [10:00<01:23,  9.03it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5428/6180 [10:00<01:23,  9.03it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5432/6180 [10:01<01:22,  9.04it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5436/6180 [10:01<01:22,  9.04it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5440/6180 [10:01<01:21,  9.05it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  16% 146/885 [00:06<00:28, 25.75it/s]\u001b[A\n",
            "Epoch 1:  88% 5444/6180 [10:01<01:21,  9.05it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5448/6180 [10:01<01:20,  9.05it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5452/6180 [10:01<01:20,  9.06it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5456/6180 [10:02<01:19,  9.06it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  18% 162/885 [00:06<00:29, 24.62it/s]\u001b[A\n",
            "Epoch 1:  88% 5460/6180 [10:02<01:19,  9.07it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5464/6180 [10:02<01:18,  9.07it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  88% 5468/6180 [10:02<01:18,  9.07it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5472/6180 [10:02<01:17,  9.08it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  20% 178/885 [00:07<00:28, 24.64it/s]\u001b[A\n",
            "Epoch 1:  89% 5476/6180 [10:02<01:17,  9.08it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5480/6180 [10:02<01:17,  9.09it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5484/6180 [10:03<01:16,  9.09it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5488/6180 [10:03<01:16,  9.10it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  22% 194/885 [00:08<00:25, 26.99it/s]\u001b[A\n",
            "Epoch 1:  89% 5492/6180 [10:03<01:15,  9.10it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5496/6180 [10:03<01:15,  9.11it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5500/6180 [10:03<01:14,  9.11it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  23% 206/885 [00:08<00:27, 25.10it/s]\u001b[A\n",
            "Epoch 1:  89% 5504/6180 [10:03<01:14,  9.11it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5508/6180 [10:04<01:13,  9.12it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5512/6180 [10:04<01:13,  9.12it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  25% 218/885 [00:09<00:29, 22.84it/s]\u001b[A\n",
            "Epoch 1:  89% 5516/6180 [10:04<01:12,  9.13it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5520/6180 [10:04<01:12,  9.13it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  89% 5524/6180 [10:04<01:11,  9.13it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  26% 230/885 [00:09<00:26, 24.71it/s]\u001b[A\n",
            "Epoch 1:  89% 5528/6180 [10:04<01:11,  9.14it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5532/6180 [10:05<01:10,  9.14it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5536/6180 [10:05<01:10,  9.15it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5540/6180 [10:05<01:09,  9.15it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  28% 246/885 [00:10<00:24, 26.34it/s]\u001b[A\n",
            "Epoch 1:  90% 5544/6180 [10:05<01:09,  9.16it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5548/6180 [10:05<01:08,  9.16it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5552/6180 [10:05<01:08,  9.16it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  29% 258/885 [00:10<00:25, 24.96it/s]\u001b[A\n",
            "Epoch 1:  90% 5556/6180 [10:05<01:08,  9.17it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5560/6180 [10:06<01:07,  9.17it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5564/6180 [10:06<01:07,  9.18it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  31% 270/885 [00:11<00:24, 24.88it/s]\u001b[A\n",
            "Epoch 1:  90% 5568/6180 [10:06<01:06,  9.18it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5572/6180 [10:06<01:06,  9.19it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5576/6180 [10:06<01:05,  9.19it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5580/6180 [10:06<01:05,  9.19it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5584/6180 [10:07<01:04,  9.20it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  90% 5588/6180 [10:07<01:04,  9.20it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  33% 294/885 [00:12<00:22, 26.53it/s]\u001b[A\n",
            "Epoch 1:  90% 5592/6180 [10:07<01:03,  9.21it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5596/6180 [10:07<01:03,  9.21it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5600/6180 [10:07<01:02,  9.21it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  35% 306/885 [00:12<00:24, 24.10it/s]\u001b[A\n",
            "Epoch 1:  91% 5604/6180 [10:07<01:02,  9.22it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5608/6180 [10:08<01:02,  9.22it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5612/6180 [10:08<01:01,  9.23it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  36% 318/885 [00:13<00:23, 23.72it/s]\u001b[A\n",
            "Epoch 1:  91% 5616/6180 [10:08<01:01,  9.23it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5620/6180 [10:08<01:00,  9.23it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5624/6180 [10:08<01:00,  9.24it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  37% 330/885 [00:13<00:25, 21.39it/s]\u001b[A\n",
            "Epoch 1:  91% 5628/6180 [10:09<00:59,  9.24it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5632/6180 [10:09<00:59,  9.24it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5636/6180 [10:09<00:58,  9.25it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  39% 342/885 [00:14<00:26, 20.47it/s]\u001b[A\n",
            "Epoch 1:  91% 5640/6180 [10:09<00:58,  9.25it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5644/6180 [10:09<00:57,  9.26it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5648/6180 [10:09<00:57,  9.26it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  91% 5652/6180 [10:10<00:56,  9.26it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  40% 358/885 [00:14<00:20, 25.31it/s]\u001b[A\n",
            "Epoch 1:  92% 5656/6180 [10:10<00:56,  9.27it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5660/6180 [10:10<00:56,  9.27it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5664/6180 [10:10<00:55,  9.28it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5668/6180 [10:10<00:55,  9.28it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5672/6180 [10:10<00:54,  9.29it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5676/6180 [10:10<00:54,  9.29it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  43% 382/885 [00:15<00:19, 25.82it/s]\u001b[A\n",
            "Epoch 1:  92% 5680/6180 [10:11<00:53,  9.29it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5684/6180 [10:11<00:53,  9.30it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5688/6180 [10:11<00:52,  9.30it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5692/6180 [10:11<00:52,  9.31it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5696/6180 [10:11<00:51,  9.31it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  45% 402/885 [00:16<00:21, 22.98it/s]\u001b[A\n",
            "Epoch 1:  92% 5700/6180 [10:11<00:51,  9.31it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5704/6180 [10:12<00:51,  9.32it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5708/6180 [10:12<00:50,  9.32it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5712/6180 [10:12<00:50,  9.33it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  92% 5716/6180 [10:12<00:49,  9.33it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  48% 422/885 [00:17<00:17, 25.77it/s]\u001b[A\n",
            "Epoch 1:  93% 5720/6180 [10:12<00:49,  9.34it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5724/6180 [10:12<00:48,  9.34it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5728/6180 [10:13<00:48,  9.34it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5732/6180 [10:13<00:47,  9.35it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  49% 438/885 [00:18<00:17, 25.59it/s]\u001b[A\n",
            "Epoch 1:  93% 5736/6180 [10:13<00:47,  9.35it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5740/6180 [10:13<00:47,  9.36it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5744/6180 [10:13<00:46,  9.36it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5748/6180 [10:13<00:46,  9.36it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  51% 454/885 [00:18<00:16, 26.46it/s]\u001b[A\n",
            "Epoch 1:  93% 5752/6180 [10:13<00:45,  9.37it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5756/6180 [10:14<00:45,  9.37it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5760/6180 [10:14<00:44,  9.38it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  53% 466/885 [00:19<00:17, 23.89it/s]\u001b[A\n",
            "Epoch 1:  93% 5764/6180 [10:14<00:44,  9.38it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5768/6180 [10:14<00:43,  9.38it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  93% 5772/6180 [10:14<00:43,  9.39it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  54% 478/885 [00:19<00:18, 22.13it/s]\u001b[A\n",
            "Epoch 1:  93% 5776/6180 [10:15<00:43,  9.39it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5780/6180 [10:15<00:42,  9.40it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5784/6180 [10:15<00:42,  9.40it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5788/6180 [10:15<00:41,  9.40it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5792/6180 [10:15<00:41,  9.41it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5796/6180 [10:15<00:40,  9.41it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  57% 502/885 [00:20<00:13, 28.07it/s]\u001b[A\n",
            "Epoch 1:  94% 5800/6180 [10:15<00:40,  9.42it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5804/6180 [10:16<00:39,  9.42it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5808/6180 [10:16<00:39,  9.43it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  58% 514/885 [00:21<00:14, 25.72it/s]\u001b[A\n",
            "Epoch 1:  94% 5812/6180 [10:16<00:39,  9.43it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5816/6180 [10:16<00:38,  9.43it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5820/6180 [10:16<00:38,  9.44it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5824/6180 [10:16<00:37,  9.44it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  60% 530/885 [00:21<00:14, 25.01it/s]\u001b[A\n",
            "Epoch 1:  94% 5828/6180 [10:17<00:37,  9.44it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5832/6180 [10:17<00:36,  9.45it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  94% 5836/6180 [10:17<00:36,  9.45it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  61% 542/885 [00:22<00:14, 23.32it/s]\u001b[A\n",
            "Epoch 1:  94% 5840/6180 [10:17<00:35,  9.46it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5844/6180 [10:17<00:35,  9.46it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5848/6180 [10:17<00:35,  9.46it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5852/6180 [10:18<00:34,  9.47it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5856/6180 [10:18<00:34,  9.47it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  64% 562/885 [00:23<00:12, 26.54it/s]\u001b[A\n",
            "Epoch 1:  95% 5860/6180 [10:18<00:33,  9.48it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5864/6180 [10:18<00:33,  9.48it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5868/6180 [10:18<00:32,  9.48it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5872/6180 [10:18<00:32,  9.49it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5876/6180 [10:18<00:32,  9.49it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5880/6180 [10:19<00:31,  9.50it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5884/6180 [10:19<00:31,  9.50it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  67% 590/885 [00:24<00:11, 25.11it/s]\u001b[A\n",
            "Epoch 1:  95% 5888/6180 [10:19<00:30,  9.50it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5892/6180 [10:19<00:30,  9.51it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  95% 5896/6180 [10:19<00:29,  9.51it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  68% 602/885 [00:24<00:11, 25.33it/s]\u001b[A\n",
            "Epoch 1:  95% 5900/6180 [10:19<00:29,  9.52it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5904/6180 [10:20<00:28,  9.52it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5908/6180 [10:20<00:28,  9.53it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5912/6180 [10:20<00:28,  9.53it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5916/6180 [10:20<00:27,  9.53it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  70% 622/885 [00:25<00:09, 26.60it/s]\u001b[A\n",
            "Epoch 1:  96% 5920/6180 [10:20<00:27,  9.54it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5924/6180 [10:20<00:26,  9.54it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5928/6180 [10:21<00:26,  9.55it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5932/6180 [10:21<00:25,  9.55it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5936/6180 [10:21<00:25,  9.55it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  73% 642/885 [00:26<00:10, 22.65it/s]\u001b[A\n",
            "Epoch 1:  96% 5940/6180 [10:21<00:25,  9.56it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5944/6180 [10:21<00:24,  9.56it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5948/6180 [10:22<00:24,  9.56it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5952/6180 [10:22<00:23,  9.57it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  74% 658/885 [00:27<00:11, 19.89it/s]\u001b[A\n",
            "Epoch 1:  96% 5956/6180 [10:22<00:23,  9.57it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  96% 5960/6180 [10:22<00:22,  9.57it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 5964/6180 [10:22<00:22,  9.58it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  76% 670/885 [00:27<00:10, 20.48it/s]\u001b[A\n",
            "Epoch 1:  97% 5968/6180 [10:22<00:22,  9.58it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 5972/6180 [10:23<00:21,  9.58it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 5976/6180 [10:23<00:21,  9.59it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  77% 682/885 [00:28<00:08, 22.69it/s]\u001b[A\n",
            "Epoch 1:  97% 5980/6180 [10:23<00:20,  9.59it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 5984/6180 [10:23<00:20,  9.60it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 5988/6180 [10:23<00:19,  9.60it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  78% 694/885 [00:28<00:07, 24.95it/s]\u001b[A\n",
            "Epoch 1:  97% 5992/6180 [10:23<00:19,  9.60it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 5996/6180 [10:24<00:19,  9.61it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 6000/6180 [10:24<00:18,  9.61it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 6004/6180 [10:24<00:18,  9.62it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  80% 710/885 [00:29<00:07, 24.78it/s]\u001b[A\n",
            "Epoch 1:  97% 6008/6180 [10:24<00:17,  9.62it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 6012/6180 [10:24<00:17,  9.62it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 6016/6180 [10:24<00:17,  9.63it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  82% 722/885 [00:29<00:06, 24.78it/s]\u001b[A\n",
            "Epoch 1:  97% 6020/6180 [10:25<00:16,  9.63it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  97% 6024/6180 [10:25<00:16,  9.64it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6028/6180 [10:25<00:15,  9.64it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6032/6180 [10:25<00:15,  9.64it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  83% 738/885 [00:30<00:06, 23.70it/s]\u001b[A\n",
            "Epoch 1:  98% 6036/6180 [10:25<00:14,  9.65it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6040/6180 [10:25<00:14,  9.65it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6044/6180 [10:26<00:14,  9.65it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  85% 750/885 [00:30<00:05, 23.35it/s]\u001b[A\n",
            "Epoch 1:  98% 6048/6180 [10:26<00:13,  9.66it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6052/6180 [10:26<00:13,  9.66it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6056/6180 [10:26<00:12,  9.67it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6060/6180 [10:26<00:12,  9.67it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  87% 766/885 [00:31<00:05, 22.79it/s]\u001b[A\n",
            "Epoch 1:  98% 6064/6180 [10:26<00:11,  9.67it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6068/6180 [10:27<00:11,  9.68it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6072/6180 [10:27<00:11,  9.68it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6076/6180 [10:27<00:10,  9.68it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  98% 6080/6180 [10:27<00:10,  9.69it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  89% 786/885 [00:32<00:03, 25.37it/s]\u001b[A\n",
            "Epoch 1:  98% 6084/6180 [10:27<00:09,  9.69it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6088/6180 [10:27<00:09,  9.70it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6092/6180 [10:27<00:09,  9.70it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6096/6180 [10:28<00:08,  9.71it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6100/6180 [10:28<00:08,  9.71it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6104/6180 [10:28<00:07,  9.71it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  92% 810/885 [00:33<00:02, 28.30it/s]\u001b[A\n",
            "Epoch 1:  99% 6108/6180 [10:28<00:07,  9.72it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6112/6180 [10:28<00:06,  9.72it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6116/6180 [10:28<00:06,  9.73it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  93% 822/885 [00:33<00:02, 25.18it/s]\u001b[A\n",
            "Epoch 1:  99% 6120/6180 [10:29<00:06,  9.73it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6124/6180 [10:29<00:05,  9.73it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6128/6180 [10:29<00:05,  9.74it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6132/6180 [10:29<00:04,  9.74it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6136/6180 [10:29<00:04,  9.75it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6140/6180 [10:29<00:04,  9.75it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  96% 846/885 [00:34<00:01, 27.60it/s]\u001b[A\n",
            "Epoch 1:  99% 6144/6180 [10:29<00:03,  9.75it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1:  99% 6148/6180 [10:30<00:03,  9.76it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1: 100% 6152/6180 [10:30<00:02,  9.76it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1: 100% 6156/6180 [10:30<00:02,  9.76it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1: 100% 6160/6180 [10:30<00:02,  9.77it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating:  98% 866/885 [00:35<00:00, 26.64it/s]\u001b[A\n",
            "Epoch 1: 100% 6164/6180 [10:30<00:01,  9.77it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1: 100% 6168/6180 [10:30<00:01,  9.78it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1: 100% 6172/6180 [10:31<00:00,  9.78it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 1: 100% 6176/6180 [10:31<00:00,  9.78it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Validating: 100% 882/885 [00:36<00:00, 26.07it/s]\u001b[A\n",
            "Epoch 1: 100% 6180/6180 [10:35<00:00,  9.73it/s, loss=0.713, v_num=2-1, train_loss=0.711]\n",
            "Epoch 2:  86% 5295/6180 [09:55<01:39,  8.89it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 5296/6180 [09:56<01:39,  8.88it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5300/6180 [09:56<01:39,  8.89it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5304/6180 [09:56<01:38,  8.89it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:   1% 10/885 [00:00<03:19,  4.39it/s]\u001b[A\n",
            "Epoch 2:  86% 5308/6180 [09:56<01:38,  8.90it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5312/6180 [09:56<01:37,  8.90it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5316/6180 [09:56<01:37,  8.91it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5320/6180 [09:57<01:36,  8.91it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:   3% 26/885 [00:01<01:03, 13.55it/s]\u001b[A\n",
            "Epoch 2:  86% 5324/6180 [09:57<01:36,  8.91it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5328/6180 [09:57<01:35,  8.92it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5332/6180 [09:57<01:35,  8.92it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5336/6180 [09:57<01:34,  8.93it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:   5% 42/885 [00:02<00:37, 22.67it/s]\u001b[A\n",
            "Epoch 2:  86% 5340/6180 [09:57<01:34,  8.93it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  86% 5344/6180 [09:58<01:33,  8.94it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5348/6180 [09:58<01:33,  8.94it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:   6% 54/885 [00:02<00:35, 23.70it/s]\u001b[A\n",
            "Epoch 2:  87% 5352/6180 [09:58<01:32,  8.94it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5356/6180 [09:58<01:32,  8.95it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5360/6180 [09:58<01:31,  8.95it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5364/6180 [09:58<01:31,  8.96it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:   8% 70/885 [00:03<00:31, 25.71it/s]\u001b[A\n",
            "Epoch 2:  87% 5368/6180 [09:59<01:30,  8.96it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5372/6180 [09:59<01:30,  8.97it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5376/6180 [09:59<01:29,  8.97it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5380/6180 [09:59<01:29,  8.98it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5384/6180 [09:59<01:28,  8.98it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5388/6180 [09:59<01:28,  8.98it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5392/6180 [09:59<01:27,  8.99it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5396/6180 [10:00<01:27,  8.99it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  12% 102/885 [00:04<00:28, 27.11it/s]\u001b[A\n",
            "Epoch 2:  87% 5400/6180 [10:00<01:26,  9.00it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  87% 5404/6180 [10:00<01:26,  9.00it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5408/6180 [10:00<01:25,  9.01it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  13% 114/885 [00:05<00:30, 24.89it/s]\u001b[A\n",
            "Epoch 2:  88% 5412/6180 [10:00<01:25,  9.01it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5416/6180 [10:00<01:24,  9.01it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5420/6180 [10:01<01:24,  9.02it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  14% 126/885 [00:05<00:29, 26.00it/s]\u001b[A\n",
            "Epoch 2:  88% 5424/6180 [10:01<01:23,  9.02it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5428/6180 [10:01<01:23,  9.03it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5432/6180 [10:01<01:22,  9.03it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5436/6180 [10:01<01:22,  9.04it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5440/6180 [10:01<01:21,  9.04it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  16% 146/885 [00:06<00:29, 25.03it/s]\u001b[A\n",
            "Epoch 2:  88% 5444/6180 [10:02<01:21,  9.04it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5448/6180 [10:02<01:20,  9.05it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5452/6180 [10:02<01:20,  9.05it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5456/6180 [10:02<01:19,  9.06it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  18% 162/885 [00:06<00:29, 24.45it/s]\u001b[A\n",
            "Epoch 2:  88% 5460/6180 [10:02<01:19,  9.06it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5464/6180 [10:02<01:18,  9.06it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  88% 5468/6180 [10:02<01:18,  9.07it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5472/6180 [10:03<01:18,  9.07it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  20% 178/885 [00:07<00:28, 24.56it/s]\u001b[A\n",
            "Epoch 2:  89% 5476/6180 [10:03<01:17,  9.08it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5480/6180 [10:03<01:17,  9.08it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5484/6180 [10:03<01:16,  9.09it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5488/6180 [10:03<01:16,  9.09it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5492/6180 [10:03<01:15,  9.09it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5496/6180 [10:04<01:15,  9.10it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  23% 202/885 [00:08<00:28, 24.28it/s]\u001b[A\n",
            "Epoch 2:  89% 5500/6180 [10:04<01:14,  9.10it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5504/6180 [10:04<01:14,  9.11it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5508/6180 [10:04<01:13,  9.11it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5512/6180 [10:04<01:13,  9.12it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  25% 218/885 [00:09<00:28, 23.32it/s]\u001b[A\n",
            "Epoch 2:  89% 5516/6180 [10:04<01:12,  9.12it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5520/6180 [10:04<01:12,  9.12it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  89% 5524/6180 [10:05<01:11,  9.13it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  26% 230/885 [00:09<00:26, 25.18it/s]\u001b[A\n",
            "Epoch 2:  89% 5528/6180 [10:05<01:11,  9.13it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5532/6180 [10:05<01:10,  9.14it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5536/6180 [10:05<01:10,  9.14it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5540/6180 [10:05<01:09,  9.15it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  28% 246/885 [00:10<00:24, 26.55it/s]\u001b[A\n",
            "Epoch 2:  90% 5544/6180 [10:05<01:09,  9.15it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5548/6180 [10:06<01:09,  9.15it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5552/6180 [10:06<01:08,  9.16it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  29% 258/885 [00:10<00:24, 25.43it/s]\u001b[A\n",
            "Epoch 2:  90% 5556/6180 [10:06<01:08,  9.16it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5560/6180 [10:06<01:07,  9.17it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5564/6180 [10:06<01:07,  9.17it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  31% 270/885 [00:11<00:24, 25.12it/s]\u001b[A\n",
            "Epoch 2:  90% 5568/6180 [10:06<01:06,  9.17it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5572/6180 [10:07<01:06,  9.18it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5576/6180 [10:07<01:05,  9.18it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5580/6180 [10:07<01:05,  9.19it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5584/6180 [10:07<01:04,  9.19it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  90% 5588/6180 [10:07<01:04,  9.20it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  33% 294/885 [00:12<00:21, 27.33it/s]\u001b[A\n",
            "Epoch 2:  90% 5592/6180 [10:07<01:03,  9.20it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5596/6180 [10:08<01:03,  9.20it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5600/6180 [10:08<01:02,  9.21it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  35% 306/885 [00:12<00:24, 23.49it/s]\u001b[A\n",
            "Epoch 2:  91% 5604/6180 [10:08<01:02,  9.21it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5608/6180 [10:08<01:02,  9.22it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5612/6180 [10:08<01:01,  9.22it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  36% 318/885 [00:13<00:24, 23.15it/s]\u001b[A\n",
            "Epoch 2:  91% 5616/6180 [10:08<01:01,  9.22it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5620/6180 [10:09<01:00,  9.23it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5624/6180 [10:09<01:00,  9.23it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5628/6180 [10:09<00:59,  9.23it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  38% 334/885 [00:13<00:23, 23.37it/s]\u001b[A\n",
            "Epoch 2:  91% 5632/6180 [10:09<00:59,  9.24it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5636/6180 [10:09<00:58,  9.24it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5640/6180 [10:09<00:58,  9.25it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  39% 346/885 [00:14<00:24, 22.31it/s]\u001b[A\n",
            "Epoch 2:  91% 5644/6180 [10:10<00:57,  9.25it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5648/6180 [10:10<00:57,  9.25it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  91% 5652/6180 [10:10<00:57,  9.26it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  40% 358/885 [00:14<00:21, 24.82it/s]\u001b[A\n",
            "Epoch 2:  92% 5656/6180 [10:10<00:56,  9.26it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5660/6180 [10:10<00:56,  9.27it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5664/6180 [10:10<00:55,  9.27it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5668/6180 [10:11<00:55,  9.28it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5672/6180 [10:11<00:54,  9.28it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5676/6180 [10:11<00:54,  9.28it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  43% 382/885 [00:15<00:19, 25.90it/s]\u001b[A\n",
            "Epoch 2:  92% 5680/6180 [10:11<00:53,  9.29it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5684/6180 [10:11<00:53,  9.29it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5688/6180 [10:11<00:52,  9.30it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5692/6180 [10:12<00:52,  9.30it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5696/6180 [10:12<00:52,  9.30it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  45% 402/885 [00:16<00:21, 22.81it/s]\u001b[A\n",
            "Epoch 2:  92% 5700/6180 [10:12<00:51,  9.31it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5704/6180 [10:12<00:51,  9.31it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5708/6180 [10:12<00:50,  9.32it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5712/6180 [10:12<00:50,  9.32it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  92% 5716/6180 [10:12<00:49,  9.33it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  48% 422/885 [00:17<00:18, 25.72it/s]\u001b[A\n",
            "Epoch 2:  93% 5720/6180 [10:13<00:49,  9.33it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5724/6180 [10:13<00:48,  9.33it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5728/6180 [10:13<00:48,  9.34it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5732/6180 [10:13<00:47,  9.34it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  49% 438/885 [00:18<00:17, 24.92it/s]\u001b[A\n",
            "Epoch 2:  93% 5736/6180 [10:13<00:47,  9.35it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5740/6180 [10:13<00:47,  9.35it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5744/6180 [10:14<00:46,  9.35it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5748/6180 [10:14<00:46,  9.36it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  51% 454/885 [00:18<00:16, 26.41it/s]\u001b[A\n",
            "Epoch 2:  93% 5752/6180 [10:14<00:45,  9.36it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5756/6180 [10:14<00:45,  9.37it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5760/6180 [10:14<00:44,  9.37it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  53% 466/885 [00:19<00:18, 22.55it/s]\u001b[A\n",
            "Epoch 2:  93% 5764/6180 [10:14<00:44,  9.37it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5768/6180 [10:15<00:43,  9.38it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  93% 5772/6180 [10:15<00:43,  9.38it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  54% 478/885 [00:19<00:18, 21.54it/s]\u001b[A\n",
            "Epoch 2:  93% 5776/6180 [10:15<00:43,  9.38it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5780/6180 [10:15<00:42,  9.39it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5784/6180 [10:15<00:42,  9.39it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5788/6180 [10:15<00:41,  9.40it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5792/6180 [10:16<00:41,  9.40it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5796/6180 [10:16<00:40,  9.41it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  57% 502/885 [00:20<00:13, 28.07it/s]\u001b[A\n",
            "Epoch 2:  94% 5800/6180 [10:16<00:40,  9.41it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5804/6180 [10:16<00:39,  9.41it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5808/6180 [10:16<00:39,  9.42it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  58% 514/885 [00:21<00:14, 25.75it/s]\u001b[A\n",
            "Epoch 2:  94% 5812/6180 [10:16<00:39,  9.42it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5816/6180 [10:17<00:38,  9.43it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5820/6180 [10:17<00:38,  9.43it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5824/6180 [10:17<00:37,  9.43it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  60% 530/885 [00:21<00:14, 25.23it/s]\u001b[A\n",
            "Epoch 2:  94% 5828/6180 [10:17<00:37,  9.44it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5832/6180 [10:17<00:36,  9.44it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  94% 5836/6180 [10:17<00:36,  9.45it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  61% 542/885 [00:22<00:14, 23.50it/s]\u001b[A\n",
            "Epoch 2:  94% 5840/6180 [10:18<00:35,  9.45it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5844/6180 [10:18<00:35,  9.45it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5848/6180 [10:18<00:35,  9.46it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5852/6180 [10:18<00:34,  9.46it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5856/6180 [10:18<00:34,  9.47it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5860/6180 [10:18<00:33,  9.47it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  64% 566/885 [00:23<00:12, 25.33it/s]\u001b[A\n",
            "Epoch 2:  95% 5864/6180 [10:18<00:33,  9.47it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5868/6180 [10:19<00:32,  9.48it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5872/6180 [10:19<00:32,  9.48it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5876/6180 [10:19<00:32,  9.49it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5880/6180 [10:19<00:31,  9.49it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5884/6180 [10:19<00:31,  9.49it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  67% 590/885 [00:24<00:11, 25.34it/s]\u001b[A\n",
            "Epoch 2:  95% 5888/6180 [10:19<00:30,  9.50it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5892/6180 [10:20<00:30,  9.50it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5896/6180 [10:20<00:29,  9.51it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  95% 5900/6180 [10:20<00:29,  9.51it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  68% 606/885 [00:24<00:10, 25.85it/s]\u001b[A\n",
            "Epoch 2:  96% 5904/6180 [10:20<00:29,  9.51it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5908/6180 [10:20<00:28,  9.52it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5912/6180 [10:20<00:28,  9.52it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5916/6180 [10:20<00:27,  9.53it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  70% 622/885 [00:25<00:09, 26.79it/s]\u001b[A\n",
            "Epoch 2:  96% 5920/6180 [10:21<00:27,  9.53it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5924/6180 [10:21<00:26,  9.53it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5928/6180 [10:21<00:26,  9.54it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5932/6180 [10:21<00:25,  9.54it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5936/6180 [10:21<00:25,  9.55it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  73% 642/885 [00:26<00:10, 22.39it/s]\u001b[A\n",
            "Epoch 2:  96% 5940/6180 [10:22<00:25,  9.55it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5944/6180 [10:22<00:24,  9.55it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5948/6180 [10:22<00:24,  9.55it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  74% 654/885 [00:26<00:12, 17.83it/s]\u001b[A\n",
            "Epoch 2:  96% 5952/6180 [10:22<00:23,  9.56it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5956/6180 [10:22<00:23,  9.56it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  96% 5960/6180 [10:22<00:22,  9.57it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  75% 666/885 [00:27<00:09, 23.05it/s]\u001b[A\n",
            "Epoch 2:  97% 5964/6180 [10:23<00:22,  9.57it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 5968/6180 [10:23<00:22,  9.57it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 5972/6180 [10:23<00:21,  9.58it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  77% 678/885 [00:28<00:08, 23.48it/s]\u001b[A\n",
            "Epoch 2:  97% 5976/6180 [10:23<00:21,  9.58it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 5980/6180 [10:23<00:20,  9.59it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 5984/6180 [10:24<00:20,  9.59it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  78% 690/885 [00:28<00:08, 24.33it/s]\u001b[A\n",
            "Epoch 2:  97% 5988/6180 [10:24<00:20,  9.59it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 5992/6180 [10:24<00:19,  9.60it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 5996/6180 [10:24<00:19,  9.60it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 6000/6180 [10:24<00:18,  9.60it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 6004/6180 [10:24<00:18,  9.61it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  80% 710/885 [00:29<00:07, 24.83it/s]\u001b[A\n",
            "Epoch 2:  97% 6008/6180 [10:25<00:17,  9.61it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 6012/6180 [10:25<00:17,  9.62it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 6016/6180 [10:25<00:17,  9.62it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  82% 722/885 [00:29<00:06, 24.74it/s]\u001b[A\n",
            "Epoch 2:  97% 6020/6180 [10:25<00:16,  9.62it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  97% 6024/6180 [10:25<00:16,  9.63it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6028/6180 [10:25<00:15,  9.63it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  83% 734/885 [00:30<00:05, 25.55it/s]\u001b[A\n",
            "Epoch 2:  98% 6032/6180 [10:25<00:15,  9.64it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6036/6180 [10:26<00:14,  9.64it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6040/6180 [10:26<00:14,  9.64it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  84% 746/885 [00:30<00:05, 26.50it/s]\u001b[A\n",
            "Epoch 2:  98% 6044/6180 [10:26<00:14,  9.65it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6048/6180 [10:26<00:13,  9.65it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6052/6180 [10:26<00:13,  9.65it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  86% 758/885 [00:31<00:05, 22.38it/s]\u001b[A\n",
            "Epoch 2:  98% 6056/6180 [10:26<00:12,  9.66it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6060/6180 [10:27<00:12,  9.66it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6064/6180 [10:27<00:12,  9.67it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  87% 770/885 [00:31<00:04, 24.32it/s]\u001b[A\n",
            "Epoch 2:  98% 6068/6180 [10:27<00:11,  9.67it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6072/6180 [10:27<00:11,  9.67it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6076/6180 [10:27<00:10,  9.68it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  98% 6080/6180 [10:27<00:10,  9.68it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  89% 786/885 [00:32<00:03, 25.47it/s]\u001b[A\n",
            "Epoch 2:  98% 6084/6180 [10:28<00:09,  9.69it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6088/6180 [10:28<00:09,  9.69it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6092/6180 [10:28<00:09,  9.69it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6096/6180 [10:28<00:08,  9.70it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6100/6180 [10:28<00:08,  9.70it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6104/6180 [10:28<00:07,  9.71it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6108/6180 [10:28<00:07,  9.71it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6112/6180 [10:29<00:06,  9.72it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  92% 818/885 [00:33<00:02, 28.62it/s]\u001b[A\n",
            "Epoch 2:  99% 6116/6180 [10:29<00:06,  9.72it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6120/6180 [10:29<00:06,  9.72it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6124/6180 [10:29<00:05,  9.73it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6128/6180 [10:29<00:05,  9.73it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  94% 834/885 [00:34<00:01, 26.66it/s]\u001b[A\n",
            "Epoch 2:  99% 6132/6180 [10:29<00:04,  9.73it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6136/6180 [10:30<00:04,  9.74it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6140/6180 [10:30<00:04,  9.74it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2:  99% 6144/6180 [10:30<00:03,  9.75it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  96% 850/885 [00:34<00:01, 25.50it/s]\u001b[A\n",
            "Epoch 2:  99% 6148/6180 [10:30<00:03,  9.75it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2: 100% 6152/6180 [10:30<00:02,  9.75it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2: 100% 6156/6180 [10:30<00:02,  9.76it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2: 100% 6160/6180 [10:31<00:02,  9.76it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating:  98% 866/885 [00:35<00:00, 26.48it/s]\u001b[A\n",
            "Epoch 2: 100% 6164/6180 [10:31<00:01,  9.77it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2: 100% 6168/6180 [10:31<00:01,  9.77it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2: 100% 6172/6180 [10:31<00:00,  9.77it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Epoch 2: 100% 6176/6180 [10:31<00:00,  9.78it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Validating: 100% 882/885 [00:36<00:00, 25.95it/s]\u001b[A\n",
            "Epoch 2: 100% 6180/6180 [10:32<00:00,  9.78it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "                                                 \u001b[ASaving latest checkpoint..\n",
            "Epoch 2: 100% 6180/6180 [10:33<00:00,  9.76it/s, loss=0.710, v_num=2-1, train_loss=0.71]\n",
            "Computing Input\n",
            "100% 5295/5295 [00:33<00:00, 160.12it/s]\n",
            "100% 885/885 [00:05<00:00, 167.00it/s]\n",
            "100% 77/77 [00:00<00:00, 170.25it/s]\n",
            "Testing:  99% 76/77 [00:03<00:00, 26.42it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "METRICS ON TESTSET\n",
            "tp: 10.0\n",
            "fp: 67.0\n",
            "fn: 0.0\n",
            "tn: 0.0\n",
            "precision: 0.12987013161182404\n",
            "recall: 1.0\n",
            "f1: 0.2298850566148758\n",
            "mcc: 0\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.1299), 'test_loss': tensor(0.6915, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 77/77 [00:03<00:00, 20.02it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPJ6phcFdEPE",
        "outputId": "c1429227-1c2f-4944-cacf-fd4762fb7515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%cd \"/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/Product Development/Google Colab/news-topic-cls/research/projects/news-topics-cls-custom-model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcWFmQUtc2xD"
      },
      "source": [
        "!git config user.name \"baradl\"\n",
        "!git config user.email \"basradloff@gmail.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91z_JTk987-F",
        "outputId": "87668e42-0bcb-41d1-ab42-436ef00b5759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git stash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved working directory and index state WIP on master: 31dcfff new metrics computation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDabuva1GsoM",
        "outputId": "accb8db5-3e78-4b9a-ef4a-b6ff3e724ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/19)\u001b[K\rremote: Counting objects:  10% (2/19)\u001b[K\rremote: Counting objects:  15% (3/19)\u001b[K\rremote: Counting objects:  21% (4/19)\u001b[K\rremote: Counting objects:  26% (5/19)\u001b[K\rremote: Counting objects:  31% (6/19)\u001b[K\rremote: Counting objects:  36% (7/19)\u001b[K\rremote: Counting objects:  42% (8/19)\u001b[K\rremote: Counting objects:  47% (9/19)\u001b[K\rremote: Counting objects:  52% (10/19)\u001b[K\rremote: Counting objects:  57% (11/19)\u001b[K\rremote: Counting objects:  63% (12/19)\u001b[K\rremote: Counting objects:  68% (13/19)\u001b[K\rremote: Counting objects:  73% (14/19)\u001b[K\rremote: Counting objects:  78% (15/19)\u001b[K\rremote: Counting objects:  84% (16/19)\u001b[K\rremote: Counting objects:  89% (17/19)\u001b[K\rremote: Counting objects:  94% (18/19)\u001b[K\rremote: Counting objects: 100% (19/19)\u001b[K\rremote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 10 (delta 7), reused 10 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n",
            "From https://github.com/baradl/research\n",
            "   cd77edb..72926cc  master     -> origin/master\n",
            "Updating cd77edb..72926cc\n",
            "Fast-forward\n",
            " .../news-topic-cls/core/models/multilabel_longformer.py     |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " .../news-topic-cls/core/models/multilabel_roberta.py        | 13 \u001b[32m+++++++\u001b[m\u001b[31m------\u001b[m\n",
            " .../news-topic-cls/core/models/rec_rob.py                   |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 3 files changed, 9 insertions(+), 8 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO7rVMbWJ-JS"
      },
      "source": [
        "!git add * "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4sJrMvlKOx8",
        "outputId": "88b3cd7f-ea39-4026-e54b-769286ba4c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!git commit -m \"Experiments\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "\t\u001b[31mmodified:   ../analytics-js/analyze-diffbot-log.sh\u001b[m\n",
            "\t\u001b[31mmodified:   ../blink-pr/script.py\u001b[m\n",
            "\t\u001b[31mmodified:   ../blink-pr/setup-service.sh\u001b[m\n",
            "\t\u001b[31mmodified:   ../blink-pr/setup/blink-pr-service\u001b[m\n",
            "\t\u001b[31mmodified:   ../graphex/README.md\u001b[m\n",
            "\t\u001b[31mmodified:   ../graphex/public/index.html\u001b[m\n",
            "\t\u001b[31mmodified:   ../graphex/public/js/main.js\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-08/news_topics_Article_sandbox.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-08/news_topics_Company_sandbox.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-08/news_topics_Consultant_sandbox.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-08/news_topics_Job_sandbox.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_article-items_article-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_article-items_job-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_company-items_article-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_company-items_job-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_consultant-items_article-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_consultant-items_job-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_job-items_article-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-02-15/news_topics_job-items_job-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-03-21/news_topics_article-items_article-live-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-03-21/news_topics_article-items_article-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-03-21/news_topics_funding_info-items_article-live-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-03-21/news_topics_funding_info-items_article-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-03-21/news_topics_funding_info-items_job-live-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/performance/gold-standard-evaluation_2018-03-21/news_topics_funding_info-items_job-sandbox-classifier.json\u001b[m\n",
            "\t\u001b[31mmodified:   ../news-topics-classification/training-sets/load-training-sets.sh\u001b[m\n",
            "\t\u001b[31mmodified:   ../quordlepleen/dance\u001b[m\n",
            "\n",
            "no changes added to commit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATA49_KIKTBs",
        "outputId": "fe22dfe4-b98b-4e64-d066-8158ee38971a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git push"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znW67P3PCI4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}